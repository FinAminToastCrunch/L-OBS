{\rtf1\ansi\ansicpg1252\deff0\deflang1033{\fonttbl{\f0\fmodern Courier New;}{\f1\fmodern\fcharset0 Courier New;}{\f2\fnil\fcharset129 Tahoma;}}
{\colortbl ;\red0\green0\blue0;\red48\green127\blue48;\red127\green127\blue27;\red127\green27\blue127;\red27\green127\blue127;\red64\green127\blue64;}
\viewkind4\uc1\pard\f0\fs18      \u9484?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9488?\cf1\highlight1 
\par \cf0\highlight0      \u9474?                        \cf2\f1\bullet  MobaXterm 10.4 \bullet  \cf0\f0                          \u9474?\cf1\highlight1 
\par \cf0\highlight0      \u9474?            \cf3 (SSH client, X-server and networking tools)\cf0              \u9474?\cf1\highlight1 
\par \cf0\highlight0      \u9474?                                                                    \u9474?\cf1\highlight1 
\par \cf0\highlight0      \u9474? \u8594? SSH session to \cf4 xdong\cf0 @\cf4 192.168.65.22                               \cf0\u9474?\cf1\highlight1 
\par \cf0\highlight0      \u9474?\f1    \bullet  SSH compression : \cf2\f0 v\cf0                                             \u9474?\cf1\highlight1 
\par \cf0\highlight0      \u9474?\f1    \bullet  SSH-browser     : \cf2\f0 v\cf0                                             \u9474?\cf1\highlight1 
\par \cf0\highlight0      \u9474?\f1    \bullet  X11-forwarding  : \cf2\f0 v\cf0   (remote display is forwarded through SSH) \u9474?\cf1\highlight1 
\par \cf0\highlight0      \u9474?\f1    \bullet  DISPLAY         : \cf2\f0 v\cf0   (automatically set on remote server)      \u9474?\cf1\highlight1 
\par \cf0\highlight0      \u9474?                                                                    \u9474?\cf1\highlight1 
\par \cf0\highlight0      \u9474? \u8594? For more info, ctrl+click on \cf5\ul help\cf0\ulnone  or visit our \cf5\ul website\cf0\ulnone            \u9474?\cf1\highlight1 
\par \cf0\highlight0      \u9492?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9496?\cf1\highlight1 
\par 
\par \cf0\highlight0 Welcome to Ubuntu 14.04.5 LTS (GNU/Linux 3.16.0-51-generic x86_64)\cf1\highlight1 
\par 
\par \cf0\highlight0  * Documentation:  https://help.ubuntu.com/\cf1\highlight1 
\par 
\par \cf0\highlight0   System information as of Mon Oct 23 15:50:20 PDT 2017\cf1\highlight1 
\par 
\par \cf0\highlight0   System load:  3.59               Processes:           549\cf1\highlight1 
\par \cf0\highlight0   Usage of /:   5.9% of 907.29GB   Users logged in:     8\cf1\highlight1 
\par \cf0\highlight0   Memory usage: 43%                IP address for eth0: 192.168.65.22\cf1\highlight1 
\par \cf0\highlight0   Swap usage:   19%\cf1\highlight1 
\par 
\par \cf0\highlight0   Graph this data and manage this system at:\cf1\highlight1 
\par \cf0\highlight0     https://landscape.canonical.com/\cf1\highlight1 
\par 
\par \cf0\highlight0 381 packages can be updated.\cf1\highlight1 
\par \cf0\highlight0 275 updates are security updates.\cf1\highlight1 
\par 
\par \cf0\highlight0 New release '16.04.3 LTS' available.\cf1\highlight1 
\par \cf0\highlight0 Run 'do-release-upgrade' to upgrade to it.\cf1\highlight1 
\par 
\par 
\par \cf0\highlight0 WARNING: Security updates for your current Hardware Enablement\cf1\highlight1 
\par \cf0\highlight0 Stack ended on 2016-08-04:\cf1\highlight1 
\par \cf0\highlight0  * http://wiki.ubuntu.com/1404_HWE_EOL\cf1\highlight1 
\par 
\par \cf0\highlight0 There is a graphics stack installed on this system. An upgrade to a\cf1\highlight1 
\par \cf0\highlight0 configuration supported for the full lifetime of the LTS will become\cf1\highlight1 
\par \cf0\highlight0 available on 2016-07-21 and can be installed by running 'update-manager'\cf1\highlight1 
\par \cf0\highlight0 in the Dash.\cf1\highlight1 
\par 
\par \cf0\highlight0 Last login: Mon Oct 23 13:36:04 2017 from 192.168.65.90\cf1\highlight1 
\par \cf0\highlight0 xdong@gpu2:~$ source ~/tf/bin/activate\cf1\highlight1 
\par \cf0\highlight0 (tf) xdong@gpu2:~$ cd /data/xdong/codes/avg_para\cf1\highlight1 
\par \cf0\highlight0 (tf) xdong@gpu2:/data/xdong/codes/avg_para$ CUDA_VISIBLE_DEVICES=0 python residual_network_cifar10.py\cf1\highlight1 
\par \cf0\highlight0 Couldn't import dot_parser, loading of dot files will not be possible.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 16:56:27.312041: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could spePU computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 16:56:27.312072: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could spePU computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 16:56:27.312080: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 16:56:27.312086: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 16:56:27.312093: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 16:56:27.902923: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties:\cf1\highlight1 
\par \cf0\highlight0 name: GeForce GTX TITAN X\cf1\highlight1 
\par \cf0\highlight0 major: 5 minor: 2 memoryClockRate (GHz) 1.076\cf1\highlight1 
\par \cf0\highlight0 pciBusID 0000:02:00.0\cf1\highlight1 
\par \cf0\highlight0 Total memory: 11.92GiB\cf1\highlight1 
\par \cf0\highlight0 Free memory: 11.81GiB\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 16:56:27.903027: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 16:56:27.903044: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 16:56:27.903063: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:02:00.0)\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 16:56:45.601400: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:02:00.0)\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Run id: resnet_cifar10\cf1\highlight1 
\par \cf0\highlight0 Log directory: /tmp/tflearn_logs/\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Preprocessing... Calculating mean over all dataset (this may take long)...\cf1\highlight1 
\par \cf0\highlight0 Mean: [ 0.49139968  0.48215841  0.44653091] (To avoid repetitive computation, add it to argument 'mean' of `add_featurewise_zero_center`)\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Training samples: 50000\cf1\highlight1 
\par \cf0\highlight0 Validation samples: 10000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 391  | total loss: \cf6 1.17464\cf0  | time: 46.746s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.17464 - acc: 0.5658 -- iter: 50000/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 [ Mon Oct 23 16:57:38 2017 ] Starting Avg ...\cf1\highlight1 
\par \cf0\highlight0 Traceback (most recent call last):\cf1\highlight1 
\par \cf0\highlight0   File "residual_network_cifar10.py", line 78, in <module>\cf1\highlight1 
\par \cf0\highlight0     tools.do_avg('ResidualBlock', 10, curr_sess)\cf1\highlight1 
\par \cf0\highlight0   File "/data/xdong/codes/avg_para/tools.py", line 41, in do_avg\cf1\highlight1 
\par \cf0\highlight0     avg_paras(name_list,sess)\cf1\highlight1 
\par \cf0\highlight0   File "/data/xdong/codes/avg_para/tools.py", line 24, in avg_paras\cf1\highlight1 
\par \cf0\highlight0     value_list = map(tflearn.variables.get_value, tensor_list, [sess]*len(tensor_list))\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tflearn/variables.py", line 150, in get_value\cf1\highlight1 
\par \cf0\highlight0     return var.eval(session)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 541, in eval\cf1\highlight1 
\par \cf0\highlight0     return _eval_using_default_session(self, feed_dict, self.graph, session)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 4071, in _eval_using_default_session\cf1\highlight1 
\par \cf0\highlight0     raise ValueError("Cannot evaluate tensor using `eval()`: No default "\cf1\highlight1 
\par \cf0\highlight0 ValueError: Cannot evaluate tensor using `eval()`: No default session is registered. Use `with sess.as_default()` or pass an explicit session to `eval(session=sess)`\cf1\highlight1 
\par \cf0\highlight0 (tf) xdong@gpu2:/data/xdong/codes/avg_para$ CUDA_VISIBLE_DEVICES=0 python residual_network_cifar10.py\cf1\highlight1 
\par \cf0\highlight0 Couldn't import dot_parser, loading of dot files will not be possible.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 17:04:36.492828: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could spePU computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 17:04:36.492862: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could spePU computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 17:04:36.492871: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 17:04:36.492878: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 17:04:36.492885: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 17:04:37.010364: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties:\cf1\highlight1 
\par \cf0\highlight0 name: GeForce GTX TITAN X\cf1\highlight1 
\par \cf0\highlight0 major: 5 minor: 2 memoryClockRate (GHz) 1.076\cf1\highlight1 
\par \cf0\highlight0 pciBusID 0000:02:00.0\cf1\highlight1 
\par \cf0\highlight0 Total memory: 11.92GiB\cf1\highlight1 
\par \cf0\highlight0 Free memory: 11.81GiB\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 17:04:37.010510: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 17:04:37.010531: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 17:04:37.010551: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:02:00.0)\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 17:04:56.142989: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:02:00.0)\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Run id: resnet_cifar10\cf1\highlight1 
\par \cf0\highlight0 Log directory: /tmp/tflearn_logs/\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Preprocessing... Calculating mean over all dataset (this may take long)...\cf1\highlight1 
\par \cf0\highlight0 Mean: [ 0.49139968  0.48215841  0.44653091] (To avoid repetitive computation, add it to argument 'mean' of `add_featurewise_zero_center`)\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Training samples: 50000\cf1\highlight1 
\par \cf0\highlight0 Validation samples: 10000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 391  | total loss: \cf6 1.26171\cf0  | time: 47.686s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.26171 - acc: 0.5424 -- iter: 50000/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 [ Mon Oct 23 17:05:49 2017 ] Starting Avg ...\cf1\highlight1 
\par \cf0\highlight0 [ Mon Oct 23 17:06:08 2017 ] Avg Done...\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Run id: resnet_cifar10\cf1\highlight1 
\par \cf0\highlight0 Log directory: /tmp/tflearn_logs/\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Training samples: 50000\cf1\highlight1 
\par \cf0\highlight0 Validation samples: 10000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 500  | total loss: \cf6 1.41293\cf0  | time: 15.265s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.41293 - acc: 0.4777 | val_loss: 1.70675 - val_acc: 0.4330 -- iter: 13952/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 782  | total loss: \cf6 1.08838\cf0  | time: 46.868s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.08838 - acc: 0.6029 -- iter: 50000/50000\cf1\highlight1 
\par \cf0\highlight0 [ Mon Oct 23 17:06:57 2017 ] Starting Avg ...\cf1\highlight1 
\par \cf0\highlight0 [ Mon Oct 23 17:07:15 2017 ] Avg Done...\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Run id: resnet_cifar10\cf1\highlight1 
\par \cf0\highlight0 Log directory: /tmp/tflearn_logs/\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Training samples: 50000\cf1\highlight1 
\par \cf0\highlight0 Validation samples: 10000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 ^Caining Step: 920  | total loss: \cf6 1.04372\cf0  | time: 16.528s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 003 | loss: 1.04372 - acc: 0.6166 -- iter: 17664/50000\cf1\highlight1 
\par \cf0\highlight0   --\cf1\highlight1 
\par \cf0\highlight0 Traceback (most recent call last):\cf1\highlight1 
\par \cf0\highlight0   File "residual_network_cifar10.py", line 74, in <module>\cf1\highlight1 
\par \cf0\highlight0     run_id='resnet_cifar10')\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tflearn/models/dnn.py", line 216, in fit\cf1\highlight1 
\par \cf0\highlight0     callbacks=callbacks)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tflearn/helpers/trainer.py", line 338, in fit\cf1\highlight1 
\par \cf0\highlight0     show_metric)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tflearn/helpers/trainer.py", line 817, in _train\cf1\highlight1 
\par \cf0\highlight0     feed_batch)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run\cf1\highlight1 
\par \cf0\highlight0     run_metadata_ptr)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1124, in _run\cf1\highlight1 
\par \cf0\highlight0     feed_dict_tensor, options, run_metadata)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1321, in _do_run\cf1\highlight1 
\par \cf0\highlight0     options, run_metadata)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1327, in _do_call\cf1\highlight1 
\par \cf0\highlight0     return fn(*args)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1306, in _run_fn\cf1\highlight1 
\par \cf0\highlight0     status, run_metadata)\cf1\highlight1 
\par \cf0\highlight0 KeyboardInterrupt\cf1\highlight1 
\par \cf0\highlight0 (tf) xdong@gpu2:/data/xdong/codes/avg_para$ CUDA_VISIBLE_DEVICES=0 python residual_network_cifar10.py\cf1\highlight1 
\par \cf0\highlight0 Couldn't import dot_parser, loading of dot files will not be possible.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 17:13:04.577695: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could spePU computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 17:13:04.577735: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could spePU computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 17:13:04.577742: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 17:13:04.577749: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 17:13:04.577755: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 17:13:05.000097: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties:\cf1\highlight1 
\par \cf0\highlight0 name: GeForce GTX TITAN X\cf1\highlight1 
\par \cf0\highlight0 major: 5 minor: 2 memoryClockRate (GHz) 1.076\cf1\highlight1 
\par \cf0\highlight0 pciBusID 0000:02:00.0\cf1\highlight1 
\par \cf0\highlight0 Total memory: 11.92GiB\cf1\highlight1 
\par \cf0\highlight0 Free memory: 11.81GiB\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 17:13:05.000159: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 17:13:05.000169: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 17:13:05.000180: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:02:00.0)\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 17:13:17.554522: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:02:00.0)\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Run id: resnet_cifar10\cf1\highlight1 
\par \cf0\highlight0 Log directory: /tmp/tflearn_logs/\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Preprocessing... Calculating mean over all dataset (this may take long)...\cf1\highlight1 
\par \cf0\highlight0 Mean: [ 0.49139968  0.48215841  0.44653091] (To avoid repetitive computation, add it to argument 'mean' of `add_featurewise_zero_center`)\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Training samples: 50000\cf1\highlight1 
\par \cf0\highlight0 Validation samples: 10000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 391  | total loss: \cf6 1.16231\cf0  | time: 47.057s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.16231 - acc: 0.5789 -- iter: 50000/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 [ Mon Oct 23 17:14:10 2017 ] Starting Avg ...\cf1\highlight1 
\par \cf0\highlight0 [0.44269999999999998]\cf1\highlight1 
\par \cf0\highlight0 [0.10000000000000001]\cf1\highlight1 
\par \cf0\highlight0 [ Mon Oct 23 17:14:30 2017 ] Avg Done...\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Run id: resnet_cifar10\cf1\highlight1 
\par \cf0\highlight0 Log directory: /tmp/tflearn_logs/\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Training samples: 50000\cf1\highlight1 
\par \cf0\highlight0 Validation samples: 10000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 500  | total loss: \cf6 1.34524\cf0  | time: 14.927s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.34524 - acc: 0.5153 | val_loss: 1.70384 - val_acc: 0.4303 -- iter: 13952/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 782  | total loss: \cf6 1.03996\cf0  | time: 46.370s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.03996 - acc: 0.6148 -- iter: 50000/50000\cf1\highlight1 
\par \cf0\highlight0 [ Mon Oct 23 17:15:19 2017 ] Starting Avg ...\cf1\highlight1 
\par \cf0\highlight0 [0.59999999999999998]\cf1\highlight1 
\par \cf0\highlight0 [0.099900000000000003]\cf1\highlight1 
\par \cf0\highlight0 [ Mon Oct 23 17:15:37 2017 ] Avg Done...\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Run id: resnet_cifar10\cf1\highlight1 
\par \cf0\highlight0 Log directory: /tmp/tflearn_logs/\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Training samples: 50000\cf1\highlight1 
\par \cf0\highlight0 Validation samples: 10000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 1000  | total loss: \cf6 0.92007\cf0  | time: 27.183s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 003 | loss: 0.92007 - acc: 0.6828 | val_loss: 1.14213 - val_acc: 0.6115 -- iter: 27904/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 1173  | total loss: \cf6 0.87571\cf0  | time: 46.182s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 003 | loss: 0.87571 - acc: 0.6886 -- iter: 50000/50000\cf1\highlight1 
\par \cf0\highlight0 [ Mon Oct 23 17:16:26 2017 ] Starting Avg ...\cf1\highlight1 
\par \cf0\highlight0 [0.64539999999999997]\cf1\highlight1 
\par \cf0\highlight0 [0.1128]\cf1\highlight1 
\par \cf0\highlight0 [ Mon Oct 23 17:16:43 2017 ] Avg Done...\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Run id: resnet_cifar10\cf1\highlight1 
\par \cf0\highlight0 Log directory: /tmp/tflearn_logs/\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Training samples: 50000\cf1\highlight1 
\par \cf0\highlight0 Validation samples: 10000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 1500  | total loss: \cf6 0.79214\cf0  | time: 39.466s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 004 | loss: 0.79214 - acc: 0.7235 | val_loss: 1.01134 - val_acc: 0.6641 -- iter: 41856/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 1564  | total loss: \cf6 0.75354\cf0  | time: 46.533s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 004 | loss: 0.75354 - acc: 0.7333 -- iter: 50000/50000\cf1\highlight1 
\par \cf0\highlight0 [ Mon Oct 23 17:17:32 2017 ] Starting Avg ...\cf1\highlight1 
\par \cf0\highlight0 [0.72019999999999995]\cf1\highlight1 
\par \cf0\highlight0 [0.1004]\cf1\highlight1 
\par \cf0\highlight0 [ Mon Oct 23 17:17:49 2017 ] Avg Done...\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Run id: resnet_cifar10\cf1\highlight1 
\par \cf0\highlight0 Log directory: /tmp/tflearn_logs/\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Training samples: 50000\cf1\highlight1 
\par \cf0\highlight0 Validation samples: 10000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 1955  | total loss: \cf6 0.69365\cf0  | time: 44.598s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 005 | loss: 0.69365 - acc: 0.7490 -- iter: 50000/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 [ Mon Oct 23 17:18:34 2017 ] Starting Avg ...\cf1\highlight1 
\par \cf0\highlight0 [0.71919999999999995]\cf1\highlight1 
\par \cf0\highlight0 [0.1249]\cf1\highlight1 
\par \cf0\highlight0 [ Mon Oct 23 17:18:52 2017 ] Avg Done...\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Run id: resnet_cifar10\cf1\highlight1 
\par \cf0\highlight0 Log directory: /tmp/tflearn_logs/\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Training samples: 50000\cf1\highlight1 
\par \cf0\highlight0 Validation samples: 10000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 2000  | total loss: \cf6 0.78311\cf0  | time: 8.035s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 006 | loss: 0.78311 - acc: 0.7363 | val_loss: 0.98709 - val_acc: 0.6732 -- iter: 05760/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 2346  | total loss: \cf6 0.66231\cf0  | time: 46.682s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 006 | loss: 0.66231 - acc: 0.7655 -- iter: 50000/50000\cf1\highlight1 
\par \cf0\highlight0 [ Mon Oct 23 17:19:41 2017 ] Starting Avg ...\cf1\highlight1 
\par \cf0\highlight0 [0.75449999999999995]\cf1\highlight1 
\par \cf0\highlight0 [0.31680000000000003]\cf1\highlight1 
\par \cf0\highlight0 [ Mon Oct 23 17:19:57 2017 ] Avg Done...\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Run id: resnet_cifar10\cf1\highlight1 
\par \cf0\highlight0 Log directory: /tmp/tflearn_logs/\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Training samples: 50000\cf1\highlight1 
\par \cf0\highlight0 Validation samples: 10000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 2500  | total loss: \cf6 0.70089\cf0  | time: 19.842s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 007 | loss: 0.70089 - acc: 0.7519 | val_loss: 0.90299 - val_acc: 0.7027 -- iter: 19712/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 2737  | total loss: \cf6 0.57830\cf0  | time: 46.318s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 007 | loss: 0.57830 - acc: 0.8023 -- iter: 50000/50000\cf1\highlight1 
\par \cf0\highlight0 [ Mon Oct 23 17:20:47 2017 ] Starting Avg ...\cf1\highlight1 
\par \cf0\highlight0 [0.77080000000000004]\cf1\highlight1 
\par \cf0\highlight0 [0.42809999999999998]\cf1\highlight1 
\par \cf0\highlight0 [ Mon Oct 23 17:21:05 2017 ] Avg Done...\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Run id: resnet_cifar10\cf1\highlight1 
\par \cf0\highlight0 Log directory: /tmp/tflearn_logs/\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Training samples: 50000\cf1\highlight1 
\par \cf0\highlight0 Validation samples: 10000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 3000  | total loss: \cf6 0.59976\cf0  | time: 32.080s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 008 | loss: 0.59976 - acc: 0.7951 | val_loss: 0.68593 - val_acc: 0.7705 -- iter: 33664/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 3128  | total loss: \cf6 0.58183\cf0  | time: 46.277s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 008 | loss: 0.58183 - acc: 0.7960 -- iter: 50000/50000\cf1\highlight1 
\par \cf0\highlight0 [ Mon Oct 23 17:21:54 2017 ] Starting Avg ...\cf1\highlight1 
\par \cf0\highlight0 [0.74970000000000003]\cf1\highlight1 
\par \cf0\highlight0 [0.41710000000000003]\cf1\highlight1 
\par \cf0\highlight0 [ Mon Oct 23 17:22:13 2017 ] Avg Done...\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Run id: resnet_cifar10\cf1\highlight1 
\par \cf0\highlight0 Log directory: /tmp/tflearn_logs/\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Training samples: 50000\cf1\highlight1 
\par \cf0\highlight0 Validation samples: 10000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 3500  | total loss: \cf6 0.61985\cf0  | time: 43.938s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 009 | loss: 0.61985 - acc: 0.8035 | val_loss: 0.75057 - val_acc: 0.7425 -- iter: 47616/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 3519  | total loss: \cf6 0.57815\cf0  | time: 45.990s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 009 | loss: 0.57815 - acc: 0.8048 -- iter: 50000/50000\cf1\highlight1 
\par \cf0\highlight0 [ Mon Oct 23 17:23:01 2017 ] Starting Avg ...\cf1\highlight1 
\par \cf0\highlight0 [0.76990000000000003]\cf1\highlight1 
\par \cf0\highlight0 [0.4773]\cf1\highlight1 
\par \cf0\highlight0 [ Mon Oct 23 17:23:19 2017 ] Avg Done...\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Run id: resnet_cifar10\cf1\highlight1 
\par \cf0\highlight0 Log directory: /tmp/tflearn_logs/\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Training samples: 50000\cf1\highlight1 
\par \cf0\highlight0 Validation samples: 10000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 3910  | total loss: \cf6 0.52969\cf0  | time: 44.719s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 010 | loss: 0.52969 - acc: 0.8129 -- iter: 50000/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 [ Mon Oct 23 17:24:04 2017 ] Starting Avg ...\cf1\highlight1 
\par \cf0\highlight0 [0.77749999999999997]\cf1\highlight1 
\par \cf0\highlight0 [0.4335]\cf1\highlight1 
\par \cf0\highlight0 [ Mon Oct 23 17:24:23 2017 ] Avg Done...\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Run id: resnet_cifar10\cf1\highlight1 
\par \cf0\highlight0 Log directory: /tmp/tflearn_logs/\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Training samples: 50000\cf1\highlight1 
\par \cf0\highlight0 Validation samples: 10000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 4000  | total loss: \cf6 0.60130\cf0  | time: 12.647s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 011 | loss: 0.60130 - acc: 0.7928 | val_loss: 0.62766 - val_acc: 0.7905 -- iter: 11520/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 4301  | total loss: \cf6 0.51181\cf0  | time: 46.376s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 011 | loss: 0.51181 - acc: 0.8197 -- iter: 50000/50000\cf1\highlight1 
\par \cf0\highlight0 [ Mon Oct 23 17:25:12 2017 ] Starting Avg ...\cf1\highlight1 
\par \cf0\highlight0 [0.77370000000000005]\cf1\highlight1 
\par \cf0\highlight0 [0.40639999999999998]\cf1\highlight1 
\par \cf0\highlight0 [ Mon Oct 23 17:25:31 2017 ] Avg Done...\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Run id: resnet_cifar10\cf1\highlight1 
\par \cf0\highlight0 Log directory: /tmp/tflearn_logs/\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Training samples: 50000\cf1\highlight1 
\par \cf0\highlight0 Validation samples: 10000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 4500  | total loss: \cf6 0.53577\cf0  | time: 25.076s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 012 | loss: 0.53577 - acc: 0.8103 | val_loss: 0.60446 - val_acc: 0.7997 -- iter: 25472/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 4692  | total loss: \cf6 0.52483\cf0  | time: 46.610s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 012 | loss: 0.52483 - acc: 0.8212 -- iter: 50000/50000\cf1\highlight1 
\par \cf0\highlight0 [ Mon Oct 23 17:26:20 2017 ] Starting Avg ...\cf1\highlight1 
\par \cf0\highlight0 [0.79590000000000005]\cf1\highlight1 
\par \cf0\highlight0 [0.62829999999999997]\cf1\highlight1 
\par \cf0\highlight0 [ Mon Oct 23 17:26:39 2017 ] Avg Done...\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Run id: resnet_cifar10\cf1\highlight1 
\par \cf0\highlight0 Log directory: /tmp/tflearn_logs/\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Training samples: 50000\cf1\highlight1 
\par \cf0\highlight0 Validation samples: 10000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 5000  | total loss: \cf6 0.53176\cf0  | time: 37.250s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 013 | loss: 0.53176 - acc: 0.8100 | val_loss: 0.67684 - val_acc: 0.7752 -- iter: 39424/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 5083  | total loss: \cf6 0.45050\cf0  | time: 46.507s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 013 | loss: 0.45050 - acc: 0.8500 -- iter: 50000/50000\cf1\highlight1 
\par \cf0\highlight0 [ Mon Oct 23 17:27:28 2017 ] Starting Avg ...\cf1\highlight1 
\par \cf0\highlight0 [0.82250000000000001]\cf1\highlight1 
\par \cf0\highlight0 [0.53029999999999999]\cf1\highlight1 
\par \cf0\highlight0 [ Mon Oct 23 17:27:48 2017 ] Avg Done...\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Run id: resnet_cifar10\cf1\highlight1 
\par \cf0\highlight0 Log directory: /tmp/tflearn_logs/\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Training samples: 50000\cf1\highlight1 
\par \cf0\highlight0 Validation samples: 10000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 5474  | total loss: \cf6 0.50206\cf0  | time: 44.453s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 014 | loss: 0.50206 - acc: 0.8259 -- iter: 50000/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 [ Mon Oct 23 17:28:33 2017 ] Starting Avg ...\cf1\highlight1 
\par \cf0\highlight0 [0.77969999999999995]\cf1\highlight1 
\par \cf0\highlight0 [0.48380000000000001]\cf1\highlight1 
\par \cf0\highlight0 [ Mon Oct 23 17:28:52 2017 ] Avg Done...\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Run id: resnet_cifar10\cf1\highlight1 
\par \cf0\highlight0 Log directory: /tmp/tflearn_logs/\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Training samples: 50000\cf1\highlight1 
\par \cf0\highlight0 Validation samples: 10000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 5500  | total loss: \cf6 0.51019\cf0  | time: 5.625s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 015 | loss: 0.51019 - acc: 0.8171 | val_loss: 0.72675 - val_acc: 0.7595 -- iter: 03328/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 5865  | total loss: \cf6 0.45925\cf0  | time: 46.109s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 015 | loss: 0.45925 - acc: 0.8443 -- iter: 50000/50000\cf1\highlight1 
\par \cf0\highlight0 [ Mon Oct 23 17:29:41 2017 ] Starting Avg ...\cf1\highlight1 
\par \cf0\highlight0 [0.81159999999999999]\cf1\highlight1 
\par \cf0\highlight0 [0.70899999999999996]\cf1\highlight1 
\par \cf0\highlight0 [ Mon Oct 23 17:30:00 2017 ] Avg Done...\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Run id: resnet_cifar10\cf1\highlight1 
\par \cf0\highlight0 Log directory: /tmp/tflearn_logs/\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Training samples: 50000\cf1\highlight1 
\par \cf0\highlight0 Validation samples: 10000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 6000  | total loss: \cf6 0.51025\cf0  | time: 18.105s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 016 | loss: 0.51025 - acc: 0.8322 | val_loss: 0.64874 - val_acc: 0.7894 -- iter: 17280/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 ^Caining Step: 6188  | total loss: \cf6 0.47719\cf0  | time: 39.256s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 016 | loss: 0.47719 - acc: 0.8313 -- iter: 41344/50000\cf1\highlight1 
\par \cf0\highlight0   Traceback (most recent call last):\cf1\highlight1 
\par \cf0\highlight0   File "residual_network_cifar10.py", line 74, in <module>\cf1\highlight1 
\par \cf0\highlight0     run_id='resnet_cifar10')\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tflearn/models/dnn.py", line 216, in fit\cf1\highlight1 
\par \cf0\highlight0     callbacks=callbacks)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tflearn/helpers/trainer.py", line 338, in fit\cf1\highlight1 
\par \cf0\highlight0     show_metric)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tflearn/helpers/trainer.py", line 817, in _train\cf1\highlight1 
\par \cf0\highlight0     feed_batch)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run\cf1\highlight1 
\par \cf0\highlight0     run_metadata_ptr)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1124, in _run\cf1\highlight1 
\par \cf0\highlight0     feed_dict_tensor, options, run_metadata)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1321, in _do_run\cf1\highlight1 
\par \cf0\highlight0     options, run_metadata)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1327, in _do_call\cf1\highlight1 
\par \cf0\highlight0     return fn(*args)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1306, in _run_fn\cf1\highlight1 
\par \cf0\highlight0     status, run_metadata)\cf1\highlight1 
\par \cf0\highlight0 KeyboardInterrupt\cf1\highlight1 
\par \cf0\highlight0 (tf) xdong@gpu2:/data/xdong/codes/avg_para$ CUDA_VISIBLE_DEVICES=0 python residual_network_cifar10.py\cf1\highlight1 
\par \cf0\highlight0 Couldn't import dot_parser, loading of dot files will not be possible.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 17:33:36.662614: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could spePU computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 17:33:36.662656: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could spePU computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 17:33:36.662665: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 17:33:36.662671: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 17:33:36.662678: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 17:33:37.073671: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties:\cf1\highlight1 
\par \cf0\highlight0 name: GeForce GTX TITAN X\cf1\highlight1 
\par \cf0\highlight0 major: 5 minor: 2 memoryClockRate (GHz) 1.076\cf1\highlight1 
\par \cf0\highlight0 pciBusID 0000:02:00.0\cf1\highlight1 
\par \cf0\highlight0 Total memory: 11.92GiB\cf1\highlight1 
\par \cf0\highlight0 Free memory: 11.81GiB\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 17:33:37.073773: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 17:33:37.073793: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 17:33:37.073815: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:02:00.0)\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 17:33:57.816170: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:02:00.0)\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Run id: resnet_cifar10\cf1\highlight1 
\par \cf0\highlight0 Log directory: /tmp/tflearn_logs/\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Preprocessing... Calculating mean over all dataset (this may take long)...\cf1\highlight1 
\par \cf0\highlight0 Mean: [ 0.49139968  0.48215841  0.44653091] (To avoid repetitive computation, add it to argument 'mean' of `add_featurewise_zero_center`)\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Training samples: 50000\cf1\highlight1 
\par \cf0\highlight0 Validation samples: 10000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 391  | total loss: \cf6 1.28832\cf0  | time: 49.860s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.28832 - acc: 0.5413 -- iter: 50000/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 [ Mon Oct 23 17:34:53 2017 ] Starting Avg ...\cf1\highlight1 
\par \cf0\highlight0 [0.45466000001907347]\cf1\highlight1 
\par \cf0\highlight0 [0.10000000000715256]\cf1\highlight1 
\par \cf0\highlight0 [ Mon Oct 23 17:35:25 2017 ] Avg Done...\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Run id: resnet_cifar10\cf1\highlight1 
\par \cf0\highlight0 Log directory: /tmp/tflearn_logs/\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Training samples: 50000\cf1\highlight1 
\par \cf0\highlight0 Validation samples: 10000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 500  | total loss: \cf6 1.37143\cf0  | time: 15.408s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.37143 - acc: 0.4899 | val_loss: 1.52254 - val_acc: 0.4591 -- iter: 13952/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 782  | total loss: \cf6 1.07337\cf0  | time: 47.286s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.07337 - acc: 0.6283 -- iter: 50000/50000\cf1\highlight1 
\par \cf0\highlight0 [ Mon Oct 23 17:36:15 2017 ] Starting Avg ...\cf1\highlight1 
\par \cf0\highlight0 [0.57082000003814692]\cf1\highlight1 
\par \cf0\highlight0 [0.10008000000476837]\cf1\highlight1 
\par \cf0\highlight0 [ Mon Oct 23 17:36:46 2017 ] Avg Done...\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Run id: resnet_cifar10\cf1\highlight1 
\par \cf0\highlight0 Log directory: /tmp/tflearn_logs/\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Training samples: 50000\cf1\highlight1 
\par \cf0\highlight0 Validation samples: 10000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 1000  | total loss: \cf6 1.00184\cf0  | time: 27.765s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 003 | loss: 1.00184 - acc: 0.6247 | val_loss: 1.31246 - val_acc: 0.5865 -- iter: 27904/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 1173  | total loss: \cf6 0.87087\cf0  | time: 47.121s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 003 | loss: 0.87087 - acc: 0.6905 -- iter: 50000/50000\cf1\highlight1 
\par \cf0\highlight0 [ Mon Oct 23 17:37:36 2017 ] Starting Avg ...\cf1\highlight1 
\par \cf0\highlight0 [0.68332000003814697]\cf1\highlight1 
\par \cf0\highlight0 [0.14804000002384185]\cf1\highlight1 
\par \cf0\highlight0 [ Mon Oct 23 17:38:08 2017 ] Avg Done...\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Run id: resnet_cifar10\cf1\highlight1 
\par \cf0\highlight0 Log directory: /tmp/tflearn_logs/\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Training samples: 50000\cf1\highlight1 
\par \cf0\highlight0 Validation samples: 10000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 ^Caining Step: 1471  | total loss: \cf6 0.82552\cf0  | time: 34.863s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 004 | loss: 0.82552 - acc: 0.7208 -- iter: 38144/50000\cf1\highlight1 
\par \cf0\highlight0   --\cf1\highlight1 
\par \cf0\highlight0 Traceback (most recent call last):\cf1\highlight1 
\par \cf0\highlight0   File "residual_network_cifar10.py", line 74, in <module>\cf1\highlight1 
\par \cf0\highlight0     run_id='resnet_cifar10')\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tflearn/models/dnn.py", line 216, in fit\cf1\highlight1 
\par \cf0\highlight0     callbacks=callbacks)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tflearn/helpers/trainer.py", line 338, in fit\cf1\highlight1 
\par \cf0\highlight0     show_metric)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tflearn/helpers/trainer.py", line 817, in _train\cf1\highlight1 
\par \cf0\highlight0     feed_batch)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run\cf1\highlight1 
\par \cf0\highlight0     run_metadata_ptr)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1124, in _run\cf1\highlight1 
\par \cf0\highlight0     feed_dict_tensor, options, run_metadata)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1321, in _do_run\cf1\highlight1 
\par \cf0\highlight0     options, run_metadata)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1327, in _do_call\cf1\highlight1 
\par \cf0\highlight0     return fn(*args)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1306, in _run_fn\cf1\highlight1 
\par \cf0\highlight0     status, run_metadata)\cf1\highlight1 
\par \cf0\highlight0 KeyboardInterrupt\cf1\highlight1 
\par \cf0\highlight0 (tf) xdong@gpu2:/data/xdong/codes/avg_para$ CUDA_VISIBLE_DEVICES=0 python residual_network_cifar10.py\cf1\highlight1 
\par \cf0\highlight0 Couldn't import dot_parser, loading of dot files will not be possible.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 17:49:09.213655: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could spePU computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 17:49:09.213689: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could spePU computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 17:49:09.213697: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 17:49:09.213704: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 17:49:09.213711: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 17:49:09.820790: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties:\cf1\highlight1 
\par \cf0\highlight0 name: GeForce GTX TITAN X\cf1\highlight1 
\par \cf0\highlight0 major: 5 minor: 2 memoryClockRate (GHz) 1.076\cf1\highlight1 
\par \cf0\highlight0 pciBusID 0000:02:00.0\cf1\highlight1 
\par \cf0\highlight0 Total memory: 11.92GiB\cf1\highlight1 
\par \cf0\highlight0 Free memory: 11.81GiB\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 17:49:09.820836: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 17:49:09.820846: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 17:49:09.820858: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:02:00.0)\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 17:49:30.163656: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:02:00.0)\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Run id: resnet_cifar10\cf1\highlight1 
\par \cf0\highlight0 Log directory: /tmp/tflearn_logs/\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Preprocessing... Calculating mean over all dataset (this may take long)...\cf1\highlight1 
\par \cf0\highlight0 Mean: [ 0.49139968  0.48215841  0.44653091] (To avoid repetitive computation, add it to argument 'mean' of `add_featurewise_zero_center`)\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Training samples: 50000\cf1\highlight1 
\par \cf0\highlight0 Validation samples: 10000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 391  | total loss: \cf6 1.25350\cf0  | time: 48.813s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.25350 - acc: 0.5468 -- iter: 50000/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 [ Mon Oct 23 17:50:24 2017 ] Starting Avg ...\cf1\highlight1 
\par \cf0\highlight0 [0.49894000000953675]\cf1\highlight1 
\par \cf0\highlight0 Traceback (most recent call last):\cf1\highlight1 
\par \cf0\highlight0   File "residual_network_cifar10.py", line 79, in <module>\cf1\highlight1 
\par \cf0\highlight0     tools.do_avg('ResidualBlock', 10, curr_sess, model)\cf1\highlight1 
\par \cf0\highlight0   File "/data/xdong/codes/avg_para/tools.py", line 43, in do_avg\cf1\highlight1 
\par \cf0\highlight0     avg_paras(name_list,sess,model)\cf1\highlight1 
\par \cf0\highlight0   File "/data/xdong/codes/avg_para/tools.py", line 25, in avg_paras\cf1\highlight1 
\par \cf0\highlight0     value_list = map(model.get_weights, tensor_list)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tflearn/models/dnn.py", line 334, in get_weights\cf1\highlight1 
\par \cf0\highlight0     return weight_tensor.eval(self.trainer.session)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 541, in eval\cf1\highlight1 
\par \cf0\highlight0     return _eval_using_default_session(self, feed_dict, self.graph, session)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 4071, in _eval_using_default_session\cf1\highlight1 
\par \cf0\highlight0     raise ValueError("Cannot evaluate tensor using `eval()`: No default "\cf1\highlight1 
\par \cf0\highlight0 ValueError: Cannot evaluate tensor using `eval()`: No default session is registered. Use `with sess.as_default()` or pass an explicit session to `eval(session=sess)`\cf1\highlight1 
\par \cf0\highlight0 (tf) xdong@gpu2:/data/xdong/codes/avg_para$ CUDA_VISIBLE_DEVICES=0 python residual_network_cifar10.py\cf1\highlight1 
\par \cf0\highlight0 Couldn't import dot_parser, loading of dot files will not be possible.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 18:05:52.307401: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could spePU computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 18:05:52.307433: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could spePU computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 18:05:52.307440: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 18:05:52.307447: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 18:05:52.307453: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 18:05:52.722311: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties:\cf1\highlight1 
\par \cf0\highlight0 name: GeForce GTX TITAN X\cf1\highlight1 
\par \cf0\highlight0 major: 5 minor: 2 memoryClockRate (GHz) 1.076\cf1\highlight1 
\par \cf0\highlight0 pciBusID 0000:02:00.0\cf1\highlight1 
\par \cf0\highlight0 Total memory: 11.92GiB\cf1\highlight1 
\par \cf0\highlight0 Free memory: 11.81GiB\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 18:05:52.722390: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 18:05:52.722404: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 18:05:52.722421: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:02:00.0)\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 18:06:12.424998: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:02:00.0)\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Run id: resnet_cifar10\cf1\highlight1 
\par \cf0\highlight0 Log directory: /tmp/tflearn_logs/\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Preprocessing... Calculating mean over all dataset (this may take long)...\cf1\highlight1 
\par \cf0\highlight0 Mean: [ 0.49139968  0.48215841  0.44653091] (To avoid repetitive computation, add it to argument 'mean' of `add_featurewise_zero_center`)\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Training samples: 50000\cf1\highlight1 
\par \cf0\highlight0 Validation samples: 10000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 391  | total loss: \cf6 1.23780\cf0  | time: 50.224s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.23780 - acc: 0.5477 -- iter: 50000/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 [ Mon Oct 23 18:07:08 2017 ] Starting Avg ...\cf1\highlight1 
\par \cf0\highlight0 [0.498180000038147]\cf1\highlight1 
\par \cf0\highlight0 Traceback (most recent call last):\cf1\highlight1 
\par \cf0\highlight0   File "residual_network_cifar10.py", line 79, in <module>\cf1\highlight1 
\par \cf0\highlight0     do_avg('ResidualBlock', 10, curr_sess, model)\cf1\highlight1 
\par \cf0\highlight0 NameError: name 'do_avg' is not defined\cf1\highlight1 
\par \cf0\highlight0 (tf) xdong@gpu2:/data/xdong/codes/avg_para$ CUDA_VISIBLE_DEVICES=0 python residual_network_cifar10.py\cf1\highlight1 
\par \cf0\highlight0 Couldn't import dot_parser, loading of dot files will not be possible.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 18:10:19.080870: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could spePU computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 18:10:19.080911: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could spePU computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 18:10:19.080919: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 18:10:19.080926: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 18:10:19.080932: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 18:10:19.503036: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties:\cf1\highlight1 
\par \cf0\highlight0 name: GeForce GTX TITAN X\cf1\highlight1 
\par \cf0\highlight0 major: 5 minor: 2 memoryClockRate (GHz) 1.076\cf1\highlight1 
\par \cf0\highlight0 pciBusID 0000:02:00.0\cf1\highlight1 
\par \cf0\highlight0 Total memory: 11.92GiB\cf1\highlight1 
\par \cf0\highlight0 Free memory: 11.81GiB\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 18:10:19.503118: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 18:10:19.503133: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 18:10:19.503149: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:02:00.0)\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 18:10:35.245265: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:02:00.0)\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Run id: resnet_cifar10\cf1\highlight1 
\par \cf0\highlight0 Log directory: /tmp/tflearn_logs/\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Preprocessing... Calculating mean over all dataset (this may take long)...\cf1\highlight1 
\par \cf0\highlight0 Mean: [ 0.49139968  0.48215841  0.44653091] (To avoid repetitive computation, add it to argument 'mean' of `add_featurewise_zero_center`)\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Training samples: 50000\cf1\highlight1 
\par \cf0\highlight0 Validation samples: 10000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 391  | total loss: \cf6 1.23502\cf0  | time: 47.263s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.23502 - acc: 0.5497 -- iter: 50000/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 [ Mon Oct 23 18:11:27 2017 ] Starting Avg ...\cf1\highlight1 
\par \cf0\highlight0 [0.52873999996185306]\cf1\highlight1 
\par \cf0\highlight0 Traceback (most recent call last):\cf1\highlight1 
\par \cf0\highlight0   File "residual_network_cifar10.py", line 109, in <module>\cf1\highlight1 
\par \cf0\highlight0     do_avg('ResidualBlock', 10, curr_sess, model)\cf1\highlight1 
\par \cf0\highlight0   File "residual_network_cifar10.py", line 47, in do_avg\cf1\highlight1 
\par \cf0\highlight0     avg_paras(name_list,sess,model)\cf1\highlight1 
\par \cf0\highlight0   File "residual_network_cifar10.py", line 29, in avg_paras\cf1\highlight1 
\par \cf0\highlight0     value_list = map(model.get_weights, tensor_list)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tflearn/models/dnn.py", line 334, in get_weights\cf1\highlight1 
\par \cf0\highlight0     return weight_tensor.eval(self.trainer.session)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 541, in eval\cf1\highlight1 
\par \cf0\highlight0     return _eval_using_default_session(self, feed_dict, self.graph, session)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 4071, in _eval_using_default_session\cf1\highlight1 
\par \cf0\highlight0     raise ValueError("Cannot evaluate tensor using `eval()`: No default "\cf1\highlight1 
\par \cf0\highlight0 ValueError: Cannot evaluate tensor using `eval()`: No default session is registered. Use `with sess.as_default()` or pass an explicit session to `eval(session=sess)`\cf1\highlight1 
\par \cf0\highlight0 (tf) xdong@gpu2:/data/xdong/codes/avg_para$ CUDA_VISIBLE_DEVICES=0 python residual_network_cifar10.py\cf1\highlight1 
\par \cf0\highlight0 Couldn't import dot_parser, loading of dot files will not be possible.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 18:16:42.451183: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could spePU computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 18:16:42.451227: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could spePU computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 18:16:42.451235: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 18:16:42.451242: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 18:16:42.451249: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 18:16:43.193397: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties:\cf1\highlight1 
\par \cf0\highlight0 name: GeForce GTX TITAN X\cf1\highlight1 
\par \cf0\highlight0 major: 5 minor: 2 memoryClockRate (GHz) 1.076\cf1\highlight1 
\par \cf0\highlight0 pciBusID 0000:02:00.0\cf1\highlight1 
\par \cf0\highlight0 Total memory: 11.92GiB\cf1\highlight1 
\par \cf0\highlight0 Free memory: 11.81GiB\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 18:16:43.193500: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 18:16:43.193523: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 18:16:43.193545: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:02:00.0)\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 18:16:50.633560: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:02:00.0)\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Run id: resnet_cifar10\cf1\highlight1 
\par \cf0\highlight0 Log directory: /tmp/tflearn_logs/\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Preprocessing... Calculating mean over all dataset (this may take long)...\cf1\highlight1 
\par \cf0\highlight0 Mean: [ 0.49139968  0.48215841  0.44653091] (To avoid repetitive computation, add it to argument 'mean' of `add_featurewise_zero_center`)\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Training samples: 50000\cf1\highlight1 
\par \cf0\highlight0 Validation samples: 10000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 391  | total loss: \cf6 1.24788\cf0  | time: 50.931s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.24788 - acc: 0.5526 -- iter: 50000/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 [ Mon Oct 23 18:17:47 2017 ] Starting Avg ...\cf1\highlight1 
\par \cf0\highlight0 [0.47839999996185301]\cf1\highlight1 
\par \cf0\highlight0 Traceback (most recent call last):\cf1\highlight1 
\par \cf0\highlight0   File "residual_network_cifar10.py", line 113, in <module>\cf1\highlight1 
\par \cf0\highlight0     do_avg('ResidualBlock', 10, curr_sess)\cf1\highlight1 
\par \cf0\highlight0   File "residual_network_cifar10.py", line 98, in do_avg\cf1\highlight1 
\par \cf0\highlight0     avg_paras(name_list,sess)\cf1\highlight1 
\par \cf0\highlight0   File "residual_network_cifar10.py", line 80, in avg_paras\cf1\highlight1 
\par \cf0\highlight0     value_list = map(model.get_weights, tensor_list)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tflearn/models/dnn.py", line 334, in get_weights\cf1\highlight1 
\par \cf0\highlight0     return weight_tensor.eval(self.trainer.session)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 541, in eval\cf1\highlight1 
\par \cf0\highlight0     return _eval_using_default_session(self, feed_dict, self.graph, session)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 4071, in _eval_using_default_session\cf1\highlight1 
\par \cf0\highlight0     raise ValueError("Cannot evaluate tensor using `eval()`: No default "\cf1\highlight1 
\par \cf0\highlight0 ValueError: Cannot evaluate tensor using `eval()`: No default session is registered. Use `with sess.as_default()` or pass an explicit session to `eval(session=sess)`\cf1\highlight1 
\par \cf0\highlight0 (tf) xdong@gpu2:/data/xdong/codes/avg_para$ CUDA_VISIBLE_DEVICES=0 python residual_network_cifar10.py\cf1\highlight1 
\par \cf0\highlight0 Couldn't import dot_parser, loading of dot files will not be possible.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 18:30:16.439132: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could spePU computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 18:30:16.439172: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could spePU computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 18:30:16.439180: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 18:30:16.439187: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 18:30:16.439194: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 18:30:16.843428: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties:\cf1\highlight1 
\par \cf0\highlight0 name: GeForce GTX TITAN X\cf1\highlight1 
\par \cf0\highlight0 major: 5 minor: 2 memoryClockRate (GHz) 1.076\cf1\highlight1 
\par \cf0\highlight0 pciBusID 0000:02:00.0\cf1\highlight1 
\par \cf0\highlight0 Total memory: 11.92GiB\cf1\highlight1 
\par \cf0\highlight0 Free memory: 11.81GiB\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 18:30:16.843493: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 18:30:16.843505: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 18:30:16.843520: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:02:00.0)\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 18:30:30.998159: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:02:00.0)\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Run id: resnet_cifar10\cf1\highlight1 
\par \cf0\highlight0 Log directory: /tmp/tflearn_logs/\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Preprocessing... Calculating mean over all dataset (this may take long)...\cf1\highlight1 
\par \cf0\highlight0 Mean: [ 0.49139968  0.48215841  0.44653091] (To avoid repetitive computation, add it to argument 'mean' of `add_featurewise_zero_center`)\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Training samples: 50000\cf1\highlight1 
\par \cf0\highlight0 Validation samples: 10000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 391  | total loss: \cf6 1.25961\cf0  | time: 48.491s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.25961 - acc: 0.5558 -- iter: 50000/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 [ Mon Oct 23 18:31:25 2017 ] Starting Avg ...\cf1\highlight1 
\par \cf0\highlight0 [0.53074000007629396]\cf1\highlight1 
\par \cf0\highlight0 Traceback (most recent call last):\cf1\highlight1 
\par \cf0\highlight0   File "residual_network_cifar10.py", line 113, in <module>\cf1\highlight1 
\par \cf0\highlight0     do_avg('ResidualBlock', 10, curr_sess)\cf1\highlight1 
\par \cf0\highlight0   File "residual_network_cifar10.py", line 98, in do_avg\cf1\highlight1 
\par \cf0\highlight0     avg_paras(name_list,sess)\cf1\highlight1 
\par \cf0\highlight0   File "residual_network_cifar10.py", line 80, in avg_paras\cf1\highlight1 
\par \cf0\highlight0     value_list = map(model.get_weights, tensor_list)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tflearn/models/dnn.py", line 334, in get_weights\cf1\highlight1 
\par \cf0\highlight0     return weight_tensor.eval(self.trainer.session)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 541, in eval\cf1\highlight1 
\par \cf0\highlight0     return _eval_using_default_session(self, feed_dict, self.graph, session)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 4071, in _eval_using_default_session\cf1\highlight1 
\par \cf0\highlight0     raise ValueError("Cannot evaluate tensor using `eval()`: No default "\cf1\highlight1 
\par \cf0\highlight0 ValueError: Cannot evaluate tensor using `eval()`: No default session is registered. Use `with sess.as_default()` or pass an explicit session to `eval(session=sess)`\cf1\highlight1 
\par \cf0\highlight0 (tf) xdong@gpu2:/data/xdong/codes/avg_para$ CUDA_VISIBLE_DEVICES=0 python residual_network_cifar10.py\cf1\highlight1 
\par \cf0\highlight0 Couldn't import dot_parser, loading of dot files will not be possible.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 18:35:31.922938: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could spePU computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 18:35:31.922980: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could spePU computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 18:35:31.922987: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 18:35:31.922994: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 18:35:31.923000: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 18:35:32.430230: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties:\cf1\highlight1 
\par \cf0\highlight0 name: GeForce GTX TITAN X\cf1\highlight1 
\par \cf0\highlight0 major: 5 minor: 2 memoryClockRate (GHz) 1.076\cf1\highlight1 
\par \cf0\highlight0 pciBusID 0000:02:00.0\cf1\highlight1 
\par \cf0\highlight0 Total memory: 11.92GiB\cf1\highlight1 
\par \cf0\highlight0 Free memory: 11.81GiB\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 18:35:32.430305: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 18:35:32.430319: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 18:35:32.430336: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:02:00.0)\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 18:35:55.474522: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:02:00.0)\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Run id: resnet_cifar10\cf1\highlight1 
\par \cf0\highlight0 Log directory: /tmp/tflearn_logs/\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Preprocessing... Calculating mean over all dataset (this may take long)...\cf1\highlight1 
\par \cf0\highlight0 Mean: [ 0.49139968  0.48215841  0.44653091] (To avoid repetitive computation, add it to argument 'mean' of `add_featurewise_zero_center`)\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Training samples: 50000\cf1\highlight1 
\par \cf0\highlight0 Validation samples: 10000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 391  | total loss: \cf6 1.17749\cf0  | time: 48.596s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.17749 - acc: 0.5776 -- iter: 50000/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 [ Mon Oct 23 18:36:49 2017 ] Starting Avg ...\cf1\highlight1 
\par \cf0\highlight0 [0.47292000000000001]\cf1\highlight1 
\par \cf0\highlight0 Traceback (most recent call last):\cf1\highlight1 
\par \cf0\highlight0   File "residual_network_cifar10.py", line 116, in <module>\cf1\highlight1 
\par \cf0\highlight0     do_avg('ResidualBlock', 10, curr_sess)\cf1\highlight1 
\par \cf0\highlight0   File "residual_network_cifar10.py", line 101, in do_avg\cf1\highlight1 
\par \cf0\highlight0     avg_paras(name_list,sess)\cf1\highlight1 
\par \cf0\highlight0   File "residual_network_cifar10.py", line 83, in avg_paras\cf1\highlight1 
\par \cf0\highlight0     value_list = map(model.get_weights, tensor_list)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tflearn/models/dnn.py", line 334, in get_weights\cf1\highlight1 
\par \cf0\highlight0     return weight_tensor.eval(self.trainer.session)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 541, in eval\cf1\highlight1 
\par \cf0\highlight0     return _eval_using_default_session(self, feed_dict, self.graph, session)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 4071, in _eval_using_default_session\cf1\highlight1 
\par \cf0\highlight0     raise ValueError("Cannot evaluate tensor using `eval()`: No default "\cf1\highlight1 
\par \cf0\highlight0 ValueError: Cannot evaluate tensor using `eval()`: No default session is registered. Use `with sess.as_default()` or pass an explicit session to `eval(session=sess)`\cf1\highlight1 
\par \cf0\highlight0 (tf) xdong@gpu2:/data/xdong/codes/avg_para$ CUDA_VISIBLE_DEVICES=0 python residual_network_cifar10.py\cf1\highlight1 
\par \cf0\highlight0 Couldn't import dot_parser, loading of dot files will not be possible.\cf1\highlight1 
\par \cf0\highlight0 ^CTraceback (most recent call last):\cf1\highlight1 
\par \cf0\highlight0   File "residual_network_cifar10.py", line 20, in <module>\cf1\highlight1 
\par \cf0\highlight0     import tflearn\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tflearn/__init__.py", line 4, in <module>\cf1\highlight1 
\par \cf0\highlight0     from . import config\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tflearn/config.py", line 5, in <module>\cf1\highlight1 
\par \cf0\highlight0     from .variables import variable\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tflearn/variables.py", line 7, in <module>\cf1\highlight1 
\par \cf0\highlight0     from tensorflow.contrib.framework.python.ops import add_arg_scope as contrib_add_arg_scope\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tensorflow/contrib/__init__.py", line 66, in <module>\cf1\highlight1 
\par \cf0\highlight0     from tensorflow.contrib import tensor_forest\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tensorflow/contrib/tensor_forest/__init__.py", line 21, in <module>\cf1\highlight1 
\par \cf0\highlight0     from tensorflow.contrib.tensor_forest.client import *\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tensorflow/contrib/tensor_forest/client/__init__.py", line 22, in <module>\cf1\highlight1 
\par \cf0\highlight0     from tensorflow.contrib.tensor_forest.client import random_forest\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tensorflow/contrib/tensor_forest/client/random_forest.py", line 27, in <module>\cf1\highlight1 
\par \cf0\highlight0     from tensorflow.contrib.tensor_forest.python import tensor_forest\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tensorflow/contrib/tensor_forest/python/__init__.py", line 23, in <module>\cf1\highlight1 
\par \cf0\highlight0     from tensorflow.contrib.tensor_forest.python import tensor_forest_v4\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tensorflow/contrib/tensor_forest/python/tensor_forest_v4.py", line 24, in <module>\cf1\highlight1 
\par \cf0\highlight0     from tensorflow.contrib.decision_trees.proto import generic_tree_model_pb2 as _tree_proto\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tensorflow/contrib/decision_trees/proto/generic_tree_model_pb2.py", line 17, in <module>\cf1\highlight1 
\par \cf0\highlight0     from google.protobuf import wrappers_pb2 as google_dot_protobuf_dot_wrappers__pb2\cf1\highlight1 
\par \cf0\highlight0 KeyboardInterrupt\cf1\highlight1 
\par \cf0\highlight0 (tf) xdong@gpu2:/data/xdong/codes/avg_para$ CUDA_VISIBLE_DEVICES=0 python residual_network_cifar10.py\cf1\highlight1 
\par \cf0\highlight0 Couldn't import dot_parser, loading of dot files will not be possible.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 18:43:35.593673: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could spePU computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 18:43:35.593727: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could spePU computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 18:43:35.593735: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 18:43:35.593742: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 18:43:35.593751: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 18:43:36.134693: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties:\cf1\highlight1 
\par \cf0\highlight0 name: GeForce GTX TITAN X\cf1\highlight1 
\par \cf0\highlight0 major: 5 minor: 2 memoryClockRate (GHz) 1.076\cf1\highlight1 
\par \cf0\highlight0 pciBusID 0000:02:00.0\cf1\highlight1 
\par \cf0\highlight0 Total memory: 11.92GiB\cf1\highlight1 
\par \cf0\highlight0 Free memory: 11.81GiB\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 18:43:36.134787: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 18:43:36.134808: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 18:43:36.134826: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:02:00.0)\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 18:43:56.795830: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:02:00.0)\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Run id: resnet_cifar10\cf1\highlight1 
\par \cf0\highlight0 Log directory: /tmp/tflearn_logs/\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Preprocessing... Calculating mean over all dataset (this may take long)...\cf1\highlight1 
\par \cf0\highlight0 Mean: [ 0.49139968  0.48215841  0.44653091] (To avoid repetitive computation, add it to argument 'mean' of `add_featurewise_zero_center`)\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Training samples: 50000\cf1\highlight1 
\par \cf0\highlight0 Validation samples: 10000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 391  | total loss: \cf6 1.29208\cf0  | time: 50.229s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.29208 - acc: 0.5265 -- iter: 50000/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 [ Mon Oct 23 18:44:52 2017 ] Starting Avg ...\cf1\highlight1 
\par \cf0\highlight0 [0.48828000003814698]\cf1\highlight1 
\par \cf0\highlight0 <tflearn.models.dnn.DNN object at 0x7ff02166dc50>\cf1\highlight1 
\par \cf0\highlight0 <tensorflow.python.client.session.Session object at 0x7ff02163bd10>\cf1\highlight1 
\par \cf0\highlight0 Traceback (most recent call last):\cf1\highlight1 
\par \cf0\highlight0   File "residual_network_cifar10.py", line 117, in <module>\cf1\highlight1 
\par \cf0\highlight0     do_avg('ResidualBlock', 10, curr_sess)\cf1\highlight1 
\par \cf0\highlight0   File "residual_network_cifar10.py", line 102, in do_avg\cf1\highlight1 
\par \cf0\highlight0     avg_paras(name_list,sess)\cf1\highlight1 
\par \cf0\highlight0   File "residual_network_cifar10.py", line 84, in avg_paras\cf1\highlight1 
\par \cf0\highlight0     value_list = map(model.get_weights, tensor_list)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tflearn/models/dnn.py", line 334, in get_weights\cf1\highlight1 
\par \cf0\highlight0     return weight_tensor.eval(self.trainer.session)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 541, in eval\cf1\highlight1 
\par \cf0\highlight0     return _eval_using_default_session(self, feed_dict, self.graph, session)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 4071, in _eval_using_default_session\cf1\highlight1 
\par \cf0\highlight0     raise ValueError("Cannot evaluate tensor using `eval()`: No default "\cf1\highlight1 
\par \cf0\highlight0 ValueError: Cannot evaluate tensor using `eval()`: No default session is registered. Use `with sess.as_default()` or pass an explicit session to `eval(session=sess)`\cf1\highlight1 
\par \cf0\highlight0 (tf) xdong@gpu2:/data/xdong/codes/avg_para$ CUDA_VISIBLE_DEVICES=0 python residual_network_cifar10.py\cf1\highlight1 
\par \cf0\highlight0 Couldn't import dot_parser, loading of dot files will not be possible.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 19:15:37.573814: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could spePU computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 19:15:37.573845: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could spePU computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 19:15:37.573853: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 19:15:37.573860: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 19:15:37.573866: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 19:15:38.371860: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties:\cf1\highlight1 
\par \cf0\highlight0 name: GeForce GTX TITAN X\cf1\highlight1 
\par \cf0\highlight0 major: 5 minor: 2 memoryClockRate (GHz) 1.076\cf1\highlight1 
\par \cf0\highlight0 pciBusID 0000:02:00.0\cf1\highlight1 
\par \cf0\highlight0 Total memory: 11.92GiB\cf1\highlight1 
\par \cf0\highlight0 Free memory: 11.81GiB\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 19:15:38.371952: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 19:15:38.371969: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 19:15:38.371987: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:02:00.0)\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 19:15:55.643744: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:02:00.0)\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Run id: resnet_cifar10\cf1\highlight1 
\par \cf0\highlight0 Log directory: /tmp/tflearn_logs/\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Preprocessing... Calculating mean over all dataset (this may take long)...\cf1\highlight1 
\par \cf0\highlight0 Mean: [ 0.49139968  0.48215841  0.44653091] (To avoid repetitive computation, add it to argument 'mean' of `add_featurewise_zero_center`)\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Training samples: 50000\cf1\highlight1 
\par \cf0\highlight0 Validation samples: 10000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 391  | total loss: \cf6 1.24936\cf0  | time: 49.291s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.24936 - acc: 0.5459 -- iter: 50000/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 [ Mon Oct 23 19:16:51 2017 ] Starting Avg ...\cf1\highlight1 
\par \cf0\highlight0 [0.53149999999999997]\cf1\highlight1 
\par \cf0\highlight0 <tflearn.models.dnn.DNN object at 0x7f8972138c90>\cf1\highlight1 
\par \cf0\highlight0 <tensorflow.python.client.session.Session object at 0x7f89720fcd50>\cf1\highlight1 
\par \cf0\highlight0 <tensorflow.python.client.session.Session object at 0x7f89720fcd50>\cf1\highlight1 
\par \cf0\highlight0 Traceback (most recent call last):\cf1\highlight1 
\par \cf0\highlight0   File "residual_network_cifar10.py", line 119, in <module>\cf1\highlight1 
\par \cf0\highlight0     do_avg('ResidualBlock', 10, curr_sess)\cf1\highlight1 
\par \cf0\highlight0   File "residual_network_cifar10.py", line 104, in do_avg\cf1\highlight1 
\par \cf0\highlight0     avg_paras(name_list,sess)\cf1\highlight1 
\par \cf0\highlight0   File "residual_network_cifar10.py", line 86, in avg_paras\cf1\highlight1 
\par \cf0\highlight0     value_list = map(model.get_weights, tensor_list)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tflearn/models/dnn.py", line 334, in get_weights\cf1\highlight1 
\par \cf0\highlight0     return weight_tensor.eval(self.trainer.session)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 541, in eval\cf1\highlight1 
\par \cf0\highlight0     return _eval_using_default_session(self, feed_dict, self.graph, session)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 4085, in _eval_using_default_session\cf1\highlight1 
\par \cf0\highlight0     return session.run(tensors, feed_dict)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run\cf1\highlight1 
\par \cf0\highlight0     run_metadata_ptr)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1063, in _run\cf1\highlight1 
\par \cf0\highlight0     feed_dict = nest.flatten_dict_items(feed_dict)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tensorflow/python/util/nest.py", line 234, in flatten_dict_items\cf1\highlight1 
\par \cf0\highlight0     raise TypeError("input must be a dictionary")\cf1\highlight1 
\par \cf0\highlight0 TypeError: input must be a dictionary\cf1\highlight1 
\par \cf0\highlight0 (tf) xdong@gpu2:/data/xdong/codes/avg_para$ CUDA_VISIBLE_DEVICES=0 python residual_network_cifar10.py\cf1\highlight1 
\par \cf0\highlight0 Couldn't import dot_parser, loading of dot files will not be possible.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 19:34:47.377297: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could spePU computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 19:34:47.377330: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could spePU computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 19:34:47.377338: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 19:34:47.377344: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 19:34:47.377352: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 19:34:47.941868: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties:\cf1\highlight1 
\par \cf0\highlight0 name: GeForce GTX TITAN X\cf1\highlight1 
\par \cf0\highlight0 major: 5 minor: 2 memoryClockRate (GHz) 1.076\cf1\highlight1 
\par \cf0\highlight0 pciBusID 0000:02:00.0\cf1\highlight1 
\par \cf0\highlight0 Total memory: 11.92GiB\cf1\highlight1 
\par \cf0\highlight0 Free memory: 11.81GiB\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 19:34:47.941971: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 19:34:47.941990: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 19:34:47.942010: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:02:00.0)\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 19:35:14.434104: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:02:00.0)\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Run id: resnet_cifar10\cf1\highlight1 
\par \cf0\highlight0 Log directory: /tmp/tflearn_logs/\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Preprocessing... Calculating mean over all dataset (this may take long)...\cf1\highlight1 
\par \cf0\highlight0 Mean: [ 0.49139968  0.48215841  0.44653091] (To avoid repetitive computation, add it to argument 'mean' of `add_featurewise_zero_center`)\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Training samples: 50000\cf1\highlight1 
\par \cf0\highlight0 Validation samples: 10000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 391  | total loss: \cf6 1.18566\cf0  | time: 50.707s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.18566 - acc: 0.5695 -- iter: 50000/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 [ Mon Oct 23 19:36:10 2017 ] Starting Avg ...\cf1\highlight1 
\par \cf0\highlight0 [0.56775999998092652]\cf1\highlight1 
\par \cf0\highlight0 Traceback (most recent call last):\cf1\highlight1 
\par \cf0\highlight0   File "residual_network_cifar10.py", line 117, in <module>\cf1\highlight1 
\par \cf0\highlight0     do_avg('ResidualBlock', 10, curr_sess, model)\cf1\highlight1 
\par \cf0\highlight0   File "residual_network_cifar10.py", line 102, in do_avg\cf1\highlight1 
\par \cf0\highlight0     avg_paras(name_list,sess, model)\cf1\highlight1 
\par \cf0\highlight0   File "residual_network_cifar10.py", line 84, in avg_paras\cf1\highlight1 
\par \cf0\highlight0     value_list = map(model.get_weights, tensor_list)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tflearn/models/dnn.py", line 334, in get_weights\cf1\highlight1 
\par \cf0\highlight0     return weight_tensor.eval(self.trainer.session)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 541, in eval\cf1\highlight1 
\par \cf0\highlight0     return _eval_using_default_session(self, feed_dict, self.graph, session)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 4085, in _eval_using_default_session\cf1\highlight1 
\par \cf0\highlight0     return session.run(tensors, feed_dict)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run\cf1\highlight1 
\par \cf0\highlight0     run_metadata_ptr)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1063, in _run\cf1\highlight1 
\par \cf0\highlight0     feed_dict = nest.flatten_dict_items(feed_dict)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tensorflow/python/util/nest.py", line 234, in flatten_dict_items\cf1\highlight1 
\par \cf0\highlight0     raise TypeError("input must be a dictionary")\cf1\highlight1 
\par \cf0\highlight0 TypeError: input must be a dictionary\cf1\highlight1 
\par \cf0\highlight0 (tf) xdong@gpu2:/data/xdong/codes/avg_para$ CUDA_VISIBLE_DEVICES=0 python residual_network_cifar10.py\cf1\highlight1 
\par \cf0\highlight0 Couldn't import dot_parser, loading of dot files will not be possible.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 19:40:09.085651: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could spePU computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 19:40:09.085693: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could spePU computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 19:40:09.085701: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 19:40:09.085708: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 19:40:09.085715: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 19:40:09.715911: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties:\cf1\highlight1 
\par \cf0\highlight0 name: GeForce GTX TITAN X\cf1\highlight1 
\par \cf0\highlight0 major: 5 minor: 2 memoryClockRate (GHz) 1.076\cf1\highlight1 
\par \cf0\highlight0 pciBusID 0000:02:00.0\cf1\highlight1 
\par \cf0\highlight0 Total memory: 11.92GiB\cf1\highlight1 
\par \cf0\highlight0 Free memory: 11.81GiB\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 19:40:09.716004: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 19:40:09.716024: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 19:40:09.716044: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:02:00.0)\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 19:40:28.608147: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:02:00.0)\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Run id: resnet_cifar10\cf1\highlight1 
\par \cf0\highlight0 Log directory: /tmp/tflearn_logs/\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Preprocessing... Calculating mean over all dataset (this may take long)...\cf1\highlight1 
\par \cf0\highlight0 Mean: [ 0.49139968  0.48215841  0.44653091] (To avoid repetitive computation, add it to argument 'mean' of `add_featurewise_zero_center`)\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Training samples: 50000\cf1\highlight1 
\par \cf0\highlight0 Validation samples: 10000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 391  | total loss: \cf6 1.23864\cf0  | time: 48.680s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.23864 - acc: 0.5381 -- iter: 50000/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 [ Mon Oct 23 19:41:23 2017 ] Starting Avg ...\cf1\highlight1 
\par \cf0\highlight0 [0.44796000000000002]\cf1\highlight1 
\par \cf0\highlight0 Traceback (most recent call last):\cf1\highlight1 
\par \cf0\highlight0   File "residual_network_cifar10.py", line 117, in <module>\cf1\highlight1 
\par \cf0\highlight0     do_avg('ResidualBlock', 10, curr_sess, model)\cf1\highlight1 
\par \cf0\highlight0   File "residual_network_cifar10.py", line 102, in do_avg\cf1\highlight1 
\par \cf0\highlight0     avg_paras(name_list,sess, model)\cf1\highlight1 
\par \cf0\highlight0   File "residual_network_cifar10.py", line 87, in avg_paras\cf1\highlight1 
\par \cf0\highlight0     map(model.set_weights,tensor_list, new_value_list, [sess]*len(tensor_list))\cf1\highlight1 
\par \cf0\highlight0 TypeError: set_weights() takes exactly 3 arguments (4 given)\cf1\highlight1 
\par \cf0\highlight0 (tf) xdong@gpu2:/data/xdong/codes/avg_para$ CUDA_VISIBLE_DEVICES=0 python residual_network_cifar10.py\cf1\highlight1 
\par \cf0\highlight0 Couldn't import dot_parser, loading of dot files will not be possible.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 19:44:08.057551: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could spePU computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 19:44:08.057594: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could spePU computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 19:44:08.057601: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 19:44:08.057608: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 19:44:08.057614: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 19:44:08.690852: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties:\cf1\highlight1 
\par \cf0\highlight0 name: GeForce GTX TITAN X\cf1\highlight1 
\par \cf0\highlight0 major: 5 minor: 2 memoryClockRate (GHz) 1.076\cf1\highlight1 
\par \cf0\highlight0 pciBusID 0000:02:00.0\cf1\highlight1 
\par \cf0\highlight0 Total memory: 11.92GiB\cf1\highlight1 
\par \cf0\highlight0 Free memory: 11.81GiB\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 19:44:08.690948: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 19:44:08.690967: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 19:44:08.690985: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:02:00.0)\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 19:44:27.942378: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:02:00.0)\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Run id: resnet_cifar10\cf1\highlight1 
\par \cf0\highlight0 Log directory: /tmp/tflearn_logs/\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Preprocessing... Calculating mean over all dataset (this may take long)...\cf1\highlight1 
\par \cf0\highlight0 Mean: [ 0.49139968  0.48215841  0.44653091] (To avoid repetitive computation, add it to argument 'mean' of `add_featurewise_zero_center`)\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Training samples: 50000\cf1\highlight1 
\par \cf0\highlight0 Validation samples: 10000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 391  | total loss: \cf6 1.22865\cf0  | time: 50.249s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.22865 - acc: 0.5563 -- iter: 50000/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 [ Mon Oct 23 19:45:25 2017 ] Starting Avg ...\cf1\highlight1 
\par \cf0\highlight0 [0.50759999998092653]\cf1\highlight1 
\par \cf0\highlight0 [0.10000000000476837]\cf1\highlight1 
\par \cf0\highlight0 [ Mon Oct 23 19:45:59 2017 ] Avg Done...\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Run id: resnet_cifar10\cf1\highlight1 
\par \cf0\highlight0 Log directory: /tmp/tflearn_logs/\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Training samples: 50000\cf1\highlight1 
\par \cf0\highlight0 Validation samples: 10000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 500  | total loss: \cf6 1.39547\cf0  | time: 15.485s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.39547 - acc: 0.4782 | val_loss: 1.62973 - val_acc: 0.4616 -- iter: 13952/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 782  | total loss: \cf6 1.03093\cf0  | time: 47.639s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.03093 - acc: 0.6262 -- iter: 50000/50000\cf1\highlight1 
\par \cf0\highlight0 [ Mon Oct 23 19:46:49 2017 ] Starting Avg ...\cf1\highlight1 
\par \cf0\highlight0 [0.57972000001907353]\cf1\highlight1 
\par \cf0\highlight0 [0.10000000000476837]\cf1\highlight1 
\par \cf0\highlight0 [ Mon Oct 23 19:47:21 2017 ] Avg Done...\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Run id: resnet_cifar10\cf1\highlight1 
\par \cf0\highlight0 Log directory: /tmp/tflearn_logs/\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Training samples: 50000\cf1\highlight1 
\par \cf0\highlight0 Validation samples: 10000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 1000  | total loss: \cf6 1.04417\cf0  | time: 27.526s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 003 | loss: 1.04417 - acc: 0.6316 | val_loss: 1.79021 - val_acc: 0.4999 -- iter: 27904/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 1173  | total loss: \cf6 0.84666\cf0  | time: 47.048s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 003 | loss: 0.84666 - acc: 0.7033 -- iter: 50000/50000\cf1\highlight1 
\par \cf0\highlight0 [ Mon Oct 23 19:48:10 2017 ] Starting Avg ...\cf1\highlight1 
\par \cf0\highlight0 [0.62712000007629398]\cf1\highlight1 
\par \cf0\highlight0 [0.1964400000190735]\cf1\highlight1 
\par \cf0\highlight0 [ Mon Oct 23 19:48:41 2017 ] Avg Done...\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Run id: resnet_cifar10\cf1\highlight1 
\par \cf0\highlight0 Log directory: /tmp/tflearn_logs/\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Training samples: 50000\cf1\highlight1 
\par \cf0\highlight0 Validation samples: 10000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 ^C--\cf1\highlight1 
\par \cf0\highlight0 Traceback (most recent call last):\cf1\highlight1 
\par \cf0\highlight0   File "residual_network_cifar10.py", line 113, in <module>\cf1\highlight1 
\par \cf0\highlight0     run_id='resnet_cifar10')\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tflearn/models/dnn.py", line 216, in fit\cf1\highlight1 
\par \cf0\highlight0     callbacks=callbacks)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tflearn/helpers/trainer.py", line 338, in fit\cf1\highlight1 
\par \cf0\highlight0     show_metric)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tflearn/helpers/trainer.py", line 815, in _train\cf1\highlight1 
\par \cf0\highlight0     tflearn.is_training(True, session=self.session)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tflearn/config.py", line 95, in is_training\cf1\highlight1 
\par \cf0\highlight0     tf.get_collection('is_training_ops')[0].eval(session=session)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 541, in eval\cf1\highlight1 
\par \cf0\highlight0     return _eval_using_default_session(self, feed_dict, self.graph, session)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 4085, in _eval_using_default_session\cf1\highlight1 
\par \cf0\highlight0     return session.run(tensors, feed_dict)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run\cf1\highlight1 
\par \cf0\highlight0     run_metadata_ptr)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1124, in _run\cf1\highlight1 
\par \cf0\highlight0     feed_dict_tensor, options, run_metadata)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1321, in _do_run\cf1\highlight1 
\par \cf0\highlight0     options, run_metadata)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1327, in _do_call\cf1\highlight1 
\par \cf0\highlight0     return fn(*args)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1297, in _run_fn\cf1\highlight1 
\par \cf0\highlight0     self._extend_graph()\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1358, in _extend_graph\cf1\highlight1 
\par \cf0\highlight0     self._session, graph_def.SerializeToString(), status)\cf1\highlight1 
\par \cf0\highlight0 KeyboardInterrupt\cf1\highlight1 
\par \cf0\highlight0 (tf) xdong@gpu2:/data/xdong/codes/avg_para$ CUDA_VISIBLE_DEVICES=0 python residual_network_cifar10.py\cf1\highlight1 
\par \cf0\highlight0 Couldn't import dot_parser, loading of dot files will not be possible.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 19:54:27.722466: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could spePU computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 19:54:27.722510: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could spePU computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 19:54:27.722523: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 19:54:27.722547: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 19:54:27.722559: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 19:54:28.170957: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties:\cf1\highlight1 
\par \cf0\highlight0 name: GeForce GTX TITAN X\cf1\highlight1 
\par \cf0\highlight0 major: 5 minor: 2 memoryClockRate (GHz) 1.076\cf1\highlight1 
\par \cf0\highlight0 pciBusID 0000:02:00.0\cf1\highlight1 
\par \cf0\highlight0 Total memory: 11.92GiB\cf1\highlight1 
\par \cf0\highlight0 Free memory: 11.81GiB\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 19:54:28.171042: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 19:54:28.171059: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 19:54:28.171077: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:02:00.0)\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 19:54:44.016158: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:02:00.0)\cf1\highlight1 
\par \cf0\highlight0 <tensorflow.python.client.session.Session object at 0x7f2e6d192c50>\cf1\highlight1 
\par \cf0\highlight0 <tensorflow.python.client.session.Session object at 0x7f2e6d192c50>\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Run id: resnet_cifar10\cf1\highlight1 
\par \cf0\highlight0 Log directory: /tmp/tflearn_logs/\cf1\highlight1 
\par \cf0\highlight0 Traceback (most recent call last):\cf1\highlight1 
\par \cf0\highlight0   File "residual_network_cifar10.py", line 115, in <module>\cf1\highlight1 
\par \cf0\highlight0     run_id='resnet_cifar10')\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tflearn/models/dnn.py", line 216, in fit\cf1\highlight1 
\par \cf0\highlight0     callbacks=callbacks)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tflearn/helpers/trainer.py", line 287, in fit\cf1\highlight1 
\par \cf0\highlight0     self.summ_writer, self.coord)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tflearn/helpers/trainer.py", line 770, in initialize_fit\cf1\highlight1 
\par \cf0\highlight0     dprep_dict[k].initialize(feed_dict[k], self.session)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tflearn/data_preprocessing.py", line 62, in initialize\cf1\highlight1 
\par \cf0\highlight0     if not self.global_mean.is_restored(session):\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tflearn/data_preprocessing.py", line 303, in is_restored\cf1\highlight1 
\par \cf0\highlight0     if self.var_r.eval(session=session):\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tensorflow/python/ops/variables.py", line 474, in eval\cf1\highlight1 
\par \cf0\highlight0     return self._variable.eval(session=session)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 541, in eval\cf1\highlight1 
\par \cf0\highlight0     return _eval_using_default_session(self, feed_dict, self.graph, session)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 4085, in _eval_using_default_session\cf1\highlight1 
\par \cf0\highlight0     return session.run(tensors, feed_dict)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run\cf1\highlight1 
\par \cf0\highlight0     run_metadata_ptr)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1124, in _run\cf1\highlight1 
\par \cf0\highlight0     feed_dict_tensor, options, run_metadata)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1321, in _do_run\cf1\highlight1 
\par \cf0\highlight0     options, run_metadata)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1340, in _do_call\cf1\highlight1 
\par \cf0\highlight0     raise type(e)(node_def, op, message)\cf1\highlight1 
\par \cf0\highlight0 tensorflow.python.framework.errors_impl.FailedPreconditionError: Attempting to use uninitialized value DataPreprocessing/mean_r\cf1\highlight1 
\par \cf0\highlight0          [[Node: _retval_DataPreprocessing/mean_r_0_0 = _Retval[T=DT_BOOL, index=0, _device="/job:localhost/replica:0/task:0/cpu:0"](DataPreprocessing/mean_r)]]\cf1\highlight1 
\par \cf0\highlight0 (tf) xdong@gpu2:/data/xdong/codes/avg_para$ CUDA_VISIBLE_DEVICES=0 python residual_network_cifar10.py\cf1\highlight1 
\par \cf0\highlight0 Couldn't import dot_parser, loading of dot files will not be possible.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 20:06:47.809904: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could spePU computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 20:06:47.809933: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could spePU computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 20:06:47.809941: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 20:06:47.809947: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 20:06:47.809953: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 20:06:48.445628: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties:\cf1\highlight1 
\par \cf0\highlight0 name: GeForce GTX TITAN X\cf1\highlight1 
\par \cf0\highlight0 major: 5 minor: 2 memoryClockRate (GHz) 1.076\cf1\highlight1 
\par \cf0\highlight0 pciBusID 0000:02:00.0\cf1\highlight1 
\par \cf0\highlight0 Total memory: 11.92GiB\cf1\highlight1 
\par \cf0\highlight0 Free memory: 11.81GiB\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 20:06:48.445674: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 20:06:48.445685: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 20:06:48.445697: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:02:00.0)\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 20:07:14.419298: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:02:00.0)\cf1\highlight1 
\par \cf0\highlight0 first <tensorflow.python.client.session.Session object at 0x7eff6d102810>\cf1\highlight1 
\par \cf0\highlight0 New_sess <tensorflow.python.client.session.Session object at 0x7eff6d102810>\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Run id: resnet_cifar10\cf1\highlight1 
\par \cf0\highlight0 Log directory: /tmp/tflearn_logs/\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Preprocessing... Calculating mean over all dataset (this may take long)...\cf1\highlight1 
\par \cf0\highlight0 Mean: [ 0.49139968  0.48215841  0.44653091] (To avoid repetitive computation, add it to argument 'mean' of `add_featurewise_zero_center`)\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Training samples: 50000\cf1\highlight1 
\par \cf0\highlight0 Validation samples: 10000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 391  | total loss: \cf6 1.26719\cf0  | time: 48.057s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.26719 - acc: 0.5251 -- iter: 50000/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 [ Mon Oct 23 20:08:08 2017 ] Starting Avg ...\cf1\highlight1 
\par \cf0\highlight0 [0.52253999996185307]\cf1\highlight1 
\par \cf0\highlight0 New_sess <tensorflow.python.client.session.Session object at 0x7eff6d102810>\cf1\highlight1 
\par \cf0\highlight0 New_sess <tensorflow.python.client.session.Session object at 0x7eff6d102810>\cf1\highlight1 
\par \cf0\highlight0 [0.10000000000715256]\cf1\highlight1 
\par \cf0\highlight0 [ Mon Oct 23 20:08:40 2017 ] Avg Done...\cf1\highlight1 
\par \cf0\highlight0 New_sess <tensorflow.python.client.session.Session object at 0x7eff6d102810>\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Run id: resnet_cifar10\cf1\highlight1 
\par \cf0\highlight0 Log directory: /tmp/tflearn_logs/\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Training samples: 50000\cf1\highlight1 
\par \cf0\highlight0 Validation samples: 10000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 500  | total loss: \cf6 1.37455\cf0  | time: 15.263s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.37455 - acc: 0.5068 | val_loss: 1.66969 - val_acc: 0.4218 -- iter: 13952/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 782  | total loss: \cf6 1.01495\cf0  | time: 47.019s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.01495 - acc: 0.6324 -- iter: 50000/50000\cf1\highlight1 
\par \cf0\highlight0 [ Mon Oct 23 20:09:29 2017 ] Starting Avg ...\cf1\highlight1 
\par \cf0\highlight0 [0.62860000000000005]\cf1\highlight1 
\par \cf0\highlight0 New_sess <tensorflow.python.client.session.Session object at 0x7eff6d102810>\cf1\highlight1 
\par \cf0\highlight0 New_sess <tensorflow.python.client.session.Session object at 0x7eff6d102810>\cf1\highlight1 
\par \cf0\highlight0 [0.11422000000476837]\cf1\highlight1 
\par \cf0\highlight0 [ Mon Oct 23 20:10:00 2017 ] Avg Done...\cf1\highlight1 
\par \cf0\highlight0 New_sess <tensorflow.python.client.session.Session object at 0x7eff6d102810>\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Run id: resnet_cifar10\cf1\highlight1 
\par \cf0\highlight0 Log directory: /tmp/tflearn_logs/\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Training samples: 50000\cf1\highlight1 
\par \cf0\highlight0 Validation samples: 10000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 1000  | total loss: \cf6 0.97463\cf0  | time: 27.275s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 003 | loss: 0.97463 - acc: 0.6523 | val_loss: 1.30359 - val_acc: 0.5733 -- iter: 27904/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 1173  | total loss: \cf6 0.87438\cf0  | time: 46.592s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 003 | loss: 0.87438 - acc: 0.6963 -- iter: 50000/50000\cf1\highlight1 
\par \cf0\highlight0 [ Mon Oct 23 20:10:49 2017 ] Starting Avg ...\cf1\highlight1 
\par \cf0\highlight0 [0.66036000007629392]\cf1\highlight1 
\par \cf0\highlight0 New_sess <tensorflow.python.client.session.Session object at 0x7eff6d102810>\cf1\highlight1 
\par \cf0\highlight0 New_sess <tensorflow.python.client.session.Session object at 0x7eff6d102810>\cf1\highlight1 
\par \cf0\highlight0 ^CTraceback (most recent call last):\cf1\highlight1 
\par \cf0\highlight0   File "residual_network_cifar10.py", line 125, in <module>\cf1\highlight1 
\par \cf0\highlight0     print model.evaluate(X, Y)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tflearn/models/dnn.py", line 370, in evaluate\cf1\highlight1 
\par \cf0\highlight0     return self.predictor.evaluate(feed_dict, ops, batch_size)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tflearn/helpers/evaluator.py", line 114, in evaluate\cf1\highlight1 
\par \cf0\highlight0     return evaluate_flow(self.session, ops, df)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tflearn/helpers/trainer.py", line 997, in evaluate_flow\cf1\highlight1 
\par \cf0\highlight0     r = session.run(ops_to_evaluate, feed_batch)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run\cf1\highlight1 
\par \cf0\highlight0     run_metadata_ptr)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1124, in _run\cf1\highlight1 
\par \cf0\highlight0     feed_dict_tensor, options, run_metadata)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1321, in _do_run\cf1\highlight1 
\par \cf0\highlight0     options, run_metadata)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1327, in _do_call\cf1\highlight1 
\par \cf0\highlight0     return fn(*args)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1306, in _run_fn\cf1\highlight1 
\par \cf0\highlight0     status, run_metadata)\cf1\highlight1 
\par \cf0\highlight0 KeyboardInterrupt\cf1\highlight1 
\par \cf0\highlight0 (tf) xdong@gpu2:/data/xdong/codes/avg_para$ CUDA_VISIBLE_DEVICES=0 python residual_network_cifar10.py\cf1\highlight1 
\par \cf0\highlight0 Couldn't import dot_parser, loading of dot files will not be possible.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 20:13:06.527195: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could spePU computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 20:13:06.527238: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could spePU computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 20:13:06.527247: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 20:13:06.527255: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 20:13:06.527261: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 20:13:07.116688: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties:\cf1\highlight1 
\par \cf0\highlight0 name: GeForce GTX TITAN X\cf1\highlight1 
\par \cf0\highlight0 major: 5 minor: 2 memoryClockRate (GHz) 1.076\cf1\highlight1 
\par \cf0\highlight0 pciBusID 0000:02:00.0\cf1\highlight1 
\par \cf0\highlight0 Total memory: 11.92GiB\cf1\highlight1 
\par \cf0\highlight0 Free memory: 11.81GiB\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 20:13:07.116788: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 20:13:07.116807: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 20:13:07.116831: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:02:00.0)\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 20:13:17.755496: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:02:00.0)\cf1\highlight1 
\par \cf0\highlight0 first <tensorflow.python.client.session.Session object at 0x7f581b71ad50>\cf1\highlight1 
\par \cf0\highlight0 New_sess <tensorflow.python.client.session.Session object at 0x7f581b71ad50>\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Run id: resnet_cifar10\cf1\highlight1 
\par \cf0\highlight0 Log directory: /tmp/tflearn_logs/\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Preprocessing... Calculating mean over all dataset (this may take long)...\cf1\highlight1 
\par \cf0\highlight0 Mean: [ 0.49139968  0.48215841  0.44653091] (To avoid repetitive computation, add it to argument 'mean' of `add_featurewise_zero_center`)\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Training samples: 50000\cf1\highlight1 
\par \cf0\highlight0 Validation samples: 10000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 1  | time: 7.327s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 0.00000 - acc: 0.0000 | val_loss: 2.80395 - val_acc: 0.1000 -- iter: 00128/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 2  | total loss: \cf6 2.06913\cf0  | time: 9.172s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 2.06913 - acc: 0.1477 | val_loss: 2.56305 - val_acc: 0.1000 -- iter: 00256/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 3  | total loss: \cf6 2.24880\cf0  | time: 11.031s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 2.24880 - acc: 0.1675 | val_loss: 2.46993 - val_acc: 0.1000 -- iter: 00384/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 4  | total loss: \cf6 2.27759\cf0  | time: 13.403s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 2.27759 - acc: 0.1649 | val_loss: 2.37726 - val_acc: 0.1000 -- iter: 00512/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 5  | total loss: \cf6 2.29959\cf0  | time: 15.321s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 2.29959 - acc: 0.1156 | val_loss: 2.32759 - val_acc: 0.1000 -- iter: 00640/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 6  | total loss: \cf6 2.30878\cf0  | time: 17.297s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 2.30878 - acc: 0.1066 | val_loss: 2.31483 - val_acc: 0.1078 -- iter: 00768/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 7  | total loss: \cf6 2.30885\cf0  | time: 19.024s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 2.30885 - acc: 0.0848 | val_loss: 2.31225 - val_acc: 0.1199 -- iter: 00896/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 8  | total loss: \cf6 2.30621\cf0  | time: 20.839s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 2.30621 - acc: 0.0898 | val_loss: 2.30708 - val_acc: 0.1092 -- iter: 01024/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 9  | total loss: \cf6 2.31171\cf0  | time: 22.785s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 2.31171 - acc: 0.0919 | val_loss: 2.30155 - val_acc: 0.1117 -- iter: 01152/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 10  | total loss: \cf6 2.30932\cf0  | time: 24.676s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 2.30932 - acc: 0.1006 | val_loss: 2.29402 - val_acc: 0.1448 -- iter: 01280/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 11  | total loss: \cf6 2.29494\cf0  | time: 26.623s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 2.29494 - acc: 0.1085 | val_loss: 2.28285 - val_acc: 0.1756 -- iter: 01408/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 12  | total loss: \cf6 2.30431\cf0  | time: 28.441s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 2.30431 - acc: 0.1019 | val_loss: 2.26997 - val_acc: 0.1733 -- iter: 01536/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 13  | total loss: \cf6 2.28180\cf0  | time: 30.566s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 2.28180 - acc: 0.1218 | val_loss: 2.25663 - val_acc: 0.1741 -- iter: 01664/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 14  | total loss: \cf6 2.26925\cf0  | time: 32.378s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 2.26925 - acc: 0.1167 | val_loss: 2.23893 - val_acc: 0.1688 -- iter: 01792/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 15  | total loss: \cf6 2.27300\cf0  | time: 34.123s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 2.27300 - acc: 0.1383 | val_loss: 2.21402 - val_acc: 0.1658 -- iter: 01920/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 16  | total loss: \cf6 2.26552\cf0  | time: 36.007s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 2.26552 - acc: 0.1509 | val_loss: 2.19576 - val_acc: 0.1705 -- iter: 02048/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 17  | total loss: \cf6 2.23751\cf0  | time: 37.712s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 2.23751 - acc: 0.1838 | val_loss: 2.23374 - val_acc: 0.1612 -- iter: 02176/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 18  | total loss: \cf6 2.21430\cf0  | time: 39.536s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 2.21430 - acc: 0.2094 | val_loss: 2.49324 - val_acc: 0.1562 -- iter: 02304/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 19  | total loss: \cf6 2.17673\cf0  | time: 41.350s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 2.17673 - acc: 0.2047 | val_loss: 2.90374 - val_acc: 0.1542 -- iter: 02432/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 20  | total loss: \cf6 2.13991\cf0  | time: 43.408s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 2.13991 - acc: 0.2092 | val_loss: 3.56047 - val_acc: 0.1565 -- iter: 02560/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 21  | total loss: \cf6 2.12584\cf0  | time: 45.244s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 2.12584 - acc: 0.1807 | val_loss: 3.25537 - val_acc: 0.1622 -- iter: 02688/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 22  | total loss: \cf6 2.11118\cf0  | time: 47.031s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 2.11118 - acc: 0.1851 | val_loss: 2.41467 - val_acc: 0.1679 -- iter: 02816/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 23  | total loss: \cf6 2.10919\cf0  | time: 48.960s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 2.10919 - acc: 0.1948 | val_loss: 2.42244 - val_acc: 0.1801 -- iter: 02944/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 24  | total loss: \cf6 2.07729\cf0  | time: 50.755s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 2.07729 - acc: 0.2060 | val_loss: 2.79593 - val_acc: 0.1735 -- iter: 03072/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 25  | total loss: \cf6 2.06313\cf0  | time: 52.651s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 2.06313 - acc: 0.2180 | val_loss: 3.74785 - val_acc: 0.1854 -- iter: 03200/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 26  | total loss: \cf6 2.06349\cf0  | time: 54.502s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 2.06349 - acc: 0.2327 | val_loss: 4.28764 - val_acc: 0.1818 -- iter: 03328/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 27  | total loss: \cf6 2.06836\cf0  | time: 56.405s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 2.06836 - acc: 0.2271 | val_loss: 4.38416 - val_acc: 0.1673 -- iter: 03456/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 28  | total loss: \cf6 2.03834\cf0  | time: 58.205s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 2.03834 - acc: 0.2426 | val_loss: 3.26718 - val_acc: 0.1701 -- iter: 03584/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 29  | total loss: \cf6 2.04182\cf0  | time: 60.120s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 2.04182 - acc: 0.2368 | val_loss: 3.35678 - val_acc: 0.1808 -- iter: 03712/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 30  | total loss: \cf6 2.03275\cf0  | time: 61.955s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 2.03275 - acc: 0.2362 | val_loss: 2.73770 - val_acc: 0.2037 -- iter: 03840/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 31  | total loss: \cf6 2.02816\cf0  | time: 63.893s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 2.02816 - acc: 0.2268 | val_loss: 2.29929 - val_acc: 0.2156 -- iter: 03968/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 32  | total loss: \cf6 2.00589\cf0  | time: 65.765s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 2.00589 - acc: 0.2408 | val_loss: 2.33898 - val_acc: 0.1969 -- iter: 04096/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 33  | total loss: \cf6 1.99391\cf0  | time: 67.670s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.99391 - acc: 0.2428 | val_loss: 2.50633 - val_acc: 0.1827 -- iter: 04224/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 34  | total loss: \cf6 1.98618\cf0  | time: 69.492s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.98618 - acc: 0.2477 | val_loss: 2.83090 - val_acc: 0.1684 -- iter: 04352/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 35  | total loss: \cf6 1.97719\cf0  | time: 71.270s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.97719 - acc: 0.2416 | val_loss: 2.99147 - val_acc: 0.1833 -- iter: 04480/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 36  | total loss: \cf6 1.95689\cf0  | time: 73.096s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.95689 - acc: 0.2561 | val_loss: 2.78831 - val_acc: 0.2013 -- iter: 04608/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 37  | total loss: \cf6 1.96723\cf0  | time: 74.955s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.96723 - acc: 0.2362 | val_loss: 2.50283 - val_acc: 0.2135 -- iter: 04736/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 38  | total loss: \cf6 1.96053\cf0  | time: 76.802s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.96053 - acc: 0.2343 | val_loss: 2.13498 - val_acc: 0.2474 -- iter: 04864/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 39  | total loss: \cf6 1.95349\cf0  | time: 78.710s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.95349 - acc: 0.2313 | val_loss: 2.00542 - val_acc: 0.2600 -- iter: 04992/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 40  | total loss: \cf6 1.91671\cf0  | time: 80.535s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.91671 - acc: 0.2568 | val_loss: 2.05911 - val_acc: 0.2528 -- iter: 05120/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 41  | total loss: \cf6 1.91725\cf0  | time: 82.236s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.91725 - acc: 0.2570 | val_loss: 2.35034 - val_acc: 0.2283 -- iter: 05248/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 42  | total loss: \cf6 1.91106\cf0  | time: 84.012s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.91106 - acc: 0.2487 | val_loss: 2.37598 - val_acc: 0.2249 -- iter: 05376/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 43  | total loss: \cf6 1.88704\cf0  | time: 85.967s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.88704 - acc: 0.2517 | val_loss: 2.23266 - val_acc: 0.2344 -- iter: 05504/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 44  | total loss: \cf6 1.88683\cf0  | time: 87.942s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.88683 - acc: 0.2527 | val_loss: 2.23372 - val_acc: 0.2372 -- iter: 05632/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 45  | total loss: \cf6 1.87518\cf0  | time: 89.783s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.87518 - acc: 0.2576 | val_loss: 2.26344 - val_acc: 0.2560 -- iter: 05760/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 46  | total loss: \cf6 1.86528\cf0  | time: 91.621s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.86528 - acc: 0.2615 | val_loss: 2.46891 - val_acc: 0.2426 -- iter: 05888/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 47  | total loss: \cf6 1.84666\cf0  | time: 93.459s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.84666 - acc: 0.2673 | val_loss: 3.38500 - val_acc: 0.2093 -- iter: 06016/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 48  | total loss: \cf6 1.85490\cf0  | time: 95.241s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.85490 - acc: 0.2670 | val_loss: 3.83794 - val_acc: 0.2096 -- iter: 06144/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 49  | total loss: \cf6 1.84810\cf0  | time: 97.160s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.84810 - acc: 0.2693 | val_loss: 3.50113 - val_acc: 0.1937 -- iter: 06272/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 50  | total loss: \cf6 1.83600\cf0  | time: 99.090s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.83600 - acc: 0.2833 | val_loss: 3.40816 - val_acc: 0.1840 -- iter: 06400/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 51  | total loss: \cf6 1.84524\cf0  | time: 100.938s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.84524 - acc: 0.2841 | val_loss: 3.34721 - val_acc: 0.1916 -- iter: 06528/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 52  | total loss: \cf6 1.83429\cf0  | time: 102.685s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.83429 - acc: 0.2849 | val_loss: 3.29204 - val_acc: 0.2051 -- iter: 06656/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 53  | total loss: \cf6 1.81910\cf0  | time: 104.549s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.81910 - acc: 0.2936 | val_loss: 3.38573 - val_acc: 0.2161 -- iter: 06784/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 54  | total loss: \cf6 1.80667\cf0  | time: 106.425s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.80667 - acc: 0.2975 | val_loss: 3.33936 - val_acc: 0.2117 -- iter: 06912/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 55  | total loss: \cf6 1.81429\cf0  | time: 108.237s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.81429 - acc: 0.3029 | val_loss: 3.47034 - val_acc: 0.2144 -- iter: 07040/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 56  | total loss: \cf6 1.82287\cf0  | time: 110.035s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.82287 - acc: 0.2988 | val_loss: 3.58377 - val_acc: 0.2225 -- iter: 07168/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 57  | total loss: \cf6 1.80895\cf0  | time: 111.865s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.80895 - acc: 0.2964 | val_loss: 3.33181 - val_acc: 0.2351 -- iter: 07296/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 58  | total loss: \cf6 1.81605\cf0  | time: 113.606s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.81605 - acc: 0.2932 | val_loss: 3.44236 - val_acc: 0.2348 -- iter: 07424/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 59  | total loss: \cf6 1.84025\cf0  | time: 115.415s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.84025 - acc: 0.2843 | val_loss: 2.61955 - val_acc: 0.2486 -- iter: 07552/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 60  | total loss: \cf6 1.84282\cf0  | time: 117.282s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.84282 - acc: 0.2860 | val_loss: 2.10238 - val_acc: 0.2636 -- iter: 07680/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 61  | total loss: \cf6 1.84181\cf0  | time: 119.183s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.84181 - acc: 0.2833 | val_loss: 1.92929 - val_acc: 0.2732 -- iter: 07808/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 62  | total loss: \cf6 1.83032\cf0  | time: 120.967s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.83032 - acc: 0.2861 | val_loss: 2.00625 - val_acc: 0.2372 -- iter: 07936/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 63  | total loss: \cf6 1.84262\cf0  | time: 122.708s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.84262 - acc: 0.2825 | val_loss: 2.13701 - val_acc: 0.2235 -- iter: 08064/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 64  | total loss: \cf6 1.85481\cf0  | time: 124.668s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.85481 - acc: 0.2843 | val_loss: 2.28964 - val_acc: 0.2082 -- iter: 08192/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 65  | total loss: \cf6 1.85847\cf0  | time: 126.665s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.85847 - acc: 0.2772 | val_loss: 2.28830 - val_acc: 0.2062 -- iter: 08320/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 66  | total loss: \cf6 1.85186\cf0  | time: 128.499s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.85186 - acc: 0.2843 | val_loss: 2.13699 - val_acc: 0.2270 -- iter: 08448/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 67  | total loss: \cf6 1.84062\cf0  | time: 130.564s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.84062 - acc: 0.2858 | val_loss: 1.96218 - val_acc: 0.2581 -- iter: 08576/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 68  | total loss: \cf6 1.85626\cf0  | time: 132.456s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.85626 - acc: 0.2825 | val_loss: 1.85478 - val_acc: 0.2931 -- iter: 08704/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 69  | total loss: \cf6 1.85603\cf0  | time: 134.272s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.85603 - acc: 0.2787 | val_loss: 1.79219 - val_acc: 0.2994 -- iter: 08832/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 70  | total loss: \cf6 1.84998\cf0  | time: 136.250s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.84998 - acc: 0.2880 | val_loss: 1.78852 - val_acc: 0.2951 -- iter: 08960/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 71  | total loss: \cf6 1.85286\cf0  | time: 138.043s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.85286 - acc: 0.2899 | val_loss: 1.81628 - val_acc: 0.2899 -- iter: 09088/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 72  | total loss: \cf6 1.84878\cf0  | time: 139.936s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.84878 - acc: 0.2863 | val_loss: 1.87076 - val_acc: 0.2781 -- iter: 09216/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 73  | total loss: \cf6 1.84098\cf0  | time: 141.823s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.84098 - acc: 0.2866 | val_loss: 1.88647 - val_acc: 0.2786 -- iter: 09344/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 74  | total loss: \cf6 1.84197\cf0  | time: 143.588s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.84197 - acc: 0.2834 | val_loss: 1.87708 - val_acc: 0.2780 -- iter: 09472/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 75  | total loss: \cf6 1.84186\cf0  | time: 145.414s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.84186 - acc: 0.2807 | val_loss: 1.93428 - val_acc: 0.2837 -- iter: 09600/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 76  | total loss: \cf6 1.84985\cf0  | time: 147.221s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.84985 - acc: 0.2765 | val_loss: 1.94581 - val_acc: 0.2869 -- iter: 09728/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 77  | total loss: \cf6 1.85019\cf0  | time: 149.050s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.85019 - acc: 0.2803 | val_loss: 1.95241 - val_acc: 0.2858 -- iter: 09856/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 78  | total loss: \cf6 1.85126\cf0  | time: 151.181s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.85126 - acc: 0.2772 | val_loss: 1.93330 - val_acc: 0.2835 -- iter: 09984/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 79  | total loss: \cf6 1.83530\cf0  | time: 153.097s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.83530 - acc: 0.2800 | val_loss: 1.90294 - val_acc: 0.2893 -- iter: 10112/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 80  | total loss: \cf6 1.82814\cf0  | time: 155.026s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.82814 - acc: 0.2857 | val_loss: 1.84857 - val_acc: 0.2962 -- iter: 10240/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 81  | total loss: \cf6 1.82325\cf0  | time: 156.862s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.82325 - acc: 0.2853 | val_loss: 1.82876 - val_acc: 0.2913 -- iter: 10368/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 82  | total loss: \cf6 1.81055\cf0  | time: 158.659s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.81055 - acc: 0.2888 | val_loss: 1.81422 - val_acc: 0.3016 -- iter: 10496/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 83  | total loss: \cf6 1.80377\cf0  | time: 160.572s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.80377 - acc: 0.2888 | val_loss: 1.82044 - val_acc: 0.2993 -- iter: 10624/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 84  | total loss: \cf6 1.79196\cf0  | time: 162.423s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.79196 - acc: 0.2974 | val_loss: 1.83382 - val_acc: 0.2915 -- iter: 10752/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 85  | total loss: \cf6 1.78608\cf0  | time: 164.281s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.78608 - acc: 0.3021 | val_loss: 1.79671 - val_acc: 0.3056 -- iter: 10880/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 86  | total loss: \cf6 1.78741\cf0  | time: 166.197s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.78741 - acc: 0.3008 | val_loss: 1.79874 - val_acc: 0.3137 -- iter: 11008/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 87  | total loss: \cf6 1.78458\cf0  | time: 168.101s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.78458 - acc: 0.3004 | val_loss: 1.81141 - val_acc: 0.3154 -- iter: 11136/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 88  | total loss: \cf6 1.78081\cf0  | time: 169.855s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.78081 - acc: 0.3039 | val_loss: 1.83849 - val_acc: 0.3116 -- iter: 11264/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 89  | total loss: \cf6 1.78139\cf0  | time: 171.725s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.78139 - acc: 0.3064 | val_loss: 1.78953 - val_acc: 0.3228 -- iter: 11392/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 90  | total loss: \cf6 1.78324\cf0  | time: 173.584s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.78324 - acc: 0.3070 | val_loss: 1.81361 - val_acc: 0.3219 -- iter: 11520/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 91  | total loss: \cf6 1.79813\cf0  | time: 175.356s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.79813 - acc: 0.3099 | val_loss: 1.84793 - val_acc: 0.3214 -- iter: 11648/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 92  | total loss: \cf6 1.79137\cf0  | time: 177.393s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.79137 - acc: 0.3164 | val_loss: 1.82836 - val_acc: 0.3203 -- iter: 11776/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 93  | total loss: \cf6 1.79731\cf0  | time: 179.221s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.79731 - acc: 0.3113 | val_loss: 1.84273 - val_acc: 0.3084 -- iter: 11904/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 94  | total loss: \cf6 1.79628\cf0  | time: 181.183s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.79628 - acc: 0.3153 | val_loss: 1.95368 - val_acc: 0.2922 -- iter: 12032/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 95  | total loss: \cf6 1.78575\cf0  | time: 183.129s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.78575 - acc: 0.3236 | val_loss: 2.03400 - val_acc: 0.3000 -- iter: 12160/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 96  | total loss: \cf6 1.78785\cf0  | time: 185.179s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.78785 - acc: 0.3178 | val_loss: 2.10123 - val_acc: 0.2889 -- iter: 12288/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 97  | total loss: \cf6 1.78276\cf0  | time: 186.982s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.78276 - acc: 0.3196 | val_loss: 2.17157 - val_acc: 0.2799 -- iter: 12416/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 98  | total loss: \cf6 1.79347\cf0  | time: 188.805s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.79347 - acc: 0.3189 | val_loss: 2.14165 - val_acc: 0.2830 -- iter: 12544/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 99  | total loss: \cf6 1.79236\cf0  | time: 190.678s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.79236 - acc: 0.3206 | val_loss: 2.06868 - val_acc: 0.2954 -- iter: 12672/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 100  | total loss: \cf6 1.79748\cf0  | time: 192.516s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.79748 - acc: 0.3190 | val_loss: 1.95558 - val_acc: 0.3128 -- iter: 12800/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 101  | total loss: \cf6 1.78816\cf0  | time: 194.332s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.78816 - acc: 0.3192 | val_loss: 1.80465 - val_acc: 0.3290 -- iter: 12928/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 102  | total loss: \cf6 1.78192\cf0  | time: 196.147s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.78192 - acc: 0.3240 | val_loss: 1.73243 - val_acc: 0.3355 -- iter: 13056/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 103  | total loss: \cf6 1.77364\cf0  | time: 197.837s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.77364 - acc: 0.3174 | val_loss: 1.71441 - val_acc: 0.3265 -- iter: 13184/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 104  | total loss: \cf6 1.76407\cf0  | time: 199.591s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.76407 - acc: 0.3239 | val_loss: 1.70699 - val_acc: 0.3324 -- iter: 13312/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 105  | total loss: \cf6 1.76314\cf0  | time: 201.388s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.76314 - acc: 0.3228 | val_loss: 1.70791 - val_acc: 0.3313 -- iter: 13440/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 106  | total loss: \cf6 1.75484\cf0  | time: 203.159s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.75484 - acc: 0.3233 | val_loss: 1.71498 - val_acc: 0.3281 -- iter: 13568/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 107  | total loss: \cf6 1.74277\cf0  | time: 205.215s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.74277 - acc: 0.3277 | val_loss: 1.75256 - val_acc: 0.3185 -- iter: 13696/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 108  | total loss: \cf6 1.73322\cf0  | time: 207.089s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.73322 - acc: 0.3316 | val_loss: 1.76144 - val_acc: 0.3272 -- iter: 13824/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 109  | total loss: \cf6 1.74274\cf0  | time: 208.944s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.74274 - acc: 0.3266 | val_loss: 1.77419 - val_acc: 0.3373 -- iter: 13952/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 110  | total loss: \cf6 1.73491\cf0  | time: 210.758s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.73491 - acc: 0.3322 | val_loss: 1.69471 - val_acc: 0.3621 -- iter: 14080/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 111  | total loss: \cf6 1.72655\cf0  | time: 212.677s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.72655 - acc: 0.3349 | val_loss: 1.68899 - val_acc: 0.3544 -- iter: 14208/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 112  | total loss: \cf6 1.71357\cf0  | time: 214.603s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.71357 - acc: 0.3405 | val_loss: 1.74601 - val_acc: 0.3407 -- iter: 14336/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 113  | total loss: \cf6 1.70467\cf0  | time: 216.457s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.70467 - acc: 0.3479 | val_loss: 1.77068 - val_acc: 0.3366 -- iter: 14464/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 114  | total loss: \cf6 1.70365\cf0  | time: 218.358s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.70365 - acc: 0.3451 | val_loss: 1.86801 - val_acc: 0.3042 -- iter: 14592/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 ^CTraceback (most recent call last):\cf1\highlight1 
\par \cf0\highlight0   File "residual_network_cifar10.py", line 115, in <module>\cf1\highlight1 
\par \cf0\highlight0     run_id='resnet_cifar10')\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tflearn/models/dnn.py", line 216, in fit\cf1\highlight1 
\par \cf0\highlight0     callbacks=callbacks)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tflearn/helpers/trainer.py", line 338, in fit\cf1\highlight1 
\par \cf0\highlight0     show_metric)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tflearn/helpers/trainer.py", line 846, in _train\cf1\highlight1 
\par \cf0\highlight0     e = evaluate_flow(self.session, eval_ops, self.test_dflow)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tflearn/helpers/trainer.py", line 997, in evaluate_flow\cf1\highlight1 
\par \cf0\highlight0     r = session.run(ops_to_evaluate, feed_batch)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run\cf1\highlight1 
\par \cf0\highlight0     run_metadata_ptr)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1124, in _run\cf1\highlight1 
\par \cf0\highlight0     feed_dict_tensor, options, run_metadata)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1321, in _do_run\cf1\highlight1 
\par \cf0\highlight0     options, run_metadata)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1327, in _do_call\cf1\highlight1 
\par \cf0\highlight0     return fn(*args)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1306, in _run_fn\cf1\highlight1 
\par \cf0\highlight0     status, run_metadata)\cf1\highlight1 
\par \cf0\highlight0 KeyboardInterrupt\cf1\highlight1 
\par \cf0\highlight0 (tf) xdong@gpu2:/data/xdong/codes/avg_para$ CUDA_VISIBLE_DEVICES=0 python residual_network_cifar10.py\cf1\highlight1 
\par \cf0\highlight0 Couldn't import dot_parser, loading of dot files will not be possible.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 20:21:33.285168: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could spePU computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 20:21:33.285206: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could spePU computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 20:21:33.285214: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 20:21:33.285220: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 20:21:33.285226: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 20:21:33.740048: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties:\cf1\highlight1 
\par \cf0\highlight0 name: GeForce GTX TITAN X\cf1\highlight1 
\par \cf0\highlight0 major: 5 minor: 2 memoryClockRate (GHz) 1.076\cf1\highlight1 
\par \cf0\highlight0 pciBusID 0000:02:00.0\cf1\highlight1 
\par \cf0\highlight0 Total memory: 11.92GiB\cf1\highlight1 
\par \cf0\highlight0 Free memory: 11.81GiB\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 20:21:33.740146: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 20:21:33.740188: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 20:21:33.740213: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:02:00.0)\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 20:21:43.827745: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:02:00.0)\cf1\highlight1 
\par \cf0\highlight0 first <tensorflow.python.client.session.Session object at 0x7f82d9abbd10>\cf1\highlight1 
\par \cf0\highlight0 New_sess <tensorflow.python.client.session.Session object at 0x7f82d9abbd10>\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Run id: resnet_cifar10\cf1\highlight1 
\par \cf0\highlight0 Log directory: /tmp/tflearn_logs/\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Preprocessing... Calculating mean over all dataset (this may take long)...\cf1\highlight1 
\par \cf0\highlight0 Mean: [ 0.49139968  0.48215841  0.44653091] (To avoid repetitive computation, add it to argument 'mean' of `add_featurewise_zero_center`)\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Training samples: 50000\cf1\highlight1 
\par \cf0\highlight0 Validation samples: 128\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 1  | time: 7.909s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 0.00000 - acc: 0.0000 | val_loss: 2.45327 - val_acc: 0.1016 -- iter: 00128/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 2  | total loss: \cf6 2.07217\cf0  | time: 9.246s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 2.07217 - acc: 0.0984 | val_loss: 2.31697 - val_acc: 0.0938 -- iter: 00256/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 3  | total loss: \cf6 2.25241\cf0  | time: 10.449s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 2.25241 - acc: 0.1010 | val_loss: 2.28419 - val_acc: 0.0859 -- iter: 00384/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 4  | total loss: \cf6 2.28403\cf0  | time: 11.975s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 2.28403 - acc: 0.1073 | val_loss: 2.26266 - val_acc: 0.1328 -- iter: 00512/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 5  | total loss: \cf6 2.28689\cf0  | time: 13.106s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 2.28689 - acc: 0.1195 | val_loss: 2.24698 - val_acc: 0.0938 -- iter: 00640/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 6  | total loss: \cf6 2.27957\cf0  | time: 14.222s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 2.27957 - acc: 0.1130 | val_loss: 2.22822 - val_acc: 0.1172 -- iter: 00768/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 7  | total loss: \cf6 2.28444\cf0  | time: 15.337s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 2.28444 - acc: 0.1249 | val_loss: 2.20791 - val_acc: 0.1328 -- iter: 00896/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 8  | total loss: \cf6 2.24067\cf0  | time: 16.452s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 2.24067 - acc: 0.1469 | val_loss: 2.20440 - val_acc: 0.1406 -- iter: 01024/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 9  | total loss: \cf6 2.26366\cf0  | time: 17.569s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 2.26366 - acc: 0.1229 | val_loss: 2.21405 - val_acc: 0.1406 -- iter: 01152/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 10  | total loss: \cf6 2.21705\cf0  | time: 18.685s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 2.21705 - acc: 0.1279 | val_loss: 2.25975 - val_acc: 0.1172 -- iter: 01280/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 11  | total loss: \cf6 2.20806\cf0  | time: 19.801s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 2.20806 - acc: 0.1154 | val_loss: 2.36134 - val_acc: 0.1328 -- iter: 01408/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 12  | total loss: \cf6 2.17592\cf0  | time: 20.917s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 2.17592 - acc: 0.1689 | val_loss: 2.65596 - val_acc: 0.1250 -- iter: 01536/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 13  | total loss: \cf6 2.17835\cf0  | time: 22.032s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 2.17835 - acc: 0.1568 | val_loss: 2.80589 - val_acc: 0.1172 -- iter: 01664/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 14  | total loss: \cf6 2.14571\cf0  | time: 23.146s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 2.14571 - acc: 0.1981 | val_loss: 2.52152 - val_acc: 0.1250 -- iter: 01792/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 15  | total loss: \cf6 2.14911\cf0  | time: 24.261s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 2.14911 - acc: 0.2001 | val_loss: 2.47097 - val_acc: 0.1328 -- iter: 01920/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 16  | total loss: \cf6 2.17757\cf0  | time: 25.376s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 2.17757 - acc: 0.1631 | val_loss: 2.33437 - val_acc: 0.1484 -- iter: 02048/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 17  | total loss: \cf6 2.15405\cf0  | time: 26.491s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 2.15405 - acc: 0.1494 | val_loss: 2.23532 - val_acc: 0.1562 -- iter: 02176/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 18  | total loss: \cf6 2.14255\cf0  | time: 27.609s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 2.14255 - acc: 0.1464 | val_loss: 2.14980 - val_acc: 0.1562 -- iter: 02304/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 19  | total loss: \cf6 2.11951\cf0  | time: 28.725s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 2.11951 - acc: 0.1705 | val_loss: 2.06113 - val_acc: 0.1562 -- iter: 02432/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 20  | total loss: \cf6 2.12085\cf0  | time: 29.841s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 2.12085 - acc: 0.1885 | val_loss: 2.03885 - val_acc: 0.1562 -- iter: 02560/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 21  | total loss: \cf6 2.10732\cf0  | time: 30.961s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 2.10732 - acc: 0.1785 | val_loss: 2.03044 - val_acc: 0.1875 -- iter: 02688/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 22  | total loss: \cf6 2.07537\cf0  | time: 32.089s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 2.07537 - acc: 0.1953 | val_loss: 2.01694 - val_acc: 0.2188 -- iter: 02816/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 23  | total loss: \cf6 2.06712\cf0  | time: 33.209s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 2.06712 - acc: 0.2066 | val_loss: 2.05131 - val_acc: 0.2734 -- iter: 02944/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 24  | total loss: \cf6 2.05191\cf0  | time: 34.327s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 2.05191 - acc: 0.1990 | val_loss: 2.17865 - val_acc: 0.2188 -- iter: 03072/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 25  | total loss: \cf6 2.03642\cf0  | time: 35.448s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 2.03642 - acc: 0.2087 | val_loss: 3.15472 - val_acc: 0.2031 -- iter: 03200/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 26  | total loss: \cf6 2.05240\cf0  | time: 36.570s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 2.05240 - acc: 0.1948 | val_loss: 3.67807 - val_acc: 0.1875 -- iter: 03328/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 27  | total loss: \cf6 2.06000\cf0  | time: 37.704s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 2.06000 - acc: 0.2070 | val_loss: 3.14330 - val_acc: 0.1797 -- iter: 03456/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 28  | total loss: \cf6 2.06062\cf0  | time: 38.828s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 2.06062 - acc: 0.2099 | val_loss: 2.68115 - val_acc: 0.1875 -- iter: 03584/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 29  | total loss: \cf6 2.03778\cf0  | time: 39.959s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 2.03778 - acc: 0.2197 | val_loss: 2.12133 - val_acc: 0.2656 -- iter: 03712/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 30  | total loss: \cf6 2.02917\cf0  | time: 41.090s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 2.02917 - acc: 0.2232 | val_loss: 1.98408 - val_acc: 0.2969 -- iter: 03840/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 31  | total loss: \cf6 2.01591\cf0  | time: 42.220s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 2.01591 - acc: 0.2348 | val_loss: 1.89549 - val_acc: 0.3594 -- iter: 03968/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 32  | total loss: \cf6 2.00793\cf0  | time: 43.351s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 2.00793 - acc: 0.2452 | val_loss: 1.86088 - val_acc: 0.2969 -- iter: 04096/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 33  | total loss: \cf6 2.00444\cf0  | time: 44.487s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 2.00444 - acc: 0.2411 | val_loss: 1.92516 - val_acc: 0.2891 -- iter: 04224/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 34  | total loss: \cf6 1.99337\cf0  | time: 45.614s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.99337 - acc: 0.2380 | val_loss: 1.94402 - val_acc: 0.2656 -- iter: 04352/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 35  | total loss: \cf6 1.98352\cf0  | time: 46.736s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.98352 - acc: 0.2438 | val_loss: 1.99250 - val_acc: 0.1719 -- iter: 04480/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 36  | total loss: \cf6 1.97223\cf0  | time: 47.857s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.97223 - acc: 0.2355 | val_loss: 1.93404 - val_acc: 0.1875 -- iter: 04608/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 37  | total loss: \cf6 1.96924\cf0  | time: 48.975s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.96924 - acc: 0.2274 | val_loss: 1.90948 - val_acc: 0.2422 -- iter: 04736/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 38  | total loss: \cf6 1.94517\cf0  | time: 50.098s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.94517 - acc: 0.2319 | val_loss: 1.89580 - val_acc: 0.2656 -- iter: 04864/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 39  | total loss: \cf6 1.93263\cf0  | time: 51.227s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.93263 - acc: 0.2353 | val_loss: 1.85710 - val_acc: 0.2734 -- iter: 04992/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 40  | total loss: \cf6 1.91817\cf0  | time: 52.354s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.91817 - acc: 0.2366 | val_loss: 1.82585 - val_acc: 0.3047 -- iter: 05120/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 41  | total loss: \cf6 1.90594\cf0  | time: 53.483s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.90594 - acc: 0.2391 | val_loss: 1.86838 - val_acc: 0.3047 -- iter: 05248/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 42  | total loss: \cf6 1.88865\cf0  | time: 54.606s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.88865 - acc: 0.2453 | val_loss: 1.89504 - val_acc: 0.3125 -- iter: 05376/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 43  | total loss: \cf6 1.90745\cf0  | time: 55.730s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.90745 - acc: 0.2447 | val_loss: 1.81830 - val_acc: 0.3203 -- iter: 05504/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 44  | total loss: \cf6 1.92366\cf0  | time: 56.849s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.92366 - acc: 0.2389 | val_loss: 1.89136 - val_acc: 0.2422 -- iter: 05632/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 45  | total loss: \cf6 1.91411\cf0  | time: 57.973s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.91411 - acc: 0.2421 | val_loss: 1.87916 - val_acc: 0.2891 -- iter: 05760/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 46  | total loss: \cf6 1.90925\cf0  | time: 59.094s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.90925 - acc: 0.2538 | val_loss: 1.95660 - val_acc: 0.2031 -- iter: 05888/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 47  | total loss: \cf6 1.89698\cf0  | time: 60.219s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.89698 - acc: 0.2673 | val_loss: 2.15560 - val_acc: 0.1875 -- iter: 06016/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 48  | total loss: \cf6 1.88650\cf0  | time: 61.350s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.88650 - acc: 0.2796 | val_loss: 2.20559 - val_acc: 0.2188 -- iter: 06144/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 49  | total loss: \cf6 1.88675\cf0  | time: 62.479s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.88675 - acc: 0.2811 | val_loss: 2.32395 - val_acc: 0.2969 -- iter: 06272/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 50  | total loss: \cf6 1.89378\cf0  | time: 63.605s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.89378 - acc: 0.2677 | val_loss: 2.08923 - val_acc: 0.2891 -- iter: 06400/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 51  | total loss: \cf6 1.89799\cf0  | time: 64.729s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.89799 - acc: 0.2746 | val_loss: 1.84419 - val_acc: 0.3359 -- iter: 06528/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 52  | total loss: \cf6 1.89157\cf0  | time: 65.850s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.89157 - acc: 0.2803 | val_loss: 1.75751 - val_acc: 0.3984 -- iter: 06656/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 53  | total loss: \cf6 1.87974\cf0  | time: 66.973s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.87974 - acc: 0.2896 | val_loss: 1.80653 - val_acc: 0.3359 -- iter: 06784/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 54  | total loss: \cf6 1.87739\cf0  | time: 68.096s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.87739 - acc: 0.2907 | val_loss: 1.90718 - val_acc: 0.2812 -- iter: 06912/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 55  | total loss: \cf6 1.87568\cf0  | time: 69.235s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.87568 - acc: 0.2860 | val_loss: 1.94876 - val_acc: 0.2891 -- iter: 07040/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 56  | total loss: \cf6 1.87204\cf0  | time: 70.373s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.87204 - acc: 0.2886 | val_loss: 2.00753 - val_acc: 0.2656 -- iter: 07168/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 57  | total loss: \cf6 1.84875\cf0  | time: 71.495s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.84875 - acc: 0.2919 | val_loss: 2.06957 - val_acc: 0.2578 -- iter: 07296/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 58  | total loss: \cf6 1.83862\cf0  | time: 72.619s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.83862 - acc: 0.2958 | val_loss: 1.85871 - val_acc: 0.2500 -- iter: 07424/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 59  | total loss: \cf6 1.84409\cf0  | time: 73.759s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.84409 - acc: 0.2865 | val_loss: 1.82006 - val_acc: 0.2500 -- iter: 07552/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 60  | total loss: \cf6 1.83411\cf0  | time: 74.881s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.83411 - acc: 0.2827 | val_loss: 1.88037 - val_acc: 0.2656 -- iter: 07680/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 61  | total loss: \cf6 1.84701\cf0  | time: 76.003s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.84701 - acc: 0.2835 | val_loss: 1.99013 - val_acc: 0.2891 -- iter: 07808/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 62  | total loss: \cf6 1.84521\cf0  | time: 77.125s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.84521 - acc: 0.2893 | val_loss: 1.90478 - val_acc: 0.3359 -- iter: 07936/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 63  | total loss: \cf6 1.83492\cf0  | time: 78.250s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.83492 - acc: 0.2853 | val_loss: 2.05915 - val_acc: 0.2578 -- iter: 08064/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 64  | total loss: \cf6 1.84486\cf0  | time: 79.372s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.84486 - acc: 0.2809 | val_loss: 1.77054 - val_acc: 0.3828 -- iter: 08192/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 65  | total loss: \cf6 1.86167\cf0  | time: 80.494s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.86167 - acc: 0.2722 | val_loss: 1.91436 - val_acc: 0.2656 -- iter: 08320/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 66  | total loss: \cf6 1.85388\cf0  | time: 81.617s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.85388 - acc: 0.2705 | val_loss: 2.67350 - val_acc: 0.2031 -- iter: 08448/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 67  | total loss: \cf6 1.84475\cf0  | time: 82.736s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.84475 - acc: 0.2671 | val_loss: 2.20782 - val_acc: 0.2266 -- iter: 08576/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 68  | total loss: \cf6 1.83485\cf0  | time: 83.861s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.83485 - acc: 0.2688 | val_loss: 2.23866 - val_acc: 0.2266 -- iter: 08704/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 69  | total loss: \cf6 1.83009\cf0  | time: 84.990s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.83009 - acc: 0.2757 | val_loss: 2.56507 - val_acc: 0.2031 -- iter: 08832/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 70  | total loss: \cf6 1.82582\cf0  | time: 86.116s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.82582 - acc: 0.2763 | val_loss: 2.39108 - val_acc: 0.2109 -- iter: 08960/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 71  | total loss: \cf6 1.81019\cf0  | time: 87.241s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.81019 - acc: 0.2805 | val_loss: 2.07288 - val_acc: 0.2578 -- iter: 09088/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 72  | total loss: \cf6 1.79592\cf0  | time: 88.366s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.79592 - acc: 0.2823 | val_loss: 2.06889 - val_acc: 0.3047 -- iter: 09216/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 73  | total loss: \cf6 1.79885\cf0  | time: 89.487s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.79885 - acc: 0.2874 | val_loss: 2.05773 - val_acc: 0.3047 -- iter: 09344/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 74  | total loss: \cf6 1.79209\cf0  | time: 90.615s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.79209 - acc: 0.2902 | val_loss: 1.97877 - val_acc: 0.3125 -- iter: 09472/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 75  | total loss: \cf6 1.79090\cf0  | time: 91.737s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.79090 - acc: 0.2977 | val_loss: 2.07921 - val_acc: 0.2734 -- iter: 09600/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 76  | total loss: \cf6 1.79120\cf0  | time: 92.874s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.79120 - acc: 0.2959 | val_loss: 2.44472 - val_acc: 0.2188 -- iter: 09728/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 77  | total loss: \cf6 1.77481\cf0  | time: 94.012s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.77481 - acc: 0.3059 | val_loss: 3.44672 - val_acc: 0.2031 -- iter: 09856/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 78  | total loss: \cf6 1.78602\cf0  | time: 95.144s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.78602 - acc: 0.3107 | val_loss: 3.61648 - val_acc: 0.2969 -- iter: 09984/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 79  | total loss: \cf6 1.77431\cf0  | time: 96.279s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.77431 - acc: 0.3190 | val_loss: 2.29029 - val_acc: 0.3203 -- iter: 10112/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 80  | total loss: \cf6 1.78599\cf0  | time: 97.406s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.78599 - acc: 0.3191 | val_loss: 1.93463 - val_acc: 0.3594 -- iter: 10240/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 81  | total loss: \cf6 1.78570\cf0  | time: 98.543s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.78570 - acc: 0.3184 | val_loss: 1.79116 - val_acc: 0.3594 -- iter: 10368/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 82  | total loss: \cf6 1.78255\cf0  | time: 99.680s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.78255 - acc: 0.3202 | val_loss: 1.77699 - val_acc: 0.3359 -- iter: 10496/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 83  | total loss: \cf6 1.78437\cf0  | time: 100.806s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.78437 - acc: 0.3171 | val_loss: 1.81704 - val_acc: 0.2734 -- iter: 10624/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 84  | total loss: \cf6 1.78559\cf0  | time: 101.935s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.78559 - acc: 0.3119 | val_loss: 1.93815 - val_acc: 0.2188 -- iter: 10752/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 85  | total loss: \cf6 1.77432\cf0  | time: 103.064s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.77432 - acc: 0.3167 | val_loss: 2.15783 - val_acc: 0.2031 -- iter: 10880/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 86  | total loss: \cf6 1.76079\cf0  | time: 104.196s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.76079 - acc: 0.3225 | val_loss: 2.27379 - val_acc: 0.1719 -- iter: 11008/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 87  | total loss: \cf6 1.76528\cf0  | time: 105.320s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.76528 - acc: 0.3192 | val_loss: 2.23393 - val_acc: 0.1719 -- iter: 11136/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 88  | total loss: \cf6 1.75522\cf0  | time: 105.445s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.75522 - acc: 0.3279 | val_loss: 0.00000 - val_acc: 0.0000 -- iter: 11264/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 89  | total loss: \cf6 1.75547\cf0  | time: 106.570s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.75547 - acc: 0.3263 | val_loss: 2.12832 - val_acc: 0.1797 -- iter: 11392/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 90  | total loss: \cf6 1.75493\cf0  | time: 107.692s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.75493 - acc: 0.3273 | val_loss: 2.08590 - val_acc: 0.2031 -- iter: 11520/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 91  | total loss: \cf6 1.74995\cf0  | time: 108.811s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.74995 - acc: 0.3336 | val_loss: 2.02589 - val_acc: 0.2344 -- iter: 11648/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 92  | total loss: \cf6 1.74469\cf0  | time: 109.932s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.74469 - acc: 0.3385 | val_loss: 1.89885 - val_acc: 0.2734 -- iter: 11776/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 93  | total loss: \cf6 1.74188\cf0  | time: 111.058s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.74188 - acc: 0.3391 | val_loss: 1.79020 - val_acc: 0.3203 -- iter: 11904/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 94  | total loss: \cf6 1.73870\cf0  | time: 112.175s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.73870 - acc: 0.3364 | val_loss: 1.69261 - val_acc: 0.4062 -- iter: 12032/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 95  | total loss: \cf6 1.73830\cf0  | time: 113.303s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.73830 - acc: 0.3301 | val_loss: 1.71195 - val_acc: 0.4141 -- iter: 12160/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 96  | total loss: \cf6 1.74090\cf0  | time: 114.424s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.74090 - acc: 0.3291 | val_loss: 1.85428 - val_acc: 0.3125 -- iter: 12288/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 97  | total loss: \cf6 1.72160\cf0  | time: 115.558s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.72160 - acc: 0.3368 | val_loss: 1.85145 - val_acc: 0.2969 -- iter: 12416/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 98  | total loss: \cf6 1.72508\cf0  | time: 116.695s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.72508 - acc: 0.3391 | val_loss: 1.81016 - val_acc: 0.3359 -- iter: 12544/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 99  | total loss: \cf6 1.72439\cf0  | time: 117.834s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.72439 - acc: 0.3388 | val_loss: 1.84267 - val_acc: 0.2969 -- iter: 12672/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 100  | total loss: \cf6 1.72153\cf0  | time: 118.956s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.72153 - acc: 0.3408 | val_loss: 1.74169 - val_acc: 0.3438 -- iter: 12800/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 101  | total loss: \cf6 1.73304\cf0  | time: 120.079s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.73304 - acc: 0.3404 | val_loss: 1.66951 - val_acc: 0.3906 -- iter: 12928/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 102  | total loss: \cf6 1.71570\cf0  | time: 121.214s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.71570 - acc: 0.3446 | val_loss: 1.67671 - val_acc: 0.4141 -- iter: 13056/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 103  | total loss: \cf6 1.72231\cf0  | time: 122.341s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.72231 - acc: 0.3476 | val_loss: 1.70854 - val_acc: 0.3906 -- iter: 13184/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 104  | total loss: \cf6 1.72470\cf0  | time: 123.464s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.72470 - acc: 0.3465 | val_loss: 1.77968 - val_acc: 0.3594 -- iter: 13312/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 105  | total loss: \cf6 1.72908\cf0  | time: 124.597s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.72908 - acc: 0.3462 | val_loss: 1.81241 - val_acc: 0.3594 -- iter: 13440/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 106  | total loss: \cf6 1.72078\cf0  | time: 125.718s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.72078 - acc: 0.3545 | val_loss: 1.87054 - val_acc: 0.3984 -- iter: 13568/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 107  | total loss: \cf6 1.72427\cf0  | time: 126.839s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.72427 - acc: 0.3550 | val_loss: 1.99747 - val_acc: 0.3594 -- iter: 13696/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 108  | total loss: \cf6 1.71568\cf0  | time: 127.966s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.71568 - acc: 0.3562 | val_loss: 2.03527 - val_acc: 0.3594 -- iter: 13824/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 109  | total loss: \cf6 1.72245\cf0  | time: 129.094s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.72245 - acc: 0.3527 | val_loss: 1.97807 - val_acc: 0.3359 -- iter: 13952/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 110  | total loss: \cf6 1.72198\cf0  | time: 130.216s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.72198 - acc: 0.3447 | val_loss: 1.98667 - val_acc: 0.2969 -- iter: 14080/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 111  | total loss: \cf6 1.72053\cf0  | time: 131.338s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.72053 - acc: 0.3454 | val_loss: 2.09651 - val_acc: 0.2891 -- iter: 14208/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 112  | total loss: \cf6 1.71611\cf0  | time: 132.456s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.71611 - acc: 0.3476 | val_loss: 2.08753 - val_acc: 0.3047 -- iter: 14336/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 113  | total loss: \cf6 1.71909\cf0  | time: 133.576s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.71909 - acc: 0.3449 | val_loss: 1.96713 - val_acc: 0.3594 -- iter: 14464/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 114  | total loss: \cf6 1.70707\cf0  | time: 134.708s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.70707 - acc: 0.3541 | val_loss: 1.71825 - val_acc: 0.3828 -- iter: 14592/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 115  | total loss: \cf6 1.71198\cf0  | time: 135.830s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.71198 - acc: 0.3500 | val_loss: 1.65269 - val_acc: 0.3828 -- iter: 14720/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 116  | total loss: \cf6 1.73134\cf0  | time: 136.964s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.73134 - acc: 0.3431 | val_loss: 1.75563 - val_acc: 0.3516 -- iter: 14848/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 117  | total loss: \cf6 1.71427\cf0  | time: 138.090s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.71427 - acc: 0.3518 | val_loss: 1.75273 - val_acc: 0.3672 -- iter: 14976/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 118  | total loss: \cf6 1.73060\cf0  | time: 138.218s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.73060 - acc: 0.3424 | val_loss: 0.00000 - val_acc: 0.0000 -- iter: 15104/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 119  | total loss: \cf6 1.73185\cf0  | time: 139.352s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.73185 - acc: 0.3355 | val_loss: 1.73600 - val_acc: 0.3828 -- iter: 15232/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 120  | total loss: \cf6 1.73557\cf0  | time: 140.486s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.73557 - acc: 0.3410 | val_loss: 1.74818 - val_acc: 0.3281 -- iter: 15360/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 121  | total loss: \cf6 1.73228\cf0  | time: 141.616s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.73228 - acc: 0.3530 | val_loss: 1.71819 - val_acc: 0.3047 -- iter: 15488/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 122  | total loss: \cf6 1.72643\cf0  | time: 142.741s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.72643 - acc: 0.3638 | val_loss: 1.65377 - val_acc: 0.3594 -- iter: 15616/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 123  | total loss: \cf6 1.71863\cf0  | time: 143.870s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.71863 - acc: 0.3665 | val_loss: 1.68460 - val_acc: 0.2969 -- iter: 15744/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 124  | total loss: \cf6 1.70723\cf0  | time: 144.991s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.70723 - acc: 0.3743 | val_loss: 1.88967 - val_acc: 0.2891 -- iter: 15872/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 125  | total loss: \cf6 1.70886\cf0  | time: 146.113s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.70886 - acc: 0.3721 | val_loss: 1.95343 - val_acc: 0.2969 -- iter: 16000/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 126  | total loss: \cf6 1.70039\cf0  | time: 147.241s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.70039 - acc: 0.3770 | val_loss: 1.82710 - val_acc: 0.3047 -- iter: 16128/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 127  | total loss: \cf6 1.69555\cf0  | time: 148.373s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.69555 - acc: 0.3807 | val_loss: 1.73477 - val_acc: 0.3359 -- iter: 16256/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 128  | total loss: \cf6 1.69742\cf0  | time: 149.493s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.69742 - acc: 0.3739 | val_loss: 1.66243 - val_acc: 0.3828 -- iter: 16384/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 129  | total loss: \cf6 1.69255\cf0  | time: 150.618s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.69255 - acc: 0.3772 | val_loss: 1.59889 - val_acc: 0.3672 -- iter: 16512/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 130  | total loss: \cf6 1.68735\cf0  | time: 151.734s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.68735 - acc: 0.3785 | val_loss: 1.60238 - val_acc: 0.3281 -- iter: 16640/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 131  | total loss: \cf6 1.67596\cf0  | time: 152.854s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.67596 - acc: 0.3782 | val_loss: 1.57664 - val_acc: 0.3281 -- iter: 16768/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 132  | total loss: \cf6 1.66567\cf0  | time: 153.979s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.66567 - acc: 0.3778 | val_loss: 1.62522 - val_acc: 0.3438 -- iter: 16896/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 133  | total loss: \cf6 1.66979\cf0  | time: 155.105s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.66979 - acc: 0.3713 | val_loss: 1.67296 - val_acc: 0.3438 -- iter: 17024/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 134  | total loss: \cf6 1.65781\cf0  | time: 156.231s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.65781 - acc: 0.3740 | val_loss: 1.58157 - val_acc: 0.3750 -- iter: 17152/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 135  | total loss: \cf6 1.65950\cf0  | time: 157.350s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.65950 - acc: 0.3702 | val_loss: 1.53981 - val_acc: 0.4297 -- iter: 17280/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 136  | total loss: \cf6 1.65536\cf0  | time: 158.471s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.65536 - acc: 0.3691 | val_loss: 1.62507 - val_acc: 0.3594 -- iter: 17408/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 137  | total loss: \cf6 1.64947\cf0  | time: 159.593s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.64947 - acc: 0.3791 | val_loss: 1.78837 - val_acc: 0.3516 -- iter: 17536/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 138  | total loss: \cf6 1.65522\cf0  | time: 160.718s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.65522 - acc: 0.3795 | val_loss: 1.98673 - val_acc: 0.3125 -- iter: 17664/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 139  | total loss: \cf6 1.66401\cf0  | time: 161.838s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.66401 - acc: 0.3759 | val_loss: 2.26335 - val_acc: 0.2812 -- iter: 17792/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 140  | total loss: \cf6 1.65212\cf0  | time: 162.961s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.65212 - acc: 0.3797 | val_loss: 2.42475 - val_acc: 0.2578 -- iter: 17920/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 141  | total loss: \cf6 1.63784\cf0  | time: 164.097s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.63784 - acc: 0.3847 | val_loss: 2.18545 - val_acc: 0.2812 -- iter: 18048/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 142  | total loss: \cf6 1.63244\cf0  | time: 165.222s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.63244 - acc: 0.3790 | val_loss: 1.76578 - val_acc: 0.3359 -- iter: 18176/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 143  | total loss: \cf6 1.63047\cf0  | time: 166.347s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.63047 - acc: 0.3810 | val_loss: 1.51725 - val_acc: 0.3828 -- iter: 18304/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 144  | total loss: \cf6 1.61590\cf0  | time: 167.468s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.61590 - acc: 0.3866 | val_loss: 1.46039 - val_acc: 0.4531 -- iter: 18432/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 145  | total loss: \cf6 1.61086\cf0  | time: 168.597s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.61086 - acc: 0.3894 | val_loss: 1.46785 - val_acc: 0.4688 -- iter: 18560/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 146  | total loss: \cf6 1.62656\cf0  | time: 169.727s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.62656 - acc: 0.3879 | val_loss: 1.55055 - val_acc: 0.4844 -- iter: 18688/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 147  | total loss: \cf6 1.62907\cf0  | time: 170.848s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.62907 - acc: 0.3937 | val_loss: 1.56612 - val_acc: 0.4531 -- iter: 18816/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 148  | total loss: \cf6 1.63337\cf0  | time: 171.973s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.63337 - acc: 0.3918 | val_loss: 1.59380 - val_acc: 0.4453 -- iter: 18944/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 149  | total loss: \cf6 1.62988\cf0  | time: 173.095s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.62988 - acc: 0.3956 | val_loss: 1.60153 - val_acc: 0.3828 -- iter: 19072/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 150  | total loss: \cf6 1.61415\cf0  | time: 174.214s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.61415 - acc: 0.4021 | val_loss: 1.58548 - val_acc: 0.4062 -- iter: 19200/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 151  | total loss: \cf6 1.59736\cf0  | time: 175.336s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.59736 - acc: 0.4096 | val_loss: 1.58210 - val_acc: 0.3984 -- iter: 19328/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 152  | total loss: \cf6 1.60105\cf0  | time: 176.451s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.60105 - acc: 0.4053 | val_loss: 1.54183 - val_acc: 0.4141 -- iter: 19456/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 153  | total loss: \cf6 1.61131\cf0  | time: 177.573s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.61131 - acc: 0.3929 | val_loss: 1.55864 - val_acc: 0.4141 -- iter: 19584/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 154  | total loss: \cf6 1.60205\cf0  | time: 178.697s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.60205 - acc: 0.3919 | val_loss: 1.58510 - val_acc: 0.4062 -- iter: 19712/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 155  | total loss: \cf6 1.59351\cf0  | time: 179.816s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.59351 - acc: 0.3988 | val_loss: 1.56362 - val_acc: 0.3594 -- iter: 19840/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 156  | total loss: \cf6 1.58615\cf0  | time: 180.941s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.58615 - acc: 0.3988 | val_loss: 1.53667 - val_acc: 0.3828 -- iter: 19968/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 157  | total loss: \cf6 1.59858\cf0  | time: 182.060s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.59858 - acc: 0.3917 | val_loss: 1.51051 - val_acc: 0.4375 -- iter: 20096/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 158  | total loss: \cf6 1.59483\cf0  | time: 183.183s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.59483 - acc: 0.3924 | val_loss: 1.50849 - val_acc: 0.4609 -- iter: 20224/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 159  | total loss: \cf6 1.59005\cf0  | time: 184.309s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.59005 - acc: 0.3992 | val_loss: 1.51764 - val_acc: 0.4609 -- iter: 20352/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 160  | total loss: \cf6 1.57759\cf0  | time: 185.428s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.57759 - acc: 0.4023 | val_loss: 1.54453 - val_acc: 0.4609 -- iter: 20480/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 161  | total loss: \cf6 1.57350\cf0  | time: 186.549s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.57350 - acc: 0.3941 | val_loss: 1.63763 - val_acc: 0.4609 -- iter: 20608/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 162  | total loss: \cf6 1.58386\cf0  | time: 187.680s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.58386 - acc: 0.3930 | val_loss: 1.70262 - val_acc: 0.4297 -- iter: 20736/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 163  | total loss: \cf6 1.58013\cf0  | time: 188.800s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.58013 - acc: 0.3951 | val_loss: 1.74406 - val_acc: 0.3672 -- iter: 20864/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 164  | total loss: \cf6 1.57834\cf0  | time: 189.924s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.57834 - acc: 0.3970 | val_loss: 1.69246 - val_acc: 0.3984 -- iter: 20992/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 165  | total loss: \cf6 1.56542\cf0  | time: 191.045s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.56542 - acc: 0.4049 | val_loss: 1.61112 - val_acc: 0.4453 -- iter: 21120/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 166  | total loss: \cf6 1.56069\cf0  | time: 192.165s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.56069 - acc: 0.4113 | val_loss: 1.54087 - val_acc: 0.4922 -- iter: 21248/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 167  | total loss: \cf6 1.55330\cf0  | time: 193.301s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.55330 - acc: 0.4155 | val_loss: 1.44587 - val_acc: 0.4688 -- iter: 21376/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 168  | total loss: \cf6 1.55121\cf0  | time: 194.429s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.55121 - acc: 0.4169 | val_loss: 1.40455 - val_acc: 0.5000 -- iter: 21504/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 169  | total loss: \cf6 1.54195\cf0  | time: 195.561s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.54195 - acc: 0.4174 | val_loss: 1.44786 - val_acc: 0.5078 -- iter: 21632/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 170  | total loss: \cf6 1.53052\cf0  | time: 196.687s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.53052 - acc: 0.4241 | val_loss: 1.51690 - val_acc: 0.4531 -- iter: 21760/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 171  | total loss: \cf6 1.51773\cf0  | time: 197.815s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.51773 - acc: 0.4247 | val_loss: 1.52673 - val_acc: 0.4375 -- iter: 21888/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 172  | total loss: \cf6 1.53422\cf0  | time: 198.935s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.53422 - acc: 0.4244 | val_loss: 1.55757 - val_acc: 0.4375 -- iter: 22016/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 173  | total loss: \cf6 1.52630\cf0  | time: 200.056s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.52630 - acc: 0.4312 | val_loss: 1.59226 - val_acc: 0.4688 -- iter: 22144/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 174  | total loss: \cf6 1.51778\cf0  | time: 201.186s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.51778 - acc: 0.4255 | val_loss: 1.65842 - val_acc: 0.4531 -- iter: 22272/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 175  | total loss: \cf6 1.50251\cf0  | time: 202.308s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.50251 - acc: 0.4306 | val_loss: 1.69547 - val_acc: 0.4531 -- iter: 22400/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 176  | total loss: \cf6 1.48952\cf0  | time: 203.432s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.48952 - acc: 0.4368 | val_loss: 1.66718 - val_acc: 0.4766 -- iter: 22528/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 177  | total loss: \cf6 1.48024\cf0  | time: 204.550s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.48024 - acc: 0.4423 | val_loss: 1.71424 - val_acc: 0.4766 -- iter: 22656/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 178  | total loss: \cf6 1.49343\cf0  | time: 205.672s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.49343 - acc: 0.4450 | val_loss: 1.79196 - val_acc: 0.4297 -- iter: 22784/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 179  | total loss: \cf6 1.49407\cf0  | time: 206.793s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.49407 - acc: 0.4497 | val_loss: 1.68616 - val_acc: 0.4297 -- iter: 22912/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 180  | total loss: \cf6 1.49567\cf0  | time: 207.913s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.49567 - acc: 0.4524 | val_loss: 1.54178 - val_acc: 0.4609 -- iter: 23040/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 181  | total loss: \cf6 1.50071\cf0  | time: 209.039s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.50071 - acc: 0.4532 | val_loss: 1.47725 - val_acc: 0.4609 -- iter: 23168/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 182  | total loss: \cf6 1.51148\cf0  | time: 210.165s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.51148 - acc: 0.4470 | val_loss: 1.51174 - val_acc: 0.4062 -- iter: 23296/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 183  | total loss: \cf6 1.51559\cf0  | time: 211.285s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.51559 - acc: 0.4406 | val_loss: 1.66716 - val_acc: 0.3594 -- iter: 23424/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 184  | total loss: \cf6 1.50526\cf0  | time: 212.404s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.50526 - acc: 0.4457 | val_loss: 1.84982 - val_acc: 0.3594 -- iter: 23552/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 185  | total loss: \cf6 1.49839\cf0  | time: 213.523s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.49839 - acc: 0.4488 | val_loss: 2.05364 - val_acc: 0.3672 -- iter: 23680/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 186  | total loss: \cf6 1.48524\cf0  | time: 214.645s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.48524 - acc: 0.4524 | val_loss: 2.18871 - val_acc: 0.3359 -- iter: 23808/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 187  | total loss: \cf6 1.49153\cf0  | time: 215.769s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.49153 - acc: 0.4454 | val_loss: 2.29547 - val_acc: 0.3594 -- iter: 23936/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 188  | total loss: \cf6 1.48498\cf0  | time: 216.884s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.48498 - acc: 0.4454 | val_loss: 2.27819 - val_acc: 0.3594 -- iter: 24064/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 189  | total loss: \cf6 1.50052\cf0  | time: 218.007s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.50052 - acc: 0.4430 | val_loss: 2.03246 - val_acc: 0.4141 -- iter: 24192/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 190  | total loss: \cf6 1.48494\cf0  | time: 219.131s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.48494 - acc: 0.4519 | val_loss: 1.75949 - val_acc: 0.4219 -- iter: 24320/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 191  | total loss: \cf6 1.47847\cf0  | time: 220.250s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.47847 - acc: 0.4497 | val_loss: 1.60066 - val_acc: 0.3984 -- iter: 24448/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 192  | total loss: \cf6 1.47993\cf0  | time: 221.380s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.47993 - acc: 0.4453 | val_loss: 1.57923 - val_acc: 0.4062 -- iter: 24576/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 193  | total loss: \cf6 1.48289\cf0  | time: 222.502s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.48289 - acc: 0.4453 | val_loss: 1.50871 - val_acc: 0.4297 -- iter: 24704/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 194  | total loss: \cf6 1.49317\cf0  | time: 223.627s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.49317 - acc: 0.4406 | val_loss: 1.48331 - val_acc: 0.4375 -- iter: 24832/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 195  | total loss: \cf6 1.49046\cf0  | time: 224.747s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.49046 - acc: 0.4356 | val_loss: 1.50130 - val_acc: 0.4531 -- iter: 24960/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 196  | total loss: \cf6 1.48185\cf0  | time: 225.868s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.48185 - acc: 0.4405 | val_loss: 1.60948 - val_acc: 0.3828 -- iter: 25088/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 197  | total loss: \cf6 1.48142\cf0  | time: 226.998s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.48142 - acc: 0.4457 | val_loss: 1.80487 - val_acc: 0.3125 -- iter: 25216/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 198  | total loss: \cf6 1.47419\cf0  | time: 228.121s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.47419 - acc: 0.4472 | val_loss: 1.94006 - val_acc: 0.3125 -- iter: 25344/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 199  | total loss: \cf6 1.45472\cf0  | time: 229.244s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.45472 - acc: 0.4572 | val_loss: 2.07534 - val_acc: 0.2891 -- iter: 25472/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 200  | total loss: \cf6 1.43617\cf0  | time: 230.366s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.43617 - acc: 0.4677 | val_loss: 2.20974 - val_acc: 0.2812 -- iter: 25600/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 201  | total loss: \cf6 1.42692\cf0  | time: 231.484s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.42692 - acc: 0.4733 | val_loss: 2.37146 - val_acc: 0.2812 -- iter: 25728/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 202  | total loss: \cf6 1.43681\cf0  | time: 232.600s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.43681 - acc: 0.4689 | val_loss: 2.39880 - val_acc: 0.2812 -- iter: 25856/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 203  | total loss: \cf6 1.44403\cf0  | time: 233.719s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.44403 - acc: 0.4658 | val_loss: 2.16363 - val_acc: 0.3281 -- iter: 25984/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 204  | total loss: \cf6 1.45727\cf0  | time: 234.837s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.45727 - acc: 0.4629 | val_loss: 1.98299 - val_acc: 0.3281 -- iter: 26112/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 205  | total loss: \cf6 1.46854\cf0  | time: 235.958s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.46854 - acc: 0.4573 | val_loss: 1.82575 - val_acc: 0.3516 -- iter: 26240/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 206  | total loss: \cf6 1.45454\cf0  | time: 237.084s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.45454 - acc: 0.4600 | val_loss: 1.77828 - val_acc: 0.3906 -- iter: 26368/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 207  | total loss: \cf6 1.44576\cf0  | time: 238.206s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.44576 - acc: 0.4655 | val_loss: 1.76260 - val_acc: 0.3906 -- iter: 26496/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 208  | total loss: \cf6 1.42570\cf0  | time: 239.330s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.42570 - acc: 0.4690 | val_loss: 1.59322 - val_acc: 0.4531 -- iter: 26624/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 209  | total loss: \cf6 1.44232\cf0  | time: 240.453s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.44232 - acc: 0.4666 | val_loss: 1.51007 - val_acc: 0.4609 -- iter: 26752/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 210  | total loss: \cf6 1.44737\cf0  | time: 241.587s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.44737 - acc: 0.4645 | val_loss: 1.54464 - val_acc: 0.5000 -- iter: 26880/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 211  | total loss: \cf6 1.43637\cf0  | time: 242.709s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.43637 - acc: 0.4712 | val_loss: 1.44588 - val_acc: 0.5469 -- iter: 27008/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 212  | total loss: \cf6 1.42212\cf0  | time: 243.830s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.42212 - acc: 0.4764 | val_loss: 1.36997 - val_acc: 0.5156 -- iter: 27136/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 213  | total loss: \cf6 1.41874\cf0  | time: 244.950s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.41874 - acc: 0.4772 | val_loss: 1.35697 - val_acc: 0.5391 -- iter: 27264/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 214  | total loss: \cf6 1.41687\cf0  | time: 246.084s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.41687 - acc: 0.4748 | val_loss: 1.38315 - val_acc: 0.5312 -- iter: 27392/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 215  | total loss: \cf6 1.42278\cf0  | time: 247.210s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.42278 - acc: 0.4773 | val_loss: 1.46762 - val_acc: 0.5391 -- iter: 27520/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 216  | total loss: \cf6 1.42221\cf0  | time: 248.333s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.42221 - acc: 0.4749 | val_loss: 1.58555 - val_acc: 0.4922 -- iter: 27648/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 217  | total loss: \cf6 1.41372\cf0  | time: 249.457s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.41372 - acc: 0.4751 | val_loss: 1.63717 - val_acc: 0.4766 -- iter: 27776/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 218  | total loss: \cf6 1.41991\cf0  | time: 250.578s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.41991 - acc: 0.4744 | val_loss: 1.63786 - val_acc: 0.5000 -- iter: 27904/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 219  | total loss: \cf6 1.42474\cf0  | time: 251.701s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.42474 - acc: 0.4754 | val_loss: 1.65186 - val_acc: 0.4766 -- iter: 28032/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 220  | total loss: \cf6 1.44345\cf0  | time: 252.833s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.44345 - acc: 0.4748 | val_loss: 1.77692 - val_acc: 0.4609 -- iter: 28160/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 221  | total loss: \cf6 1.45789\cf0  | time: 253.955s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.45789 - acc: 0.4687 | val_loss: 1.99392 - val_acc: 0.4375 -- iter: 28288/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 222  | total loss: \cf6 1.45662\cf0  | time: 255.083s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.45662 - acc: 0.4687 | val_loss: 1.97828 - val_acc: 0.4375 -- iter: 28416/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 223  | total loss: \cf6 1.45357\cf0  | time: 256.212s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.45357 - acc: 0.4710 | val_loss: 1.92397 - val_acc: 0.4219 -- iter: 28544/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 224  | total loss: \cf6 1.43387\cf0  | time: 257.339s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.43387 - acc: 0.4739 | val_loss: 1.92514 - val_acc: 0.4297 -- iter: 28672/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 225  | total loss: \cf6 1.43420\cf0  | time: 258.463s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.43420 - acc: 0.4719 | val_loss: 2.08082 - val_acc: 0.4219 -- iter: 28800/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 226  | total loss: \cf6 1.42842\cf0  | time: 259.585s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.42842 - acc: 0.4747 | val_loss: 2.18549 - val_acc: 0.4297 -- iter: 28928/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 227  | total loss: \cf6 1.44739\cf0  | time: 260.706s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.44739 - acc: 0.4678 | val_loss: 2.33067 - val_acc: 0.4141 -- iter: 29056/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 228  | total loss: \cf6 1.44581\cf0  | time: 261.822s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.44581 - acc: 0.4656 | val_loss: 2.25496 - val_acc: 0.3906 -- iter: 29184/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 229  | total loss: \cf6 1.45122\cf0  | time: 262.942s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.45122 - acc: 0.4628 | val_loss: 1.93754 - val_acc: 0.4297 -- iter: 29312/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 230  | total loss: \cf6 1.43979\cf0  | time: 264.064s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.43979 - acc: 0.4673 | val_loss: 1.68602 - val_acc: 0.4844 -- iter: 29440/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 231  | total loss: \cf6 1.44146\cf0  | time: 265.190s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.44146 - acc: 0.4620 | val_loss: 1.52658 - val_acc: 0.5078 -- iter: 29568/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 232  | total loss: \cf6 1.45188\cf0  | time: 266.312s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.45188 - acc: 0.4564 | val_loss: 1.40492 - val_acc: 0.5156 -- iter: 29696/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 233  | total loss: \cf6 1.45552\cf0  | time: 267.433s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.45552 - acc: 0.4514 | val_loss: 1.40261 - val_acc: 0.5234 -- iter: 29824/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 234  | total loss: \cf6 1.44698\cf0  | time: 268.551s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.44698 - acc: 0.4562 | val_loss: 1.48157 - val_acc: 0.4844 -- iter: 29952/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 235  | total loss: \cf6 1.43748\cf0  | time: 269.667s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.43748 - acc: 0.4575 | val_loss: 1.59166 - val_acc: 0.4219 -- iter: 30080/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 236  | total loss: \cf6 1.43266\cf0  | time: 270.792s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.43266 - acc: 0.4563 | val_loss: 1.71743 - val_acc: 0.3672 -- iter: 30208/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 237  | total loss: \cf6 1.42694\cf0  | time: 271.915s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.42694 - acc: 0.4591 | val_loss: 1.78347 - val_acc: 0.3672 -- iter: 30336/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 238  | total loss: \cf6 1.41372\cf0  | time: 273.036s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.41372 - acc: 0.4608 | val_loss: 1.88440 - val_acc: 0.3672 -- iter: 30464/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 239  | total loss: \cf6 1.40102\cf0  | time: 274.157s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.40102 - acc: 0.4663 | val_loss: 1.97127 - val_acc: 0.3750 -- iter: 30592/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 240  | total loss: \cf6 1.41311\cf0  | time: 275.278s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.41311 - acc: 0.4603 | val_loss: 1.95555 - val_acc: 0.4062 -- iter: 30720/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 241  | total loss: \cf6 1.40906\cf0  | time: 276.395s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.40906 - acc: 0.4658 | val_loss: 1.73535 - val_acc: 0.4375 -- iter: 30848/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 242  | total loss: \cf6 1.40242\cf0  | time: 277.517s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.40242 - acc: 0.4653 | val_loss: 1.55175 - val_acc: 0.4922 -- iter: 30976/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 243  | total loss: \cf6 1.38893\cf0  | time: 278.650s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.38893 - acc: 0.4758 | val_loss: 1.50068 - val_acc: 0.5156 -- iter: 31104/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 244  | total loss: \cf6 1.37716\cf0  | time: 279.769s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.37716 - acc: 0.4783 | val_loss: 1.53734 - val_acc: 0.4844 -- iter: 31232/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 245  | total loss: \cf6 1.37822\cf0  | time: 280.890s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.37822 - acc: 0.4836 | val_loss: 1.58334 - val_acc: 0.4297 -- iter: 31360/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 246  | total loss: \cf6 1.36572\cf0  | time: 282.010s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.36572 - acc: 0.4922 | val_loss: 1.51373 - val_acc: 0.4141 -- iter: 31488/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 247  | total loss: \cf6 1.35799\cf0  | time: 283.130s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.35799 - acc: 0.4961 | val_loss: 1.44075 - val_acc: 0.4609 -- iter: 31616/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 248  | total loss: \cf6 1.35191\cf0  | time: 283.246s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.35191 - acc: 0.4996 | val_loss: 0.00000 - val_acc: 0.0000 -- iter: 31744/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 249  | total loss: \cf6 1.36061\cf0  | time: 284.365s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.36061 - acc: 0.4958 | val_loss: 1.37810 - val_acc: 0.4844 -- iter: 31872/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 250  | total loss: \cf6 1.37694\cf0  | time: 285.480s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.37694 - acc: 0.4907 | val_loss: 1.40755 - val_acc: 0.4609 -- iter: 32000/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 251  | total loss: \cf6 1.37021\cf0  | time: 286.604s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.37021 - acc: 0.4893 | val_loss: 1.40100 - val_acc: 0.4453 -- iter: 32128/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 252  | total loss: \cf6 1.36051\cf0  | time: 287.726s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.36051 - acc: 0.4927 | val_loss: 1.43663 - val_acc: 0.4766 -- iter: 32256/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 253  | total loss: \cf6 1.34400\cf0  | time: 288.846s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.34400 - acc: 0.4942 | val_loss: 1.45025 - val_acc: 0.4766 -- iter: 32384/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 254  | total loss: \cf6 1.33967\cf0  | time: 289.965s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.33967 - acc: 0.4932 | val_loss: 1.41736 - val_acc: 0.4609 -- iter: 32512/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 255  | total loss: \cf6 1.33380\cf0  | time: 291.087s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.33380 - acc: 0.5002 | val_loss: 1.42169 - val_acc: 0.4766 -- iter: 32640/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 256  | total loss: \cf6 1.33214\cf0  | time: 292.211s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.33214 - acc: 0.5002 | val_loss: 1.42495 - val_acc: 0.4688 -- iter: 32768/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 257  | total loss: \cf6 1.34873\cf0  | time: 293.332s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.34873 - acc: 0.4923 | val_loss: 1.42292 - val_acc: 0.4922 -- iter: 32896/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 258  | total loss: \cf6 1.33268\cf0  | time: 294.452s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.33268 - acc: 0.5001 | val_loss: 1.42326 - val_acc: 0.5078 -- iter: 33024/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 259  | total loss: \cf6 1.32520\cf0  | time: 295.571s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.32520 - acc: 0.5048 | val_loss: 1.50316 - val_acc: 0.4453 -- iter: 33152/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 260  | total loss: \cf6 1.31389\cf0  | time: 296.690s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.31389 - acc: 0.5114 | val_loss: 1.68044 - val_acc: 0.4453 -- iter: 33280/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 261  | total loss: \cf6 1.28722\cf0  | time: 297.814s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.28722 - acc: 0.5196 | val_loss: 1.94619 - val_acc: 0.3906 -- iter: 33408/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 262  | total loss: \cf6 1.31511\cf0  | time: 298.938s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.31511 - acc: 0.5059 | val_loss: 2.15200 - val_acc: 0.3984 -- iter: 33536/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 263  | total loss: \cf6 1.29016\cf0  | time: 300.063s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.29016 - acc: 0.5170 | val_loss: 2.27277 - val_acc: 0.4375 -- iter: 33664/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 264  | total loss: \cf6 1.30912\cf0  | time: 301.184s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.30912 - acc: 0.5106 | val_loss: 2.20306 - val_acc: 0.4375 -- iter: 33792/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 265  | total loss: \cf6 1.31486\cf0  | time: 302.308s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.31486 - acc: 0.5033 | val_loss: 1.95976 - val_acc: 0.4141 -- iter: 33920/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 266  | total loss: \cf6 1.31908\cf0  | time: 303.425s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.31908 - acc: 0.5061 | val_loss: 1.92181 - val_acc: 0.4375 -- iter: 34048/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 267  | total loss: \cf6 1.32218\cf0  | time: 304.542s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.32218 - acc: 0.5079 | val_loss: 1.94893 - val_acc: 0.4609 -- iter: 34176/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 268  | total loss: \cf6 1.31351\cf0  | time: 305.659s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.31351 - acc: 0.5164 | val_loss: 1.89428 - val_acc: 0.5000 -- iter: 34304/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 269  | total loss: \cf6 1.30675\cf0  | time: 306.784s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.30675 - acc: 0.5187 | val_loss: 1.81656 - val_acc: 0.4922 -- iter: 34432/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 270  | total loss: \cf6 1.30821\cf0  | time: 307.906s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.30821 - acc: 0.5145 | val_loss: 1.71284 - val_acc: 0.4922 -- iter: 34560/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 271  | total loss: \cf6 1.31802\cf0  | time: 309.027s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.31802 - acc: 0.5130 | val_loss: 1.58081 - val_acc: 0.4688 -- iter: 34688/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 272  | total loss: \cf6 1.31564\cf0  | time: 310.148s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.31564 - acc: 0.5164 | val_loss: 1.47289 - val_acc: 0.4844 -- iter: 34816/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 273  | total loss: \cf6 1.30869\cf0  | time: 311.268s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.30869 - acc: 0.5203 | val_loss: 1.37015 - val_acc: 0.4844 -- iter: 34944/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 274  | total loss: \cf6 1.32708\cf0  | time: 312.389s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.32708 - acc: 0.5104 | val_loss: 1.36674 - val_acc: 0.5078 -- iter: 35072/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 275  | total loss: \cf6 1.31843\cf0  | time: 313.508s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.31843 - acc: 0.5180 | val_loss: 1.37621 - val_acc: 0.5156 -- iter: 35200/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 276  | total loss: \cf6 1.32845\cf0  | time: 314.628s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.32845 - acc: 0.5138 | val_loss: 1.36210 - val_acc: 0.5312 -- iter: 35328/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 277  | total loss: \cf6 1.32509\cf0  | time: 315.747s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.32509 - acc: 0.5085 | val_loss: 1.39792 - val_acc: 0.5000 -- iter: 35456/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 278  | total loss: \cf6 1.32541\cf0  | time: 316.866s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.32541 - acc: 0.5061 | val_loss: 1.48474 - val_acc: 0.4844 -- iter: 35584/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 279  | total loss: \cf6 1.32197\cf0  | time: 317.985s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.32197 - acc: 0.5102 | val_loss: 1.64068 - val_acc: 0.4453 -- iter: 35712/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 280  | total loss: \cf6 1.31890\cf0  | time: 319.110s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.31890 - acc: 0.5131 | val_loss: 1.82653 - val_acc: 0.3984 -- iter: 35840/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 281  | total loss: \cf6 1.32117\cf0  | time: 320.229s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.32117 - acc: 0.5133 | val_loss: 1.75504 - val_acc: 0.4297 -- iter: 35968/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 282  | total loss: \cf6 1.32255\cf0  | time: 321.348s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.32255 - acc: 0.5151 | val_loss: 1.72534 - val_acc: 0.4062 -- iter: 36096/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 283  | total loss: \cf6 1.31229\cf0  | time: 322.467s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.31229 - acc: 0.5238 | val_loss: 1.90767 - val_acc: 0.4062 -- iter: 36224/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 284  | total loss: \cf6 1.29969\cf0  | time: 323.584s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.29969 - acc: 0.5292 | val_loss: 2.22360 - val_acc: 0.3906 -- iter: 36352/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 285  | total loss: \cf6 1.29920\cf0  | time: 324.702s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.29920 - acc: 0.5302 | val_loss: 2.29134 - val_acc: 0.3750 -- iter: 36480/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 286  | total loss: \cf6 1.29314\cf0  | time: 325.826s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.29314 - acc: 0.5311 | val_loss: 2.12057 - val_acc: 0.3906 -- iter: 36608/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 287  | total loss: \cf6 1.30441\cf0  | time: 326.944s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.30441 - acc: 0.5233 | val_loss: 1.85389 - val_acc: 0.4453 -- iter: 36736/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 288  | total loss: \cf6 1.31065\cf0  | time: 328.062s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.31065 - acc: 0.5202 | val_loss: 1.57653 - val_acc: 0.4688 -- iter: 36864/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 289  | total loss: \cf6 1.29420\cf0  | time: 329.182s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.29420 - acc: 0.5228 | val_loss: 1.44798 - val_acc: 0.5000 -- iter: 36992/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 290  | total loss: \cf6 1.28296\cf0  | time: 330.305s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.28296 - acc: 0.5323 | val_loss: 1.47782 - val_acc: 0.5078 -- iter: 37120/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 291  | total loss: \cf6 1.27862\cf0  | time: 331.429s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.27862 - acc: 0.5337 | val_loss: 1.59534 - val_acc: 0.4688 -- iter: 37248/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 292  | total loss: \cf6 1.26773\cf0  | time: 332.546s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.26773 - acc: 0.5366 | val_loss: 1.60562 - val_acc: 0.4375 -- iter: 37376/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 293  | total loss: \cf6 1.27010\cf0  | time: 333.666s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.27010 - acc: 0.5345 | val_loss: 1.55068 - val_acc: 0.4688 -- iter: 37504/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 294  | total loss: \cf6 1.25143\cf0  | time: 334.788s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.25143 - acc: 0.5358 | val_loss: 1.49622 - val_acc: 0.4922 -- iter: 37632/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 295  | total loss: \cf6 1.25429\cf0  | time: 335.905s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.25429 - acc: 0.5400 | val_loss: 1.45880 - val_acc: 0.5156 -- iter: 37760/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 296  | total loss: \cf6 1.23945\cf0  | time: 337.022s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.23945 - acc: 0.5493 | val_loss: 1.40283 - val_acc: 0.5312 -- iter: 37888/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 297  | total loss: \cf6 1.24930\cf0  | time: 338.143s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.24930 - acc: 0.5420 | val_loss: 1.35208 - val_acc: 0.5391 -- iter: 38016/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 298  | total loss: \cf6 1.26042\cf0  | time: 339.270s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.26042 - acc: 0.5362 | val_loss: 1.32179 - val_acc: 0.5391 -- iter: 38144/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 299  | total loss: \cf6 1.26774\cf0  | time: 340.386s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.26774 - acc: 0.5365 | val_loss: 1.34891 - val_acc: 0.5547 -- iter: 38272/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 300  | total loss: \cf6 1.26319\cf0  | time: 341.504s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.26319 - acc: 0.5360 | val_loss: 1.40086 - val_acc: 0.5156 -- iter: 38400/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 301  | total loss: \cf6 1.26828\cf0  | time: 342.622s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.26828 - acc: 0.5340 | val_loss: 1.43819 - val_acc: 0.4766 -- iter: 38528/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 302  | total loss: \cf6 1.27389\cf0  | time: 343.743s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.27389 - acc: 0.5321 | val_loss: 1.41080 - val_acc: 0.5625 -- iter: 38656/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 303  | total loss: \cf6 1.26609\cf0  | time: 344.864s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.26609 - acc: 0.5359 | val_loss: 1.36476 - val_acc: 0.5859 -- iter: 38784/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 304  | total loss: \cf6 1.29038\cf0  | time: 345.986s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.29038 - acc: 0.5269 | val_loss: 1.31989 - val_acc: 0.5703 -- iter: 38912/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 305  | total loss: \cf6 1.28629\cf0  | time: 347.105s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.28629 - acc: 0.5265 | val_loss: 1.25455 - val_acc: 0.5234 -- iter: 39040/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 306  | total loss: \cf6 1.30853\cf0  | time: 348.228s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.30853 - acc: 0.5192 | val_loss: 1.24525 - val_acc: 0.5078 -- iter: 39168/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 307  | total loss: \cf6 1.31093\cf0  | time: 349.351s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.31093 - acc: 0.5188 | val_loss: 1.24309 - val_acc: 0.5000 -- iter: 39296/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 308  | total loss: \cf6 1.29406\cf0  | time: 350.474s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.29406 - acc: 0.5209 | val_loss: 1.23329 - val_acc: 0.5000 -- iter: 39424/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 309  | total loss: \cf6 1.28852\cf0  | time: 351.591s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.28852 - acc: 0.5274 | val_loss: 1.17996 - val_acc: 0.5391 -- iter: 39552/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 310  | total loss: \cf6 1.30307\cf0  | time: 352.713s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.30307 - acc: 0.5270 | val_loss: 1.17220 - val_acc: 0.5547 -- iter: 39680/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 311  | total loss: \cf6 1.29495\cf0  | time: 353.833s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.29495 - acc: 0.5282 | val_loss: 1.22229 - val_acc: 0.5391 -- iter: 39808/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 312  | total loss: \cf6 1.27957\cf0  | time: 354.960s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.27957 - acc: 0.5308 | val_loss: 1.27968 - val_acc: 0.5312 -- iter: 39936/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 313  | total loss: \cf6 1.26681\cf0  | time: 356.083s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.26681 - acc: 0.5332 | val_loss: 1.31689 - val_acc: 0.5000 -- iter: 40064/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 314  | total loss: \cf6 1.26971\cf0  | time: 357.204s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.26971 - acc: 0.5338 | val_loss: 1.37262 - val_acc: 0.4688 -- iter: 40192/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 315  | total loss: \cf6 1.26494\cf0  | time: 358.323s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.26494 - acc: 0.5335 | val_loss: 1.36912 - val_acc: 0.5156 -- iter: 40320/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 316  | total loss: \cf6 1.25396\cf0  | time: 359.440s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.25396 - acc: 0.5388 | val_loss: 1.41021 - val_acc: 0.5312 -- iter: 40448/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 317  | total loss: \cf6 1.26985\cf0  | time: 360.557s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.26985 - acc: 0.5326 | val_loss: 1.43603 - val_acc: 0.5312 -- iter: 40576/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 318  | total loss: \cf6 1.27198\cf0  | time: 361.677s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.27198 - acc: 0.5348 | val_loss: 1.40492 - val_acc: 0.5625 -- iter: 40704/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 319  | total loss: \cf6 1.27125\cf0  | time: 362.802s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.27125 - acc: 0.5360 | val_loss: 1.41964 - val_acc: 0.5391 -- iter: 40832/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 320  | total loss: \cf6 1.28021\cf0  | time: 363.924s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.28021 - acc: 0.5340 | val_loss: 1.46928 - val_acc: 0.5234 -- iter: 40960/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 321  | total loss: \cf6 1.26480\cf0  | time: 365.043s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.26480 - acc: 0.5399 | val_loss: 1.48470 - val_acc: 0.4922 -- iter: 41088/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 322  | total loss: \cf6 1.25091\cf0  | time: 366.163s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.25091 - acc: 0.5477 | val_loss: 1.44130 - val_acc: 0.5078 -- iter: 41216/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 323  | total loss: \cf6 1.25230\cf0  | time: 367.282s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.25230 - acc: 0.5429 | val_loss: 1.35410 - val_acc: 0.5312 -- iter: 41344/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 324  | total loss: \cf6 1.24385\cf0  | time: 368.402s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.24385 - acc: 0.5472 | val_loss: 1.23443 - val_acc: 0.5625 -- iter: 41472/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 325  | total loss: \cf6 1.25307\cf0  | time: 369.521s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.25307 - acc: 0.5425 | val_loss: 1.15748 - val_acc: 0.5859 -- iter: 41600/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 326  | total loss: \cf6 1.23957\cf0  | time: 370.647s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.23957 - acc: 0.5468 | val_loss: 1.15126 - val_acc: 0.5703 -- iter: 41728/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 327  | total loss: \cf6 1.22365\cf0  | time: 371.763s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.22365 - acc: 0.5539 | val_loss: 1.19601 - val_acc: 0.5625 -- iter: 41856/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 328  | total loss: \cf6 1.21146\cf0  | time: 372.880s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.21146 - acc: 0.5563 | val_loss: 1.25581 - val_acc: 0.5312 -- iter: 41984/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 329  | total loss: \cf6 1.22130\cf0  | time: 374.007s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.22130 - acc: 0.5553 | val_loss: 1.29302 - val_acc: 0.5312 -- iter: 42112/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 330  | total loss: \cf6 1.20891\cf0  | time: 375.128s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.20891 - acc: 0.5592 | val_loss: 1.28684 - val_acc: 0.5234 -- iter: 42240/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 331  | total loss: \cf6 1.20872\cf0  | time: 376.250s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.20872 - acc: 0.5595 | val_loss: 1.23210 - val_acc: 0.5859 -- iter: 42368/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 332  | total loss: \cf6 1.21594\cf0  | time: 377.372s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.21594 - acc: 0.5528 | val_loss: 1.20056 - val_acc: 0.5938 -- iter: 42496/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 333  | total loss: \cf6 1.21475\cf0  | time: 378.492s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.21475 - acc: 0.5530 | val_loss: 1.20507 - val_acc: 0.5781 -- iter: 42624/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 334  | total loss: \cf6 1.21334\cf0  | time: 379.614s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.21334 - acc: 0.5524 | val_loss: 1.25364 - val_acc: 0.5625 -- iter: 42752/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 335  | total loss: \cf6 1.24106\cf0  | time: 380.740s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.24106 - acc: 0.5417 | val_loss: 1.31230 - val_acc: 0.5625 -- iter: 42880/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 336  | total loss: \cf6 1.23246\cf0  | time: 381.860s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.23246 - acc: 0.5461 | val_loss: 1.34413 - val_acc: 0.5547 -- iter: 43008/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 337  | total loss: \cf6 1.23755\cf0  | time: 382.983s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.23755 - acc: 0.5454 | val_loss: 1.35237 - val_acc: 0.5469 -- iter: 43136/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 338  | total loss: \cf6 1.23447\cf0  | time: 384.100s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.23447 - acc: 0.5471 | val_loss: 1.31543 - val_acc: 0.5703 -- iter: 43264/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 339  | total loss: \cf6 1.22810\cf0  | time: 385.219s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.22810 - acc: 0.5525 | val_loss: 1.24702 - val_acc: 0.5469 -- iter: 43392/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 340  | total loss: \cf6 1.23753\cf0  | time: 386.341s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.23753 - acc: 0.5512 | val_loss: 1.19906 - val_acc: 0.5859 -- iter: 43520/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 341  | total loss: \cf6 1.23314\cf0  | time: 387.469s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.23314 - acc: 0.5531 | val_loss: 1.25757 - val_acc: 0.6016 -- iter: 43648/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 342  | total loss: \cf6 1.22618\cf0  | time: 388.589s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.22618 - acc: 0.5540 | val_loss: 1.52768 - val_acc: 0.5469 -- iter: 43776/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 343  | total loss: \cf6 1.21710\cf0  | time: 389.708s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.21710 - acc: 0.5596 | val_loss: 1.80137 - val_acc: 0.5078 -- iter: 43904/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 344  | total loss: \cf6 1.21688\cf0  | time: 390.832s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.21688 - acc: 0.5630 | val_loss: 1.84741 - val_acc: 0.4922 -- iter: 44032/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 345  | total loss: \cf6 1.21862\cf0  | time: 391.965s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.21862 - acc: 0.5622 | val_loss: 1.76219 - val_acc: 0.5078 -- iter: 44160/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 346  | total loss: \cf6 1.22552\cf0  | time: 393.088s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.22552 - acc: 0.5638 | val_loss: 1.67563 - val_acc: 0.5000 -- iter: 44288/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 347  | total loss: \cf6 1.22639\cf0  | time: 394.211s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.22639 - acc: 0.5644 | val_loss: 1.57369 - val_acc: 0.5000 -- iter: 44416/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 348  | total loss: \cf6 1.21957\cf0  | time: 395.334s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.21957 - acc: 0.5658 | val_loss: 1.46731 - val_acc: 0.5391 -- iter: 44544/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 349  | total loss: \cf6 1.21286\cf0  | time: 396.455s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.21286 - acc: 0.5686 | val_loss: 1.42952 - val_acc: 0.5391 -- iter: 44672/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 350  | total loss: \cf6 1.21646\cf0  | time: 397.581s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.21646 - acc: 0.5664 | val_loss: 1.41768 - val_acc: 0.5391 -- iter: 44800/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 351  | total loss: \cf6 1.22920\cf0  | time: 398.701s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.22920 - acc: 0.5574 | val_loss: 1.45826 - val_acc: 0.5547 -- iter: 44928/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 352  | total loss: \cf6 1.20896\cf0  | time: 399.824s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.20896 - acc: 0.5618 | val_loss: 1.51734 - val_acc: 0.5234 -- iter: 45056/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 353  | total loss: \cf6 1.20388\cf0  | time: 400.944s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.20388 - acc: 0.5666 | val_loss: 1.52755 - val_acc: 0.5312 -- iter: 45184/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 354  | total loss: \cf6 1.21003\cf0  | time: 402.064s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.21003 - acc: 0.5654 | val_loss: 1.44981 - val_acc: 0.5312 -- iter: 45312/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 355  | total loss: \cf6 1.20347\cf0  | time: 403.187s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.20347 - acc: 0.5659 | val_loss: 1.36813 - val_acc: 0.5156 -- iter: 45440/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 356  | total loss: \cf6 1.19784\cf0  | time: 404.309s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.19784 - acc: 0.5679 | val_loss: 1.32100 - val_acc: 0.5234 -- iter: 45568/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 357  | total loss: \cf6 1.18900\cf0  | time: 405.431s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.18900 - acc: 0.5744 | val_loss: 1.25490 - val_acc: 0.5547 -- iter: 45696/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 358  | total loss: \cf6 1.19112\cf0  | time: 406.550s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.19112 - acc: 0.5732 | val_loss: 1.21574 - val_acc: 0.5625 -- iter: 45824/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 359  | total loss: \cf6 1.19872\cf0  | time: 407.672s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.19872 - acc: 0.5682 | val_loss: 1.21373 - val_acc: 0.5703 -- iter: 45952/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 360  | total loss: \cf6 1.39958\cf0  | time: 408.789s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.39958 - acc: 0.5333 | val_loss: 1.22935 - val_acc: 0.5703 -- iter: 46080/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 361  | total loss: \cf6 1.40083\cf0  | time: 409.911s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.40083 - acc: 0.5315 | val_loss: 1.19421 - val_acc: 0.5703 -- iter: 46208/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 362  | total loss: \cf6 1.38950\cf0  | time: 411.032s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.38950 - acc: 0.5291 | val_loss: 1.24592 - val_acc: 0.5547 -- iter: 46336/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 363  | total loss: \cf6 1.36493\cf0  | time: 412.158s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.36493 - acc: 0.5317 | val_loss: 1.30001 - val_acc: 0.5469 -- iter: 46464/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 364  | total loss: \cf6 1.34331\cf0  | time: 413.276s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.34331 - acc: 0.5356 | val_loss: 1.28339 - val_acc: 0.5234 -- iter: 46592/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 365  | total loss: \cf6 1.32962\cf0  | time: 414.395s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.32962 - acc: 0.5445 | val_loss: 1.22302 - val_acc: 0.5312 -- iter: 46720/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 366  | total loss: \cf6 1.31520\cf0  | time: 415.511s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.31520 - acc: 0.5471 | val_loss: 1.13018 - val_acc: 0.5625 -- iter: 46848/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 367  | total loss: \cf6 1.30312\cf0  | time: 416.630s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.30312 - acc: 0.5478 | val_loss: 1.11715 - val_acc: 0.5469 -- iter: 46976/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 368  | total loss: \cf6 1.28995\cf0  | time: 417.752s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.28995 - acc: 0.5524 | val_loss: 1.13381 - val_acc: 0.5703 -- iter: 47104/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 369  | total loss: \cf6 1.28208\cf0  | time: 418.876s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.28208 - acc: 0.5573 | val_loss: 1.12288 - val_acc: 0.5938 -- iter: 47232/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 370  | total loss: \cf6 1.29685\cf0  | time: 419.994s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.29685 - acc: 0.5555 | val_loss: 1.12972 - val_acc: 0.5781 -- iter: 47360/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 371  | total loss: \cf6 1.27915\cf0  | time: 421.113s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.27915 - acc: 0.5601 | val_loss: 1.14776 - val_acc: 0.6016 -- iter: 47488/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 372  | total loss: \cf6 1.26438\cf0  | time: 422.232s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.26438 - acc: 0.5619 | val_loss: 1.16364 - val_acc: 0.6094 -- iter: 47616/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 373  | total loss: \cf6 1.25137\cf0  | time: 423.359s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.25137 - acc: 0.5651 | val_loss: 1.14231 - val_acc: 0.6094 -- iter: 47744/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 374  | total loss: \cf6 1.25669\cf0  | time: 424.478s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.25669 - acc: 0.5625 | val_loss: 1.15643 - val_acc: 0.5781 -- iter: 47872/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 375  | total loss: \cf6 1.23824\cf0  | time: 425.601s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.23824 - acc: 0.5680 | val_loss: 1.16855 - val_acc: 0.5625 -- iter: 48000/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 376  | total loss: \cf6 1.24436\cf0  | time: 426.723s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.24436 - acc: 0.5620 | val_loss: 1.18251 - val_acc: 0.5547 -- iter: 48128/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 377  | total loss: \cf6 1.24355\cf0  | time: 427.842s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.24355 - acc: 0.5659 | val_loss: 1.18748 - val_acc: 0.5391 -- iter: 48256/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 378  | total loss: \cf6 1.23595\cf0  | time: 428.961s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.23595 - acc: 0.5687 | val_loss: 1.19905 - val_acc: 0.5703 -- iter: 48384/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 379  | total loss: \cf6 1.24537\cf0  | time: 430.078s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.24537 - acc: 0.5650 | val_loss: 1.20006 - val_acc: 0.5547 -- iter: 48512/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 380  | total loss: \cf6 1.24856\cf0  | time: 431.199s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.24856 - acc: 0.5608 | val_loss: 1.20197 - val_acc: 0.5391 -- iter: 48640/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 381  | total loss: \cf6 1.23814\cf0  | time: 432.326s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.23814 - acc: 0.5657 | val_loss: 1.21155 - val_acc: 0.5234 -- iter: 48768/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 382  | total loss: \cf6 1.23733\cf0  | time: 433.451s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.23733 - acc: 0.5653 | val_loss: 1.20378 - val_acc: 0.5234 -- iter: 48896/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 383  | total loss: \cf6 1.21981\cf0  | time: 434.572s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.21981 - acc: 0.5721 | val_loss: 1.17762 - val_acc: 0.5156 -- iter: 49024/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 384  | total loss: \cf6 1.21731\cf0  | time: 435.692s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.21731 - acc: 0.5704 | val_loss: 1.16444 - val_acc: 0.5625 -- iter: 49152/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 385  | total loss: \cf6 1.21175\cf0  | time: 436.810s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.21175 - acc: 0.5696 | val_loss: 1.16716 - val_acc: 0.5391 -- iter: 49280/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 386  | total loss: \cf6 1.19631\cf0  | time: 437.928s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.19631 - acc: 0.5673 | val_loss: 1.22016 - val_acc: 0.5156 -- iter: 49408/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 387  | total loss: \cf6 1.18304\cf0  | time: 439.045s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.18304 - acc: 0.5692 | val_loss: 1.29244 - val_acc: 0.4922 -- iter: 49536/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 388  | total loss: \cf6 1.17825\cf0  | time: 440.166s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.17825 - acc: 0.5724 | val_loss: 1.33251 - val_acc: 0.5156 -- iter: 49664/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 389  | total loss: \cf6 1.18086\cf0  | time: 441.287s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.18086 - acc: 0.5698 | val_loss: 1.29453 - val_acc: 0.5391 -- iter: 49792/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 390  | total loss: \cf6 1.17950\cf0  | time: 442.404s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.17950 - acc: 0.5699 | val_loss: 1.20399 - val_acc: 0.5859 -- iter: 49920/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 391  | total loss: \cf6 1.18451\cf0  | time: 443.542s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.18451 - acc: 0.5707 | val_loss: 1.14681 - val_acc: 0.6094 -- iter: 50000/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 [ Mon Oct 23 20:29:13 2017 ] Starting Avg ...\cf1\highlight1 
\par \cf0\highlight0 [0.5796600000572204]\cf1\highlight1 
\par \cf0\highlight0 New_sess <tensorflow.python.client.session.Session object at 0x7f82d9abbd10>\cf1\highlight1 
\par \cf0\highlight0 New_sess <tensorflow.python.client.session.Session object at 0x7f82d9abbd10>\cf1\highlight1 
\par \cf0\highlight0 [0.10275999999523162]\cf1\highlight1 
\par \cf0\highlight0 [ Mon Oct 23 20:29:48 2017 ] Avg Done...\cf1\highlight1 
\par \cf0\highlight0 New_sess <tensorflow.python.client.session.Session object at 0x7f82d9abbd10>\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Run id: resnet_cifar10\cf1\highlight1 
\par \cf0\highlight0 Log directory: /tmp/tflearn_logs/\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Training samples: 50000\cf1\highlight1 
\par \cf0\highlight0 Validation samples: 128\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 392  | total loss: \cf6 1.16726\cf0  | time: 2.222s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.16726 - acc: 0.5761 | val_loss: 3.14767 - val_acc: 0.0859 -- iter: 00128/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 393  | total loss: \cf6 1.28581\cf0  | time: 3.341s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.28581 - acc: 0.5326 | val_loss: 3.21195 - val_acc: 0.0859 -- iter: 00256/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 394  | total loss: \cf6 1.38044\cf0  | time: 4.463s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.38044 - acc: 0.4996 | val_loss: 2.87715 - val_acc: 0.0781 -- iter: 00384/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 395  | total loss: \cf6 1.48928\cf0  | time: 5.587s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.48928 - acc: 0.4677 | val_loss: 2.64954 - val_acc: 0.1094 -- iter: 00512/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 396  | total loss: \cf6 1.56329\cf0  | time: 6.713s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.56329 - acc: 0.4420 | val_loss: 2.53510 - val_acc: 0.1250 -- iter: 00640/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 397  | total loss: \cf6 1.62376\cf0  | time: 7.835s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.62376 - acc: 0.4228 | val_loss: 2.43723 - val_acc: 0.2266 -- iter: 00768/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 398  | total loss: \cf6 1.67316\cf0  | time: 8.955s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.67316 - acc: 0.4047 | val_loss: 2.35345 - val_acc: 0.2344 -- iter: 00896/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 399  | total loss: \cf6 1.71088\cf0  | time: 10.077s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.71088 - acc: 0.3885 | val_loss: 2.27712 - val_acc: 0.2344 -- iter: 01024/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 400  | total loss: \cf6 1.74080\cf0  | time: 11.196s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.74080 - acc: 0.3723 | val_loss: 2.19656 - val_acc: 0.2266 -- iter: 01152/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 401  | total loss: \cf6 1.76965\cf0  | time: 12.315s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.76965 - acc: 0.3663 | val_loss: 2.16502 - val_acc: 0.2656 -- iter: 01280/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 402  | total loss: \cf6 1.77266\cf0  | time: 13.435s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.77266 - acc: 0.3609 | val_loss: 2.23296 - val_acc: 0.2344 -- iter: 01408/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 403  | total loss: \cf6 1.78132\cf0  | time: 14.558s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.78132 - acc: 0.3576 | val_loss: 2.20481 - val_acc: 0.2500 -- iter: 01536/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 404  | total loss: \cf6 1.80064\cf0  | time: 15.682s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.80064 - acc: 0.3461 | val_loss: 2.17024 - val_acc: 0.2734 -- iter: 01664/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 405  | total loss: \cf6 1.79376\cf0  | time: 16.805s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.79376 - acc: 0.3443 | val_loss: 2.14123 - val_acc: 0.2578 -- iter: 01792/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 406  | total loss: \cf6 1.80045\cf0  | time: 17.925s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.80045 - acc: 0.3396 | val_loss: 2.12158 - val_acc: 0.2422 -- iter: 01920/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 407  | total loss: \cf6 1.79385\cf0  | time: 19.044s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.79385 - acc: 0.3400 | val_loss: 2.19109 - val_acc: 0.2734 -- iter: 02048/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 408  | total loss: \cf6 1.80041\cf0  | time: 20.165s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.80041 - acc: 0.3310 | val_loss: 1.99758 - val_acc: 0.2500 -- iter: 02176/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 409  | total loss: \cf6 1.81820\cf0  | time: 21.286s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.81820 - acc: 0.3315 | val_loss: 1.93234 - val_acc: 0.2656 -- iter: 02304/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 410  | total loss: \cf6 1.82146\cf0  | time: 22.415s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.82146 - acc: 0.3280 | val_loss: 1.95455 - val_acc: 0.2734 -- iter: 02432/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 411  | total loss: \cf6 1.82133\cf0  | time: 23.538s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.82133 - acc: 0.3280 | val_loss: 1.95381 - val_acc: 0.3203 -- iter: 02560/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 412  | total loss: \cf6 1.82017\cf0  | time: 24.658s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.82017 - acc: 0.3257 | val_loss: 2.10682 - val_acc: 0.2812 -- iter: 02688/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 413  | total loss: \cf6 1.79672\cf0  | time: 25.781s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.79672 - acc: 0.3252 | val_loss: 2.33487 - val_acc: 0.2969 -- iter: 02816/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 414  | total loss: \cf6 1.80097\cf0  | time: 26.915s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.80097 - acc: 0.3286 | val_loss: 2.77376 - val_acc: 0.2891 -- iter: 02944/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 415  | total loss: \cf6 1.80089\cf0  | time: 28.043s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.80089 - acc: 0.3301 | val_loss: 2.64454 - val_acc: 0.2891 -- iter: 03072/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 416  | total loss: \cf6 1.80407\cf0  | time: 29.164s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.80407 - acc: 0.3236 | val_loss: 2.57939 - val_acc: 0.2734 -- iter: 03200/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 417  | total loss: \cf6 1.81218\cf0  | time: 30.285s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.81218 - acc: 0.3210 | val_loss: 2.35580 - val_acc: 0.2891 -- iter: 03328/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 418  | total loss: \cf6 1.80868\cf0  | time: 31.405s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.80868 - acc: 0.3147 | val_loss: 2.40885 - val_acc: 0.2812 -- iter: 03456/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 419  | total loss: \cf6 1.80426\cf0  | time: 32.529s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.80426 - acc: 0.3144 | val_loss: 2.85737 - val_acc: 0.2188 -- iter: 03584/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 420  | total loss: \cf6 1.80668\cf0  | time: 33.651s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.80668 - acc: 0.3166 | val_loss: 2.90969 - val_acc: 0.2188 -- iter: 03712/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 421  | total loss: \cf6 1.81133\cf0  | time: 34.775s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.81133 - acc: 0.3177 | val_loss: 2.64646 - val_acc: 0.1953 -- iter: 03840/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 422  | total loss: \cf6 1.80355\cf0  | time: 35.896s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.80355 - acc: 0.3266 | val_loss: 2.21420 - val_acc: 0.2266 -- iter: 03968/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 423  | total loss: \cf6 1.78856\cf0  | time: 37.020s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.78856 - acc: 0.3330 | val_loss: 2.11957 - val_acc: 0.2500 -- iter: 04096/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 424  | total loss: \cf6 1.77703\cf0  | time: 38.145s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.77703 - acc: 0.3302 | val_loss: 2.03706 - val_acc: 0.2656 -- iter: 04224/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 425  | total loss: \cf6 1.78326\cf0  | time: 39.265s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.78326 - acc: 0.3284 | val_loss: 1.96493 - val_acc: 0.3047 -- iter: 04352/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 426  | total loss: \cf6 1.76657\cf0  | time: 40.386s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.76657 - acc: 0.3385 | val_loss: 1.81573 - val_acc: 0.3281 -- iter: 04480/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 427  | total loss: \cf6 1.77439\cf0  | time: 41.506s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.77439 - acc: 0.3351 | val_loss: 1.72093 - val_acc: 0.3125 -- iter: 04608/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 428  | total loss: \cf6 1.76216\cf0  | time: 42.640s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.76216 - acc: 0.3415 | val_loss: 1.72978 - val_acc: 0.3203 -- iter: 04736/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 429  | total loss: \cf6 1.75990\cf0  | time: 43.760s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.75990 - acc: 0.3417 | val_loss: 1.80470 - val_acc: 0.2891 -- iter: 04864/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 430  | total loss: \cf6 1.76010\cf0  | time: 44.879s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.76010 - acc: 0.3442 | val_loss: 1.77568 - val_acc: 0.3047 -- iter: 04992/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 431  | total loss: \cf6 1.76312\cf0  | time: 46.002s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.76312 - acc: 0.3411 | val_loss: 1.73363 - val_acc: 0.3281 -- iter: 05120/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 432  | total loss: \cf6 1.74800\cf0  | time: 47.126s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.74800 - acc: 0.3531 | val_loss: 1.72362 - val_acc: 0.3516 -- iter: 05248/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 433  | total loss: \cf6 1.72086\cf0  | time: 48.245s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.72086 - acc: 0.3709 | val_loss: 1.71702 - val_acc: 0.3672 -- iter: 05376/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 434  | total loss: \cf6 1.70337\cf0  | time: 49.367s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.70337 - acc: 0.3697 | val_loss: 1.70133 - val_acc: 0.4297 -- iter: 05504/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 435  | total loss: \cf6 1.69493\cf0  | time: 50.484s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.69493 - acc: 0.3749 | val_loss: 1.67820 - val_acc: 0.4297 -- iter: 05632/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 436  | total loss: \cf6 1.70126\cf0  | time: 51.615s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.70126 - acc: 0.3734 | val_loss: 1.65492 - val_acc: 0.4219 -- iter: 05760/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 437  | total loss: \cf6 1.69359\cf0  | time: 52.740s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.69359 - acc: 0.3743 | val_loss: 1.65266 - val_acc: 0.4219 -- iter: 05888/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 438  | total loss: \cf6 1.69083\cf0  | time: 53.863s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.69083 - acc: 0.3775 | val_loss: 1.64311 - val_acc: 0.4219 -- iter: 06016/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 439  | total loss: \cf6 1.66912\cf0  | time: 54.981s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.66912 - acc: 0.3812 | val_loss: 1.63419 - val_acc: 0.3828 -- iter: 06144/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 440  | total loss: \cf6 1.66130\cf0  | time: 56.101s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.66130 - acc: 0.3860 | val_loss: 1.65086 - val_acc: 0.3906 -- iter: 06272/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 441  | total loss: \cf6 1.65882\cf0  | time: 57.222s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.65882 - acc: 0.3865 | val_loss: 1.62830 - val_acc: 0.3594 -- iter: 06400/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 442  | total loss: \cf6 1.66460\cf0  | time: 58.345s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.66460 - acc: 0.3807 | val_loss: 1.74167 - val_acc: 0.3203 -- iter: 06528/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 443  | total loss: \cf6 1.65148\cf0  | time: 59.478s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.65148 - acc: 0.3770 | val_loss: 1.78044 - val_acc: 0.2500 -- iter: 06656/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 444  | total loss: \cf6 1.63521\cf0  | time: 60.600s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.63521 - acc: 0.3830 | val_loss: 1.69763 - val_acc: 0.3125 -- iter: 06784/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 445  | total loss: \cf6 1.63516\cf0  | time: 61.731s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.63516 - acc: 0.3814 | val_loss: 1.69287 - val_acc: 0.3594 -- iter: 06912/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 446  | total loss: \cf6 1.62570\cf0  | time: 62.862s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.62570 - acc: 0.3847 | val_loss: 1.75768 - val_acc: 0.4297 -- iter: 07040/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 447  | total loss: \cf6 1.62658\cf0  | time: 63.993s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.62658 - acc: 0.3814 | val_loss: 1.75983 - val_acc: 0.4219 -- iter: 07168/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 448  | total loss: \cf6 1.63321\cf0  | time: 65.128s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.63321 - acc: 0.3784 | val_loss: 1.74974 - val_acc: 0.3750 -- iter: 07296/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 449  | total loss: \cf6 1.63569\cf0  | time: 66.252s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.63569 - acc: 0.3765 | val_loss: 1.80889 - val_acc: 0.3906 -- iter: 07424/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 450  | total loss: \cf6 1.63775\cf0  | time: 67.382s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.63775 - acc: 0.3779 | val_loss: 1.84566 - val_acc: 0.4141 -- iter: 07552/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 451  | total loss: \cf6 1.61667\cf0  | time: 68.508s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.61667 - acc: 0.3870 | val_loss: 1.76133 - val_acc: 0.4766 -- iter: 07680/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 452  | total loss: \cf6 1.61742\cf0  | time: 69.638s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.61742 - acc: 0.3866 | val_loss: 1.64096 - val_acc: 0.4531 -- iter: 07808/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 453  | total loss: \cf6 1.62096\cf0  | time: 70.758s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.62096 - acc: 0.3823 | val_loss: 1.60342 - val_acc: 0.4297 -- iter: 07936/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 454  | total loss: \cf6 1.62411\cf0  | time: 71.890s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.62411 - acc: 0.3831 | val_loss: 1.58197 - val_acc: 0.4141 -- iter: 08064/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 455  | total loss: \cf6 1.61874\cf0  | time: 73.026s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.61874 - acc: 0.3823 | val_loss: 1.61703 - val_acc: 0.4297 -- iter: 08192/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 456  | total loss: \cf6 1.60080\cf0  | time: 74.157s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.60080 - acc: 0.3871 | val_loss: 1.72556 - val_acc: 0.3828 -- iter: 08320/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 457  | total loss: \cf6 1.59830\cf0  | time: 75.280s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.59830 - acc: 0.3851 | val_loss: 1.77137 - val_acc: 0.3281 -- iter: 08448/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 458  | total loss: \cf6 1.60318\cf0  | time: 76.408s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.60318 - acc: 0.3809 | val_loss: 1.74403 - val_acc: 0.3203 -- iter: 08576/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 459  | total loss: \cf6 1.60808\cf0  | time: 77.527s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.60808 - acc: 0.3764 | val_loss: 1.74986 - val_acc: 0.3438 -- iter: 08704/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 460  | total loss: \cf6 1.60389\cf0  | time: 78.648s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.60389 - acc: 0.3747 | val_loss: 1.73624 - val_acc: 0.3516 -- iter: 08832/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 461  | total loss: \cf6 1.59407\cf0  | time: 79.768s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.59407 - acc: 0.3802 | val_loss: 1.73952 - val_acc: 0.3281 -- iter: 08960/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 462  | total loss: \cf6 1.59493\cf0  | time: 80.892s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.59493 - acc: 0.3844 | val_loss: 1.74809 - val_acc: 0.3281 -- iter: 09088/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 463  | total loss: \cf6 1.58232\cf0  | time: 82.013s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.58232 - acc: 0.3897 | val_loss: 1.54815 - val_acc: 0.4766 -- iter: 09216/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 464  | total loss: \cf6 1.57967\cf0  | time: 83.132s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.57967 - acc: 0.3976 | val_loss: 1.42556 - val_acc: 0.5547 -- iter: 09344/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 465  | total loss: \cf6 1.57148\cf0  | time: 84.252s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.57148 - acc: 0.3953 | val_loss: 1.42207 - val_acc: 0.5234 -- iter: 09472/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 466  | total loss: \cf6 1.57511\cf0  | time: 85.370s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.57511 - acc: 0.3957 | val_loss: 1.40306 - val_acc: 0.4844 -- iter: 09600/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 467  | total loss: \cf6 1.56689\cf0  | time: 86.487s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.56689 - acc: 0.4030 | val_loss: 1.40649 - val_acc: 0.4609 -- iter: 09728/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 468  | total loss: \cf6 1.55744\cf0  | time: 87.604s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.55744 - acc: 0.4080 | val_loss: 1.42654 - val_acc: 0.4219 -- iter: 09856/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 469  | total loss: \cf6 1.54671\cf0  | time: 88.729s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.54671 - acc: 0.4180 | val_loss: 1.45238 - val_acc: 0.4297 -- iter: 09984/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 470  | total loss: \cf6 1.53808\cf0  | time: 89.848s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.53808 - acc: 0.4246 | val_loss: 1.50362 - val_acc: 0.4453 -- iter: 10112/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 471  | total loss: \cf6 1.52912\cf0  | time: 90.967s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.52912 - acc: 0.4314 | val_loss: 1.53018 - val_acc: 0.4453 -- iter: 10240/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 472  | total loss: \cf6 1.51209\cf0  | time: 92.083s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.51209 - acc: 0.4414 | val_loss: 1.54537 - val_acc: 0.4375 -- iter: 10368/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 473  | total loss: \cf6 1.50331\cf0  | time: 93.204s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.50331 - acc: 0.4441 | val_loss: 1.53521 - val_acc: 0.4531 -- iter: 10496/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 474  | total loss: \cf6 1.50867\cf0  | time: 94.319s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.50867 - acc: 0.4419 | val_loss: 1.50214 - val_acc: 0.4453 -- iter: 10624/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 475  | total loss: \cf6 1.51467\cf0  | time: 95.438s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.51467 - acc: 0.4407 | val_loss: 1.59233 - val_acc: 0.3906 -- iter: 10752/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 476  | total loss: \cf6 1.49655\cf0  | time: 96.562s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.49655 - acc: 0.4536 | val_loss: 1.75213 - val_acc: 0.3672 -- iter: 10880/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 477  | total loss: \cf6 1.47643\cf0  | time: 97.683s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.47643 - acc: 0.4567 | val_loss: 1.87202 - val_acc: 0.3359 -- iter: 11008/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 478  | total loss: \cf6 1.46026\cf0  | time: 98.818s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.46026 - acc: 0.4618 | val_loss: 1.82088 - val_acc: 0.3672 -- iter: 11136/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 479  | total loss: \cf6 1.45757\cf0  | time: 99.939s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.45757 - acc: 0.4641 | val_loss: 1.69771 - val_acc: 0.3750 -- iter: 11264/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 480  | total loss: \cf6 1.46411\cf0  | time: 100.056s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.46411 - acc: 0.4653 | val_loss: 0.00000 - val_acc: 0.0000 -- iter: 11392/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 481  | total loss: \cf6 1.45941\cf0  | time: 101.176s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.45941 - acc: 0.4672 | val_loss: 1.60539 - val_acc: 0.3984 -- iter: 11520/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 482  | total loss: \cf6 1.44155\cf0  | time: 102.292s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.44155 - acc: 0.4697 | val_loss: 1.61088 - val_acc: 0.4219 -- iter: 11648/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 483  | total loss: \cf6 1.43700\cf0  | time: 103.411s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.43700 - acc: 0.4743 | val_loss: 1.55729 - val_acc: 0.4531 -- iter: 11776/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 484  | total loss: \cf6 1.43831\cf0  | time: 104.529s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.43831 - acc: 0.4784 | val_loss: 1.43207 - val_acc: 0.5391 -- iter: 11904/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 485  | total loss: \cf6 1.44494\cf0  | time: 105.650s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.44494 - acc: 0.4743 | val_loss: 1.36584 - val_acc: 0.5234 -- iter: 12032/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 486  | total loss: \cf6 1.43311\cf0  | time: 106.767s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.43311 - acc: 0.4793 | val_loss: 1.36682 - val_acc: 0.4688 -- iter: 12160/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 487  | total loss: \cf6 1.43887\cf0  | time: 107.899s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.43887 - acc: 0.4704 | val_loss: 1.44371 - val_acc: 0.4609 -- iter: 12288/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 488  | total loss: \cf6 1.44292\cf0  | time: 109.019s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.44292 - acc: 0.4632 | val_loss: 1.56811 - val_acc: 0.4609 -- iter: 12416/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 489  | total loss: \cf6 1.45206\cf0  | time: 110.140s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.45206 - acc: 0.4606 | val_loss: 1.71109 - val_acc: 0.4297 -- iter: 12544/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 490  | total loss: \cf6 1.46227\cf0  | time: 111.260s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.46227 - acc: 0.4544 | val_loss: 1.88573 - val_acc: 0.4688 -- iter: 12672/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 491  | total loss: \cf6 1.44467\cf0  | time: 112.388s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.44467 - acc: 0.4629 | val_loss: 2.12618 - val_acc: 0.3984 -- iter: 12800/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 492  | total loss: \cf6 1.43080\cf0  | time: 113.508s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.43080 - acc: 0.4674 | val_loss: 2.25619 - val_acc: 0.3828 -- iter: 12928/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 493  | total loss: \cf6 1.45076\cf0  | time: 114.629s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.45076 - acc: 0.4581 | val_loss: 2.05060 - val_acc: 0.3984 -- iter: 13056/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 494  | total loss: \cf6 1.43884\cf0  | time: 115.746s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.43884 - acc: 0.4615 | val_loss: 1.82087 - val_acc: 0.4219 -- iter: 13184/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 495  | total loss: \cf6 1.43549\cf0  | time: 116.866s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.43549 - acc: 0.4630 | val_loss: 1.62471 - val_acc: 0.4062 -- iter: 13312/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 496  | total loss: \cf6 1.44377\cf0  | time: 117.988s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.44377 - acc: 0.4542 | val_loss: 1.55174 - val_acc: 0.4375 -- iter: 13440/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 497  | total loss: \cf6 1.44974\cf0  | time: 119.105s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.44974 - acc: 0.4565 | val_loss: 1.49105 - val_acc: 0.4609 -- iter: 13568/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 498  | total loss: \cf6 1.44590\cf0  | time: 120.224s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.44590 - acc: 0.4632 | val_loss: 1.43668 - val_acc: 0.4688 -- iter: 13696/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 499  | total loss: \cf6 1.43867\cf0  | time: 121.341s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.43867 - acc: 0.4668 | val_loss: 1.39702 - val_acc: 0.5078 -- iter: 13824/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 500  | total loss: \cf6 1.43704\cf0  | time: 122.460s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.43704 - acc: 0.4749 | val_loss: 1.36216 - val_acc: 0.4844 -- iter: 13952/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 501  | total loss: \cf6 1.43835\cf0  | time: 123.581s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.43835 - acc: 0.4750 | val_loss: 1.36555 - val_acc: 0.4922 -- iter: 14080/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 502  | total loss: \cf6 1.43173\cf0  | time: 123.700s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.43173 - acc: 0.4783 | val_loss: 0.00000 - val_acc: 0.0000 -- iter: 14208/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 503  | total loss: \cf6 1.42019\cf0  | time: 124.820s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.42019 - acc: 0.4805 | val_loss: 1.39388 - val_acc: 0.4766 -- iter: 14336/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 504  | total loss: \cf6 1.42159\cf0  | time: 125.939s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.42159 - acc: 0.4762 | val_loss: 1.50001 - val_acc: 0.4219 -- iter: 14464/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 505  | total loss: \cf6 1.41725\cf0  | time: 127.058s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.41725 - acc: 0.4778 | val_loss: 1.49893 - val_acc: 0.3984 -- iter: 14592/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 506  | total loss: \cf6 1.40806\cf0  | time: 128.173s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.40806 - acc: 0.4792 | val_loss: 1.47114 - val_acc: 0.4141 -- iter: 14720/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 507  | total loss: \cf6 1.41090\cf0  | time: 129.293s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.41090 - acc: 0.4766 | val_loss: 1.43984 - val_acc: 0.4844 -- iter: 14848/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 508  | total loss: \cf6 1.39352\cf0  | time: 130.413s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.39352 - acc: 0.4852 | val_loss: 1.41305 - val_acc: 0.5234 -- iter: 14976/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 509  | total loss: \cf6 1.39113\cf0  | time: 131.535s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.39113 - acc: 0.4906 | val_loss: 1.38455 - val_acc: 0.5391 -- iter: 15104/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 510  | total loss: \cf6 1.38081\cf0  | time: 132.652s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.38081 - acc: 0.4978 | val_loss: 1.38030 - val_acc: 0.5234 -- iter: 15232/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 511  | total loss: \cf6 1.35453\cf0  | time: 133.772s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.35453 - acc: 0.5042 | val_loss: 1.40276 - val_acc: 0.5156 -- iter: 15360/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 512  | total loss: \cf6 1.34936\cf0  | time: 134.891s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.34936 - acc: 0.5030 | val_loss: 1.44004 - val_acc: 0.5078 -- iter: 15488/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 513  | total loss: \cf6 1.33599\cf0  | time: 136.011s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.33599 - acc: 0.5066 | val_loss: 1.45775 - val_acc: 0.4922 -- iter: 15616/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 514  | total loss: \cf6 1.32775\cf0  | time: 137.133s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.32775 - acc: 0.5122 | val_loss: 1.41489 - val_acc: 0.5000 -- iter: 15744/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 515  | total loss: \cf6 1.32755\cf0  | time: 138.250s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.32755 - acc: 0.5126 | val_loss: 1.37659 - val_acc: 0.5156 -- iter: 15872/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 516  | total loss: \cf6 1.32740\cf0  | time: 139.367s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.32740 - acc: 0.5152 | val_loss: 1.37534 - val_acc: 0.5078 -- iter: 16000/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 517  | total loss: \cf6 1.33572\cf0  | time: 140.497s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.33572 - acc: 0.5129 | val_loss: 1.39872 - val_acc: 0.5078 -- iter: 16128/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 518  | total loss: \cf6 1.33107\cf0  | time: 141.615s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.33107 - acc: 0.5163 | val_loss: 1.41559 - val_acc: 0.5000 -- iter: 16256/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 519  | total loss: \cf6 1.34300\cf0  | time: 142.736s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.34300 - acc: 0.5131 | val_loss: 1.44788 - val_acc: 0.5078 -- iter: 16384/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 520  | total loss: \cf6 1.33375\cf0  | time: 143.855s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.33375 - acc: 0.5157 | val_loss: 1.45838 - val_acc: 0.5000 -- iter: 16512/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 521  | total loss: \cf6 1.34549\cf0  | time: 144.973s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.34549 - acc: 0.5141 | val_loss: 1.44783 - val_acc: 0.4844 -- iter: 16640/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 522  | total loss: \cf6 1.35499\cf0  | time: 146.092s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.35499 - acc: 0.5088 | val_loss: 1.38674 - val_acc: 0.5078 -- iter: 16768/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 523  | total loss: \cf6 1.36206\cf0  | time: 147.210s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.36206 - acc: 0.5142 | val_loss: 1.38113 - val_acc: 0.5078 -- iter: 16896/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 524  | total loss: \cf6 1.37011\cf0  | time: 148.329s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.37011 - acc: 0.5096 | val_loss: 1.37453 - val_acc: 0.4922 -- iter: 17024/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 ^CTraceback (most recent call last):\cf1\highlight1 
\par \cf0\highlight0   File "residual_network_cifar10.py", line 115, in <module>\cf1\highlight1 
\par \cf0\highlight0     run_id='resnet_cifar10')\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tflearn/models/dnn.py", line 216, in fit\cf1\highlight1 
\par \cf0\highlight0     callbacks=callbacks)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tflearn/helpers/trainer.py", line 338, in fit\cf1\highlight1 
\par \cf0\highlight0     show_metric)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tflearn/helpers/trainer.py", line 846, in _train\cf1\highlight1 
\par \cf0\highlight0     e = evaluate_flow(self.session, eval_ops, self.test_dflow)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tflearn/helpers/trainer.py", line 1001, in evaluate_flow\cf1\highlight1 
\par \cf0\highlight0     feed_batch = dataflow.next()\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tflearn/data_flow.py", line 129, in next\cf1\highlight1 
\par \cf0\highlight0     return self.feed_dict_queue.get(timeout=timeout)\cf1\highlight1 
\par \cf0\highlight0   File "/usr/lib/python2.7/Queue.py", line 168, in get\cf1\highlight1 
\par \cf0\highlight0     self.not_empty.wait()\cf1\highlight1 
\par \cf0\highlight0   File "/usr/lib/python2.7/threading.py", line 339, in wait\cf1\highlight1 
\par \cf0\highlight0     waiter.acquire()\cf1\highlight1 
\par \cf0\highlight0 KeyboardInterrupt\cf1\highlight1 
\par \cf0\highlight0 (tf) xdong@gpu2:/data/xdong/codes/avg_para$ CUDA_VISIBLE_DEVICES=0 python residual_network_cifar10.py\cf1\highlight1 
\par \cf0\highlight0 Couldn't import dot_parser, loading of dot files will not be possible.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 20:52:22.909242: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could spePU computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 20:52:22.909272: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could spePU computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 20:52:22.909280: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 20:52:22.909286: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 20:52:22.909303: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 20:52:23.470206: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties:\cf1\highlight1 
\par \cf0\highlight0 name: GeForce GTX TITAN X\cf1\highlight1 
\par \cf0\highlight0 major: 5 minor: 2 memoryClockRate (GHz) 1.076\cf1\highlight1 
\par \cf0\highlight0 pciBusID 0000:02:00.0\cf1\highlight1 
\par \cf0\highlight0 Total memory: 11.92GiB\cf1\highlight1 
\par \cf0\highlight0 Free memory: 11.81GiB\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 20:52:23.470345: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 20:52:23.470364: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 20:52:23.470383: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:02:00.0)\cf1\highlight1 
\par \cf0\highlight0 2017-10-23 20:52:41.678757: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:02:00.0)\cf1\highlight1 
\par \cf0\highlight0 first <tensorflow.python.client.session.Session object at 0x7feedc6237d0>\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Run id: resnet_cifar10\cf1\highlight1 
\par \cf0\highlight0 Log directory: /tmp/tflearn_logs/\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Preprocessing... Calculating mean over all dataset (this may take long)...\cf1\highlight1 
\par \cf0\highlight0 Mean: [ 0.49139968  0.48215841  0.44653091] (To avoid repetitive computation, add it to argument 'mean' of `add_featurewise_zero_center`)\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Training samples: 50000\cf1\highlight1 
\par \cf0\highlight0 Validation samples: 128\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 391  | total loss: \cf6 1.23309\cf0  | time: 51.672s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 001 | loss: 1.23309 - acc: 0.5604 | val_loss: 1.22418 - val_acc: 0.5781 -- iter: 50000/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 [ Mon Oct 23 20:53:39 2017 ] Starting Avg ...\cf1\highlight1 
\par \cf0\highlight0 [0.578125]\cf1\highlight1 
\par \cf0\highlight0 New_sess <tensorflow.python.client.session.Session object at 0x7feedc6237d0>\cf1\highlight1 
\par \cf0\highlight0 New_sess <tensorflow.python.client.session.Session object at 0x7feedc6237d0>\cf1\highlight1 
\par \cf0\highlight0 [0.078125]\cf1\highlight1 
\par \cf0\highlight0 [ Mon Oct 23 20:53:57 2017 ] Avg Done...\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Run id: resnet_cifar10\cf1\highlight1 
\par \cf0\highlight0 Log directory: /tmp/tflearn_logs/\cf1\highlight1 
\par \cf0\highlight0 ---------------------------------\cf1\highlight1 
\par \cf0\highlight0 Training samples: 50000\cf1\highlight1 
\par \cf0\highlight0 Validation samples: 128\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 392  | total loss: \cf6 1.24763\cf0  | time: 2.144s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.24763 - acc: 0.5469 | val_loss: 3.41752 - val_acc: 0.1094 -- iter: 00128/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 393  | total loss: \cf6 1.35407\cf0  | time: 3.261s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.35407 - acc: 0.5101 | val_loss: 3.41715 - val_acc: 0.1016 -- iter: 00256/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 394  | total loss: \cf6 1.43722\cf0  | time: 4.374s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.43722 - acc: 0.4818 | val_loss: 3.40668 - val_acc: 0.0938 -- iter: 00384/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 395  | total loss: \cf6 1.50656\cf0  | time: 5.492s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.50656 - acc: 0.4570 | val_loss: 3.46577 - val_acc: 0.1016 -- iter: 00512/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 396  | total loss: \cf6 1.53744\cf0  | time: 6.613s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.53744 - acc: 0.4426 | val_loss: 3.56125 - val_acc: 0.1016 -- iter: 00640/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 397  | total loss: \cf6 1.55182\cf0  | time: 7.732s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.55182 - acc: 0.4343 | val_loss: 3.35412 - val_acc: 0.0859 -- iter: 00768/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 398  | total loss: \cf6 1.58539\cf0  | time: 8.849s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.58539 - acc: 0.4237 | val_loss: 3.14091 - val_acc: 0.0938 -- iter: 00896/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 399  | total loss: \cf6 1.62263\cf0  | time: 9.964s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.62263 - acc: 0.4079 | val_loss: 2.99943 - val_acc: 0.1016 -- iter: 01024/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 400  | total loss: \cf6 1.63844\cf0  | time: 11.083s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.63844 - acc: 0.3983 | val_loss: 2.90376 - val_acc: 0.1250 -- iter: 01152/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 401  | total loss: \cf6 1.66383\cf0  | time: 12.204s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.66383 - acc: 0.3905 | val_loss: 2.81763 - val_acc: 0.1250 -- iter: 01280/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 402  | total loss: \cf6 1.68823\cf0  | time: 13.329s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.68823 - acc: 0.3788 | val_loss: 2.67176 - val_acc: 0.1250 -- iter: 01408/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 403  | total loss: \cf6 1.71554\cf0  | time: 14.446s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.71554 - acc: 0.3706 | val_loss: 2.49426 - val_acc: 0.1250 -- iter: 01536/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 404  | total loss: \cf6 1.73722\cf0  | time: 15.565s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.73722 - acc: 0.3687 | val_loss: 2.40040 - val_acc: 0.1406 -- iter: 01664/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 405  | total loss: \cf6 1.74369\cf0  | time: 16.684s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.74369 - acc: 0.3631 | val_loss: 2.32965 - val_acc: 0.1562 -- iter: 01792/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 406  | total loss: \cf6 1.75183\cf0  | time: 17.807s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.75183 - acc: 0.3580 | val_loss: 2.31162 - val_acc: 0.1641 -- iter: 01920/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 407  | total loss: \cf6 1.75780\cf0  | time: 18.926s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.75780 - acc: 0.3566 | val_loss: 2.18050 - val_acc: 0.1719 -- iter: 02048/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 408  | total loss: \cf6 1.76212\cf0  | time: 20.050s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.76212 - acc: 0.3498 | val_loss: 2.06943 - val_acc: 0.1875 -- iter: 02176/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 409  | total loss: \cf6 1.76210\cf0  | time: 21.170s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.76210 - acc: 0.3469 | val_loss: 2.03842 - val_acc: 0.2266 -- iter: 02304/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 410  | total loss: \cf6 1.76729\cf0  | time: 22.289s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.76729 - acc: 0.3388 | val_loss: 1.97437 - val_acc: 0.2422 -- iter: 02432/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 411  | total loss: \cf6 1.77666\cf0  | time: 23.411s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.77666 - acc: 0.3291 | val_loss: 1.86766 - val_acc: 0.2500 -- iter: 02560/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 412  | total loss: \cf6 1.76351\cf0  | time: 24.535s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.76351 - acc: 0.3415 | val_loss: 1.93411 - val_acc: 0.2578 -- iter: 02688/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 413  | total loss: \cf6 1.75804\cf0  | time: 25.654s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.75804 - acc: 0.3402 | val_loss: 2.18043 - val_acc: 0.2891 -- iter: 02816/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 414  | total loss: \cf6 1.75037\cf0  | time: 26.774s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.75037 - acc: 0.3483 | val_loss: 2.69520 - val_acc: 0.2656 -- iter: 02944/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 415  | total loss: \cf6 1.73624\cf0  | time: 27.893s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.73624 - acc: 0.3557 | val_loss: 2.99736 - val_acc: 0.2031 -- iter: 03072/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 416  | total loss: \cf6 1.73202\cf0  | time: 29.017s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.73202 - acc: 0.3537 | val_loss: 3.29650 - val_acc: 0.2266 -- iter: 03200/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 417  | total loss: \cf6 1.73151\cf0  | time: 30.136s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.73151 - acc: 0.3598 | val_loss: 4.02974 - val_acc: 0.2266 -- iter: 03328/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 418  | total loss: \cf6 1.72316\cf0  | time: 31.255s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.72316 - acc: 0.3636 | val_loss: 3.98678 - val_acc: 0.2422 -- iter: 03456/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 419  | total loss: \cf6 1.71994\cf0  | time: 32.381s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.71994 - acc: 0.3679 | val_loss: 3.21362 - val_acc: 0.2344 -- iter: 03584/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 420  | total loss: \cf6 1.70197\cf0  | time: 33.500s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.70197 - acc: 0.3780 | val_loss: 2.88768 - val_acc: 0.2109 -- iter: 03712/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 421  | total loss: \cf6 1.68917\cf0  | time: 34.621s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.68917 - acc: 0.3785 | val_loss: 2.77315 - val_acc: 0.1953 -- iter: 03840/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 422  | total loss: \cf6 1.69366\cf0  | time: 35.742s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.69366 - acc: 0.3797 | val_loss: 2.55758 - val_acc: 0.2109 -- iter: 03968/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 423  | total loss: \cf6 1.70238\cf0  | time: 36.864s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.70238 - acc: 0.3815 | val_loss: 2.42044 - val_acc: 0.2422 -- iter: 04096/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 424  | total loss: \cf6 1.69303\cf0  | time: 37.998s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.69303 - acc: 0.3832 | val_loss: 2.47891 - val_acc: 0.2578 -- iter: 04224/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 425  | total loss: \cf6 1.70127\cf0  | time: 39.115s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.70127 - acc: 0.3762 | val_loss: 2.36050 - val_acc: 0.2422 -- iter: 04352/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 426  | total loss: \cf6 1.68986\cf0  | time: 40.234s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.68986 - acc: 0.3815 | val_loss: 2.16534 - val_acc: 0.2578 -- iter: 04480/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 427  | total loss: \cf6 1.69580\cf0  | time: 41.352s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.69580 - acc: 0.3770 | val_loss: 2.26669 - val_acc: 0.2578 -- iter: 04608/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 428  | total loss: \cf6 1.69798\cf0  | time: 42.467s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.69798 - acc: 0.3775 | val_loss: 2.38180 - val_acc: 0.2500 -- iter: 04736/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 429  | total loss: \cf6 1.68885\cf0  | time: 43.584s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.68885 - acc: 0.3812 | val_loss: 2.48662 - val_acc: 0.2891 -- iter: 04864/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 430  | total loss: \cf6 1.68424\cf0  | time: 44.703s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.68424 - acc: 0.3853 | val_loss: 2.51911 - val_acc: 0.2344 -- iter: 04992/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 431  | total loss: \cf6 1.67827\cf0  | time: 45.819s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.67827 - acc: 0.3889 | val_loss: 2.59180 - val_acc: 0.2109 -- iter: 05120/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 432  | total loss: \cf6 1.66656\cf0  | time: 46.939s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.66656 - acc: 0.3938 | val_loss: 2.53465 - val_acc: 0.2422 -- iter: 05248/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 433  | total loss: \cf6 1.67303\cf0  | time: 48.056s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.67303 - acc: 0.3974 | val_loss: 2.46605 - val_acc: 0.3203 -- iter: 05376/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 434  | total loss: \cf6 1.66569\cf0  | time: 49.174s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.66569 - acc: 0.4053 | val_loss: 2.07041 - val_acc: 0.3203 -- iter: 05504/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 435  | total loss: \cf6 1.66832\cf0  | time: 50.295s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.66832 - acc: 0.4038 | val_loss: 1.87459 - val_acc: 0.3438 -- iter: 05632/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 436  | total loss: \cf6 1.65688\cf0  | time: 51.412s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.65688 - acc: 0.3994 | val_loss: 1.81005 - val_acc: 0.3672 -- iter: 05760/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 437  | total loss: \cf6 1.65113\cf0  | time: 52.526s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.65113 - acc: 0.4008 | val_loss: 1.90874 - val_acc: 0.3672 -- iter: 05888/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 438  | total loss: \cf6 1.64593\cf0  | time: 53.644s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.64593 - acc: 0.4045 | val_loss: 2.21447 - val_acc: 0.2734 -- iter: 06016/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 439  | total loss: \cf6 1.64398\cf0  | time: 54.760s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.64398 - acc: 0.4016 | val_loss: 2.35112 - val_acc: 0.2500 -- iter: 06144/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 440  | total loss: \cf6 1.64526\cf0  | time: 55.880s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.64526 - acc: 0.3981 | val_loss: 2.44382 - val_acc: 0.2422 -- iter: 06272/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 441  | total loss: \cf6 1.63688\cf0  | time: 56.998s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.63688 - acc: 0.4005 | val_loss: 2.49764 - val_acc: 0.2344 -- iter: 06400/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 442  | total loss: \cf6 1.62465\cf0  | time: 58.114s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.62465 - acc: 0.4058 | val_loss: 2.47844 - val_acc: 0.2344 -- iter: 06528/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 443  | total loss: \cf6 1.61195\cf0  | time: 59.237s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.61195 - acc: 0.4121 | val_loss: 2.31257 - val_acc: 0.2812 -- iter: 06656/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 444  | total loss: \cf6 1.61146\cf0  | time: 60.353s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.61146 - acc: 0.4123 | val_loss: 2.04030 - val_acc: 0.3359 -- iter: 06784/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 445  | total loss: \cf6 1.60028\cf0  | time: 61.468s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.60028 - acc: 0.4148 | val_loss: 1.76628 - val_acc: 0.4141 -- iter: 06912/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 446  | total loss: \cf6 1.62176\cf0  | time: 62.587s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.62176 - acc: 0.4069 | val_loss: 1.59428 - val_acc: 0.4062 -- iter: 07040/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 447  | total loss: \cf6 1.61851\cf0  | time: 63.704s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.61851 - acc: 0.4053 | val_loss: 1.63693 - val_acc: 0.3906 -- iter: 07168/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 448  | total loss: \cf6 1.61199\cf0  | time: 64.819s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.61199 - acc: 0.4108 | val_loss: 1.84437 - val_acc: 0.3906 -- iter: 07296/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 449  | total loss: \cf6 1.60693\cf0  | time: 65.936s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.60693 - acc: 0.4151 | val_loss: 2.02010 - val_acc: 0.3594 -- iter: 07424/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 450  | total loss: \cf6 1.59046\cf0  | time: 67.054s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.59046 - acc: 0.4228 | val_loss: 1.82097 - val_acc: 0.3594 -- iter: 07552/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 451  | total loss: \cf6 1.58712\cf0  | time: 68.170s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.58712 - acc: 0.4219 | val_loss: 1.68990 - val_acc: 0.4062 -- iter: 07680/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 452  | total loss: \cf6 1.57854\cf0  | time: 69.285s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.57854 - acc: 0.4258 | val_loss: 1.73927 - val_acc: 0.4062 -- iter: 07808/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 453  | total loss: \cf6 1.57041\cf0  | time: 70.403s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.57041 - acc: 0.4199 | val_loss: 1.81575 - val_acc: 0.3828 -- iter: 07936/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 454  | total loss: \cf6 1.56647\cf0  | time: 71.521s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.56647 - acc: 0.4186 | val_loss: 1.80745 - val_acc: 0.4141 -- iter: 08064/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 455  | total loss: \cf6 1.56995\cf0  | time: 72.639s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.56995 - acc: 0.4150 | val_loss: 1.62602 - val_acc: 0.4609 -- iter: 08192/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 456  | total loss: \cf6 1.57980\cf0  | time: 73.759s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.57980 - acc: 0.4110 | val_loss: 1.53607 - val_acc: 0.5312 -- iter: 08320/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 457  | total loss: \cf6 1.57104\cf0  | time: 74.877s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.57104 - acc: 0.4097 | val_loss: 1.51073 - val_acc: 0.4844 -- iter: 08448/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 458  | total loss: \cf6 1.56873\cf0  | time: 75.994s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.56873 - acc: 0.4063 | val_loss: 1.53992 - val_acc: 0.4531 -- iter: 08576/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 459  | total loss: \cf6 1.55041\cf0  | time: 77.114s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.55041 - acc: 0.4078 | val_loss: 1.61000 - val_acc: 0.4062 -- iter: 08704/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 460  | total loss: \cf6 1.54143\cf0  | time: 78.251s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.54143 - acc: 0.4077 | val_loss: 1.60430 - val_acc: 0.4219 -- iter: 08832/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 461  | total loss: \cf6 1.53180\cf0  | time: 79.385s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.53180 - acc: 0.4122 | val_loss: 1.60789 - val_acc: 0.3984 -- iter: 08960/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 462  | total loss: \cf6 1.51456\cf0  | time: 80.521s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.51456 - acc: 0.4265 | val_loss: 1.61132 - val_acc: 0.4219 -- iter: 09088/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 463  | total loss: \cf6 1.50703\cf0  | time: 81.641s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.50703 - acc: 0.4205 | val_loss: 1.57986 - val_acc: 0.3828 -- iter: 09216/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 464  | total loss: \cf6 1.51651\cf0  | time: 82.757s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.51651 - acc: 0.4207 | val_loss: 1.56779 - val_acc: 0.3906 -- iter: 09344/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 465  | total loss: \cf6 1.50444\cf0  | time: 83.875s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.50444 - acc: 0.4239 | val_loss: 1.56783 - val_acc: 0.3906 -- iter: 09472/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 466  | total loss: \cf6 1.50464\cf0  | time: 84.991s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.50464 - acc: 0.4237 | val_loss: 1.54806 - val_acc: 0.4219 -- iter: 09600/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 467  | total loss: \cf6 1.50454\cf0  | time: 86.106s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.50454 - acc: 0.4282 | val_loss: 1.41916 - val_acc: 0.4453 -- iter: 09728/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 468  | total loss: \cf6 1.51152\cf0  | time: 87.222s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.51152 - acc: 0.4229 | val_loss: 1.35798 - val_acc: 0.4688 -- iter: 09856/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 469  | total loss: \cf6 1.50234\cf0  | time: 88.346s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.50234 - acc: 0.4251 | val_loss: 1.39888 - val_acc: 0.4609 -- iter: 09984/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 470  | total loss: \cf6 1.49200\cf0  | time: 89.466s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.49200 - acc: 0.4357 | val_loss: 1.44112 - val_acc: 0.4531 -- iter: 10112/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 471  | total loss: \cf6 1.50325\cf0  | time: 90.586s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.50325 - acc: 0.4359 | val_loss: 1.50361 - val_acc: 0.4609 -- iter: 10240/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 472  | total loss: \cf6 1.50609\cf0  | time: 91.710s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.50609 - acc: 0.4384 | val_loss: 1.49119 - val_acc: 0.4688 -- iter: 10368/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 473  | total loss: \cf6 1.47877\cf0  | time: 92.830s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.47877 - acc: 0.4508 | val_loss: 1.50060 - val_acc: 0.4453 -- iter: 10496/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 474  | total loss: \cf6 1.47289\cf0  | time: 93.953s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.47289 - acc: 0.4479 | val_loss: 1.53047 - val_acc: 0.4375 -- iter: 10624/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 475  | total loss: \cf6 1.47489\cf0  | time: 95.072s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.47489 - acc: 0.4477 | val_loss: 1.56687 - val_acc: 0.4297 -- iter: 10752/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 476  | total loss: \cf6 1.48707\cf0  | time: 96.195s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.48707 - acc: 0.4420 | val_loss: 1.51813 - val_acc: 0.4375 -- iter: 10880/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 477  | total loss: \cf6 1.48588\cf0  | time: 97.316s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.48588 - acc: 0.4454 | val_loss: 1.43604 - val_acc: 0.4375 -- iter: 11008/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 478  | total loss: \cf6 1.49905\cf0  | time: 98.437s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.49905 - acc: 0.4407 | val_loss: 1.36846 - val_acc: 0.5234 -- iter: 11136/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 479  | total loss: \cf6 1.51242\cf0  | time: 99.557s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.51242 - acc: 0.4357 | val_loss: 1.34843 - val_acc: 0.5078 -- iter: 11264/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 480  | total loss: \cf6 1.49864\cf0  | time: 100.696s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.49864 - acc: 0.4406 | val_loss: 1.36797 - val_acc: 0.5078 -- iter: 11392/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 481  | total loss: \cf6 1.50149\cf0  | time: 101.817s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.50149 - acc: 0.4348 | val_loss: 1.36797 - val_acc: 0.4766 -- iter: 11520/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 482  | total loss: \cf6 1.48392\cf0  | time: 102.937s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.48392 - acc: 0.4413 | val_loss: 1.37476 - val_acc: 0.4688 -- iter: 11648/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 483  | total loss: \cf6 1.46870\cf0  | time: 104.057s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.46870 - acc: 0.4464 | val_loss: 1.37332 - val_acc: 0.4922 -- iter: 11776/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 484  | total loss: \cf6 1.45566\cf0  | time: 105.180s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.45566 - acc: 0.4541 | val_loss: 1.34291 - val_acc: 0.5078 -- iter: 11904/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 485  | total loss: \cf6 1.45192\cf0  | time: 106.299s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.45192 - acc: 0.4556 | val_loss: 1.34305 - val_acc: 0.5156 -- iter: 12032/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 486  | total loss: \cf6 1.44292\cf0  | time: 107.419s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.44292 - acc: 0.4631 | val_loss: 1.37865 - val_acc: 0.5000 -- iter: 12160/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 487  | total loss: \cf6 1.43818\cf0  | time: 108.533s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.43818 - acc: 0.4676 | val_loss: 1.37620 - val_acc: 0.5078 -- iter: 12288/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 488  | total loss: \cf6 1.43453\cf0  | time: 109.652s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.43453 - acc: 0.4685 | val_loss: 1.36289 - val_acc: 0.4922 -- iter: 12416/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 489  | total loss: \cf6 1.44337\cf0  | time: 110.777s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.44337 - acc: 0.4576 | val_loss: 1.40307 - val_acc: 0.5000 -- iter: 12544/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 490  | total loss: \cf6 1.43584\cf0  | time: 111.897s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.43584 - acc: 0.4611 | val_loss: 1.47098 - val_acc: 0.4922 -- iter: 12672/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 491  | total loss: \cf6 1.42432\cf0  | time: 113.014s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.42432 - acc: 0.4618 | val_loss: 1.63887 - val_acc: 0.4297 -- iter: 12800/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 492  | total loss: \cf6 1.41064\cf0  | time: 114.132s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.41064 - acc: 0.4664 | val_loss: 1.91012 - val_acc: 0.4219 -- iter: 12928/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 493  | total loss: \cf6 1.41121\cf0  | time: 115.251s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.41121 - acc: 0.4690 | val_loss: 2.25502 - val_acc: 0.4062 -- iter: 13056/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 494  | total loss: \cf6 1.40530\cf0  | time: 116.370s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.40530 - acc: 0.4744 | val_loss: 2.38486 - val_acc: 0.3828 -- iter: 13184/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 495  | total loss: \cf6 1.40296\cf0  | time: 117.492s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.40296 - acc: 0.4778 | val_loss: 2.24657 - val_acc: 0.3516 -- iter: 13312/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 496  | total loss: \cf6 1.40400\cf0  | time: 118.612s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.40400 - acc: 0.4769 | val_loss: 1.94054 - val_acc: 0.3828 -- iter: 13440/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 497  | total loss: \cf6 1.40515\cf0  | time: 119.735s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.40515 - acc: 0.4831 | val_loss: 1.77989 - val_acc: 0.3828 -- iter: 13568/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 498  | total loss: \cf6 1.41427\cf0  | time: 120.855s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.41427 - acc: 0.4809 | val_loss: 1.77565 - val_acc: 0.3672 -- iter: 13696/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 499  | total loss: \cf6 1.41638\cf0  | time: 121.971s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.41638 - acc: 0.4758 | val_loss: 1.90982 - val_acc: 0.3750 -- iter: 13824/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 500  | total loss: \cf6 1.41235\cf0  | time: 123.094s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.41235 - acc: 0.4813 | val_loss: 2.01004 - val_acc: 0.3516 -- iter: 13952/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 501  | total loss: \cf6 1.39870\cf0  | time: 124.214s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.39870 - acc: 0.4824 | val_loss: 2.10169 - val_acc: 0.3594 -- iter: 14080/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 502  | total loss: \cf6 1.38652\cf0  | time: 125.337s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.38652 - acc: 0.4857 | val_loss: 2.00580 - val_acc: 0.3750 -- iter: 14208/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 503  | total loss: \cf6 1.37913\cf0  | time: 126.452s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.37913 - acc: 0.4926 | val_loss: 1.85867 - val_acc: 0.4141 -- iter: 14336/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 504  | total loss: \cf6 1.36925\cf0  | time: 127.569s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.36925 - acc: 0.4996 | val_loss: 1.49962 - val_acc: 0.5078 -- iter: 14464/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 505  | total loss: \cf6 1.38060\cf0  | time: 128.685s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.38060 - acc: 0.4957 | val_loss: 1.33032 - val_acc: 0.5312 -- iter: 14592/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 506  | total loss: \cf6 1.40723\cf0  | time: 129.799s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.40723 - acc: 0.4868 | val_loss: 1.35180 - val_acc: 0.5156 -- iter: 14720/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 507  | total loss: \cf6 1.39764\cf0  | time: 130.920s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.39764 - acc: 0.4850 | val_loss: 1.45516 - val_acc: 0.5000 -- iter: 14848/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 508  | total loss: \cf6 1.37856\cf0  | time: 132.041s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.37856 - acc: 0.4896 | val_loss: 1.54164 - val_acc: 0.4766 -- iter: 14976/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 509  | total loss: \cf6 1.38739\cf0  | time: 133.157s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.38739 - acc: 0.4891 | val_loss: 1.60321 - val_acc: 0.4297 -- iter: 15104/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 510  | total loss: \cf6 1.38595\cf0  | time: 134.274s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.38595 - acc: 0.4941 | val_loss: 1.58447 - val_acc: 0.4297 -- iter: 15232/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 511  | total loss: \cf6 1.40184\cf0  | time: 135.392s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.40184 - acc: 0.4861 | val_loss: 1.51213 - val_acc: 0.4375 -- iter: 15360/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 512  | total loss: \cf6 1.38608\cf0  | time: 136.512s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.38608 - acc: 0.5000 | val_loss: 1.46725 - val_acc: 0.4609 -- iter: 15488/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 513  | total loss: \cf6 1.37880\cf0  | time: 137.632s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.37880 - acc: 0.5023 | val_loss: 1.41645 - val_acc: 0.4766 -- iter: 15616/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 514  | total loss: \cf6 1.37581\cf0  | time: 138.753s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.37581 - acc: 0.5021 | val_loss: 1.36262 - val_acc: 0.4766 -- iter: 15744/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 515  | total loss: \cf6 1.37340\cf0  | time: 139.890s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.37340 - acc: 0.5066 | val_loss: 1.38339 - val_acc: 0.4766 -- iter: 15872/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 516  | total loss: \cf6 1.35612\cf0  | time: 141.015s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.35612 - acc: 0.5176 | val_loss: 1.45537 - val_acc: 0.4844 -- iter: 16000/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 517  | total loss: \cf6 1.35267\cf0  | time: 142.137s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.35267 - acc: 0.5159 | val_loss: 1.59706 - val_acc: 0.5078 -- iter: 16128/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 518  | total loss: \cf6 1.32714\cf0  | time: 143.260s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.32714 - acc: 0.5299 | val_loss: 1.67356 - val_acc: 0.5234 -- iter: 16256/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 519  | total loss: \cf6 1.32093\cf0  | time: 144.380s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.32093 - acc: 0.5254 | val_loss: 1.84042 - val_acc: 0.5312 -- iter: 16384/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 520  | total loss: \cf6 1.32212\cf0  | time: 145.499s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.32212 - acc: 0.5259 | val_loss: 1.97553 - val_acc: 0.5000 -- iter: 16512/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 521  | total loss: \cf6 1.32358\cf0  | time: 146.615s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.32358 - acc: 0.5296 | val_loss: 1.96050 - val_acc: 0.4766 -- iter: 16640/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 522  | total loss: \cf6 1.30852\cf0  | time: 147.729s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.30852 - acc: 0.5337 | val_loss: 1.75441 - val_acc: 0.5000 -- iter: 16768/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 523  | total loss: \cf6 1.31476\cf0  | time: 148.844s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.31476 - acc: 0.5342 | val_loss: 1.43315 - val_acc: 0.5469 -- iter: 16896/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 524  | total loss: \cf6 1.33352\cf0  | time: 149.967s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.33352 - acc: 0.5245 | val_loss: 1.31077 - val_acc: 0.5625 -- iter: 17024/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 525  | total loss: \cf6 1.32591\cf0  | time: 151.086s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.32591 - acc: 0.5307 | val_loss: 1.28881 - val_acc: 0.5156 -- iter: 17152/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 526  | total loss: \cf6 1.32821\cf0  | time: 152.203s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.32821 - acc: 0.5300 | val_loss: 1.30560 - val_acc: 0.5000 -- iter: 17280/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 527  | total loss: \cf6 1.33107\cf0  | time: 153.319s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.33107 - acc: 0.5262 | val_loss: 1.29548 - val_acc: 0.5078 -- iter: 17408/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 528  | total loss: \cf6 1.32389\cf0  | time: 154.435s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.32389 - acc: 0.5228 | val_loss: 1.30061 - val_acc: 0.5078 -- iter: 17536/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 529  | total loss: \cf6 1.31151\cf0  | time: 155.551s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.31151 - acc: 0.5346 | val_loss: 1.26416 - val_acc: 0.5312 -- iter: 17664/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 530  | total loss: \cf6 1.30363\cf0  | time: 156.663s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.30363 - acc: 0.5397 | val_loss: 1.21627 - val_acc: 0.5625 -- iter: 17792/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 531  | total loss: \cf6 1.29655\cf0  | time: 157.780s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.29655 - acc: 0.5428 | val_loss: 1.19113 - val_acc: 0.5391 -- iter: 17920/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 532  | total loss: \cf6 1.28645\cf0  | time: 158.899s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.28645 - acc: 0.5432 | val_loss: 1.19849 - val_acc: 0.5391 -- iter: 18048/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 533  | total loss: \cf6 1.27499\cf0  | time: 160.015s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.27499 - acc: 0.5506 | val_loss: 1.23389 - val_acc: 0.5469 -- iter: 18176/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 534  | total loss: \cf6 1.25260\cf0  | time: 161.144s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.25260 - acc: 0.5565 | val_loss: 1.28228 - val_acc: 0.5156 -- iter: 18304/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 535  | total loss: \cf6 1.24751\cf0  | time: 162.261s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.24751 - acc: 0.5555 | val_loss: 1.32878 - val_acc: 0.5156 -- iter: 18432/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 536  | total loss: \cf6 1.25438\cf0  | time: 163.376s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.25438 - acc: 0.5507 | val_loss: 1.40505 - val_acc: 0.4844 -- iter: 18560/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 537  | total loss: \cf6 1.24115\cf0  | time: 164.497s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.24115 - acc: 0.5519 | val_loss: 1.45032 - val_acc: 0.4844 -- iter: 18688/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 538  | total loss: \cf6 1.23111\cf0  | time: 165.613s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.23111 - acc: 0.5530 | val_loss: 1.53363 - val_acc: 0.4531 -- iter: 18816/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 539  | total loss: \cf6 1.23323\cf0  | time: 166.730s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.23323 - acc: 0.5539 | val_loss: 1.50834 - val_acc: 0.4844 -- iter: 18944/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 540  | total loss: \cf6 1.22560\cf0  | time: 167.845s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.22560 - acc: 0.5595 | val_loss: 1.48168 - val_acc: 0.4688 -- iter: 19072/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 541  | total loss: \cf6 1.23488\cf0  | time: 168.961s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.23488 - acc: 0.5559 | val_loss: 1.37596 - val_acc: 0.5078 -- iter: 19200/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 542  | total loss: \cf6 1.23897\cf0  | time: 170.080s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.23897 - acc: 0.5565 | val_loss: 1.32827 - val_acc: 0.5078 -- iter: 19328/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 543  | total loss: \cf6 1.24301\cf0  | time: 171.199s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.24301 - acc: 0.5571 | val_loss: 1.31658 - val_acc: 0.5000 -- iter: 19456/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 544  | total loss: \cf6 1.23435\cf0  | time: 172.318s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.23435 - acc: 0.5608 | val_loss: 1.33047 - val_acc: 0.5156 -- iter: 19584/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 545  | total loss: \cf6 1.24304\cf0  | time: 173.440s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.24304 - acc: 0.5516 | val_loss: 1.31919 - val_acc: 0.5000 -- iter: 19712/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 546  | total loss: \cf6 1.24246\cf0  | time: 174.559s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.24246 - acc: 0.5511 | val_loss: 1.30829 - val_acc: 0.5312 -- iter: 19840/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 547  | total loss: \cf6 1.23680\cf0  | time: 175.680s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.23680 - acc: 0.5523 | val_loss: 1.27297 - val_acc: 0.5547 -- iter: 19968/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 548  | total loss: \cf6 1.21844\cf0  | time: 176.803s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.21844 - acc: 0.5587 | val_loss: 1.25279 - val_acc: 0.5469 -- iter: 20096/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 549  | total loss: \cf6 1.22937\cf0  | time: 177.924s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.22937 - acc: 0.5544 | val_loss: 1.24063 - val_acc: 0.5781 -- iter: 20224/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 550  | total loss: \cf6 1.22477\cf0  | time: 179.045s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.22477 - acc: 0.5529 | val_loss: 1.25622 - val_acc: 0.5703 -- iter: 20352/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 551  | total loss: \cf6 1.22567\cf0  | time: 180.170s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.22567 - acc: 0.5523 | val_loss: 1.34219 - val_acc: 0.5469 -- iter: 20480/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 552  | total loss: \cf6 1.23213\cf0  | time: 181.291s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.23213 - acc: 0.5518 | val_loss: 1.38319 - val_acc: 0.5156 -- iter: 20608/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 553  | total loss: \cf6 1.24766\cf0  | time: 182.410s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.24766 - acc: 0.5497 | val_loss: 1.43944 - val_acc: 0.4766 -- iter: 20736/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 554  | total loss: \cf6 1.27849\cf0  | time: 183.530s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.27849 - acc: 0.5408 | val_loss: 1.52461 - val_acc: 0.4141 -- iter: 20864/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 555  | total loss: \cf6 1.28273\cf0  | time: 184.668s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.28273 - acc: 0.5406 | val_loss: 1.61679 - val_acc: 0.3906 -- iter: 20992/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 556  | total loss: \cf6 1.29083\cf0  | time: 185.789s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.29083 - acc: 0.5389 | val_loss: 1.60688 - val_acc: 0.3672 -- iter: 21120/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 557  | total loss: \cf6 1.28530\cf0  | time: 186.907s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.28530 - acc: 0.5389 | val_loss: 1.53359 - val_acc: 0.4062 -- iter: 21248/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 558  | total loss: \cf6 1.28751\cf0  | time: 188.027s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.28751 - acc: 0.5382 | val_loss: 1.51163 - val_acc: 0.4453 -- iter: 21376/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 559  | total loss: \cf6 1.30591\cf0  | time: 189.153s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.30591 - acc: 0.5359 | val_loss: 1.47225 - val_acc: 0.4297 -- iter: 21504/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 560  | total loss: \cf6 1.31813\cf0  | time: 190.274s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.31813 - acc: 0.5339 | val_loss: 1.42129 - val_acc: 0.4453 -- iter: 21632/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 561  | total loss: \cf6 1.30772\cf0  | time: 191.395s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.30772 - acc: 0.5391 | val_loss: 1.45241 - val_acc: 0.4688 -- iter: 21760/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 562  | total loss: \cf6 1.30894\cf0  | time: 192.516s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.30894 - acc: 0.5375 | val_loss: 1.54666 - val_acc: 0.4688 -- iter: 21888/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 563  | total loss: \cf6 1.30081\cf0  | time: 193.634s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.30081 - acc: 0.5392 | val_loss: 1.68633 - val_acc: 0.4453 -- iter: 22016/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 564  | total loss: \cf6 1.29905\cf0  | time: 194.754s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.29905 - acc: 0.5377 | val_loss: 1.82009 - val_acc: 0.4531 -- iter: 22144/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 565  | total loss: \cf6 1.27942\cf0  | time: 195.878s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.27942 - acc: 0.5464 | val_loss: 1.79748 - val_acc: 0.4609 -- iter: 22272/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 566  | total loss: \cf6 1.29000\cf0  | time: 196.997s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.29000 - acc: 0.5472 | val_loss: 1.69966 - val_acc: 0.4609 -- iter: 22400/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 567  | total loss: \cf6 1.28656\cf0  | time: 198.119s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.28656 - acc: 0.5448 | val_loss: 1.66545 - val_acc: 0.4922 -- iter: 22528/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 568  | total loss: \cf6 1.27688\cf0  | time: 199.238s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.27688 - acc: 0.5505 | val_loss: 1.65681 - val_acc: 0.4922 -- iter: 22656/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 569  | total loss: \cf6 1.28754\cf0  | time: 200.357s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.28754 - acc: 0.5462 | val_loss: 1.63907 - val_acc: 0.4688 -- iter: 22784/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 570  | total loss: \cf6 1.28004\cf0  | time: 201.478s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.28004 - acc: 0.5487 | val_loss: 1.61549 - val_acc: 0.4453 -- iter: 22912/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 571  | total loss: \cf6 1.27230\cf0  | time: 202.603s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.27230 - acc: 0.5485 | val_loss: 1.57700 - val_acc: 0.4375 -- iter: 23040/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 572  | total loss: \cf6 1.27275\cf0  | time: 203.724s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.27275 - acc: 0.5452 | val_loss: 1.53283 - val_acc: 0.4453 -- iter: 23168/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 573  | total loss: \cf6 1.26911\cf0  | time: 204.845s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.26911 - acc: 0.5485 | val_loss: 1.44312 - val_acc: 0.4453 -- iter: 23296/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 574  | total loss: \cf6 1.27575\cf0  | time: 205.966s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.27575 - acc: 0.5522 | val_loss: 1.30086 - val_acc: 0.4453 -- iter: 23424/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 575  | total loss: \cf6 1.28721\cf0  | time: 207.084s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.28721 - acc: 0.5478 | val_loss: 1.23888 - val_acc: 0.5469 -- iter: 23552/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 576  | total loss: \cf6 1.27695\cf0  | time: 208.205s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.27695 - acc: 0.5532 | val_loss: 1.23888 - val_acc: 0.5547 -- iter: 23680/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 577  | total loss: \cf6 1.28140\cf0  | time: 209.328s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.28140 - acc: 0.5510 | val_loss: 1.28597 - val_acc: 0.5000 -- iter: 23808/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 578  | total loss: \cf6 1.27468\cf0  | time: 210.449s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.27468 - acc: 0.5513 | val_loss: 1.37805 - val_acc: 0.4766 -- iter: 23936/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 579  | total loss: \cf6 1.25556\cf0  | time: 211.570s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.25556 - acc: 0.5540 | val_loss: 1.49939 - val_acc: 0.4297 -- iter: 24064/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 580  | total loss: \cf6 1.24772\cf0  | time: 212.690s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.24772 - acc: 0.5533 | val_loss: 1.58303 - val_acc: 0.3750 -- iter: 24192/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 581  | total loss: \cf6 1.26107\cf0  | time: 213.823s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.26107 - acc: 0.5495 | val_loss: 1.53707 - val_acc: 0.4219 -- iter: 24320/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 582  | total loss: \cf6 1.25004\cf0  | time: 214.947s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.25004 - acc: 0.5524 | val_loss: 1.43199 - val_acc: 0.4766 -- iter: 24448/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 583  | total loss: \cf6 1.24101\cf0  | time: 216.068s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.24101 - acc: 0.5511 | val_loss: 1.33059 - val_acc: 0.4688 -- iter: 24576/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 584  | total loss: \cf6 1.23457\cf0  | time: 217.190s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.23457 - acc: 0.5546 | val_loss: 1.24518 - val_acc: 0.5469 -- iter: 24704/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 585  | total loss: \cf6 1.22714\cf0  | time: 218.313s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.22714 - acc: 0.5561 | val_loss: 1.18324 - val_acc: 0.5625 -- iter: 24832/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 586  | total loss: \cf6 1.22306\cf0  | time: 219.436s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.22306 - acc: 0.5513 | val_loss: 1.15582 - val_acc: 0.5703 -- iter: 24960/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 587  | total loss: \cf6 1.20427\cf0  | time: 220.571s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.20427 - acc: 0.5579 | val_loss: 1.16404 - val_acc: 0.5781 -- iter: 25088/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 588  | total loss: \cf6 1.21219\cf0  | time: 221.691s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.21219 - acc: 0.5482 | val_loss: 1.23613 - val_acc: 0.5938 -- iter: 25216/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 589  | total loss: \cf6 1.20114\cf0  | time: 222.807s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.20114 - acc: 0.5512 | val_loss: 1.35103 - val_acc: 0.5391 -- iter: 25344/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 590  | total loss: \cf6 1.21080\cf0  | time: 223.925s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.21080 - acc: 0.5508 | val_loss: 1.46072 - val_acc: 0.4922 -- iter: 25472/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 591  | total loss: \cf6 1.20623\cf0  | time: 225.042s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.20623 - acc: 0.5535 | val_loss: 1.45783 - val_acc: 0.4688 -- iter: 25600/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 592  | total loss: \cf6 1.21498\cf0  | time: 226.161s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.21498 - acc: 0.5560 | val_loss: 1.41426 - val_acc: 0.4609 -- iter: 25728/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 593  | total loss: \cf6 1.21915\cf0  | time: 227.280s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.21915 - acc: 0.5543 | val_loss: 1.36986 - val_acc: 0.4609 -- iter: 25856/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 594  | total loss: \cf6 1.22839\cf0  | time: 228.397s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.22839 - acc: 0.5535 | val_loss: 1.33321 - val_acc: 0.4688 -- iter: 25984/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 595  | total loss: \cf6 1.23262\cf0  | time: 229.515s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.23262 - acc: 0.5552 | val_loss: 1.30037 - val_acc: 0.5312 -- iter: 26112/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 596  | total loss: \cf6 1.22552\cf0  | time: 230.631s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.22552 - acc: 0.5606 | val_loss: 1.31748 - val_acc: 0.5156 -- iter: 26240/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 597  | total loss: \cf6 1.21855\cf0  | time: 231.750s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.21855 - acc: 0.5632 | val_loss: 1.32943 - val_acc: 0.5156 -- iter: 26368/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 598  | total loss: \cf6 1.22865\cf0  | time: 232.871s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.22865 - acc: 0.5553 | val_loss: 1.30853 - val_acc: 0.5156 -- iter: 26496/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 599  | total loss: \cf6 1.22796\cf0  | time: 233.994s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.22796 - acc: 0.5560 | val_loss: 1.25864 - val_acc: 0.5156 -- iter: 26624/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 600  | total loss: \cf6 1.21369\cf0  | time: 235.118s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.21369 - acc: 0.5566 | val_loss: 1.24506 - val_acc: 0.5391 -- iter: 26752/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 601  | total loss: \cf6 1.22820\cf0  | time: 236.241s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.22820 - acc: 0.5502 | val_loss: 1.24603 - val_acc: 0.5469 -- iter: 26880/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 602  | total loss: \cf6 1.21556\cf0  | time: 237.358s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.21556 - acc: 0.5522 | val_loss: 1.25490 - val_acc: 0.5938 -- iter: 27008/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 603  | total loss: \cf6 1.21545\cf0  | time: 238.481s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.21545 - acc: 0.5501 | val_loss: 1.25390 - val_acc: 0.5938 -- iter: 27136/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 604  | total loss: \cf6 1.20986\cf0  | time: 239.600s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.20986 - acc: 0.5521 | val_loss: 1.23576 - val_acc: 0.5938 -- iter: 27264/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 605  | total loss: \cf6 1.20668\cf0  | time: 240.730s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.20668 - acc: 0.5579 | val_loss: 1.24656 - val_acc: 0.6172 -- iter: 27392/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 606  | total loss: \cf6 1.22096\cf0  | time: 241.851s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.22096 - acc: 0.5529 | val_loss: 1.28158 - val_acc: 0.6094 -- iter: 27520/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 607  | total loss: \cf6 1.20847\cf0  | time: 242.970s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.20847 - acc: 0.5554 | val_loss: 1.33561 - val_acc: 0.5703 -- iter: 27648/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 608  | total loss: \cf6 1.20412\cf0  | time: 244.088s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.20412 - acc: 0.5545 | val_loss: 1.38010 - val_acc: 0.5703 -- iter: 27776/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 609  | total loss: \cf6 1.19594\cf0  | time: 245.205s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.19594 - acc: 0.5545 | val_loss: 1.43075 - val_acc: 0.5547 -- iter: 27904/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 610  | total loss: \cf6 1.19058\cf0  | time: 246.322s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.19058 - acc: 0.5592 | val_loss: 1.44644 - val_acc: 0.5547 -- iter: 28032/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 611  | total loss: \cf6 1.19275\cf0  | time: 247.441s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.19275 - acc: 0.5650 | val_loss: 1.43049 - val_acc: 0.5547 -- iter: 28160/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 612  | total loss: \cf6 1.17551\cf0  | time: 248.567s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.17551 - acc: 0.5687 | val_loss: 1.43547 - val_acc: 0.5391 -- iter: 28288/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 613  | total loss: \cf6 1.18767\cf0  | time: 249.691s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.18767 - acc: 0.5681 | val_loss: 1.38325 - val_acc: 0.5625 -- iter: 28416/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 614  | total loss: \cf6 1.18775\cf0  | time: 250.814s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.18775 - acc: 0.5675 | val_loss: 1.31916 - val_acc: 0.5234 -- iter: 28544/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 615  | total loss: \cf6 1.19059\cf0  | time: 251.934s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.19059 - acc: 0.5670 | val_loss: 1.26723 - val_acc: 0.5391 -- iter: 28672/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 616  | total loss: \cf6 1.18946\cf0  | time: 253.053s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.18946 - acc: 0.5650 | val_loss: 1.31411 - val_acc: 0.5547 -- iter: 28800/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 617  | total loss: \cf6 1.18716\cf0  | time: 254.171s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.18716 - acc: 0.5687 | val_loss: 1.33962 - val_acc: 0.5156 -- iter: 28928/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 618  | total loss: \cf6 1.17735\cf0  | time: 255.292s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.17735 - acc: 0.5727 | val_loss: 1.38736 - val_acc: 0.5078 -- iter: 29056/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 619  | total loss: \cf6 1.18463\cf0  | time: 256.412s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.18463 - acc: 0.5701 | val_loss: 1.38191 - val_acc: 0.5156 -- iter: 29184/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 620  | total loss: \cf6 1.17905\cf0  | time: 257.532s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.17905 - acc: 0.5694 | val_loss: 1.34684 - val_acc: 0.5156 -- iter: 29312/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 621  | total loss: \cf6 1.16906\cf0  | time: 258.653s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.16906 - acc: 0.5765 | val_loss: 1.27065 - val_acc: 0.5391 -- iter: 29440/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 622  | total loss: \cf6 1.17566\cf0  | time: 259.765s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.17566 - acc: 0.5743 | val_loss: 1.20362 - val_acc: 0.5469 -- iter: 29568/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 623  | total loss: \cf6 1.17229\cf0  | time: 260.884s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.17229 - acc: 0.5755 | val_loss: 1.16903 - val_acc: 0.5703 -- iter: 29696/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 624  | total loss: \cf6 1.16690\cf0  | time: 262.005s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.16690 - acc: 0.5781 | val_loss: 1.20736 - val_acc: 0.5156 -- iter: 29824/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 625  | total loss: \cf6 1.15043\cf0  | time: 263.132s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.15043 - acc: 0.5859 | val_loss: 1.31483 - val_acc: 0.5000 -- iter: 29952/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 626  | total loss: \cf6 1.17726\cf0  | time: 264.256s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.17726 - acc: 0.5758 | val_loss: 1.45625 - val_acc: 0.4766 -- iter: 30080/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 627  | total loss: \cf6 1.16927\cf0  | time: 265.379s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.16927 - acc: 0.5768 | val_loss: 1.46762 - val_acc: 0.4844 -- iter: 30208/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 628  | total loss: \cf6 1.15395\cf0  | time: 266.499s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.15395 - acc: 0.5800 | val_loss: 1.37252 - val_acc: 0.5000 -- iter: 30336/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 629  | total loss: \cf6 1.15254\cf0  | time: 267.617s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.15254 - acc: 0.5837 | val_loss: 1.32794 - val_acc: 0.5078 -- iter: 30464/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 630  | total loss: \cf6 1.15219\cf0  | time: 268.735s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.15219 - acc: 0.5840 | val_loss: 1.31763 - val_acc: 0.5156 -- iter: 30592/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 631  | total loss: \cf6 1.14408\cf0  | time: 269.856s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.14408 - acc: 0.5889 | val_loss: 1.42315 - val_acc: 0.4688 -- iter: 30720/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 632  | total loss: \cf6 1.14332\cf0  | time: 270.982s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.14332 - acc: 0.5901 | val_loss: 1.56765 - val_acc: 0.4844 -- iter: 30848/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 633  | total loss: \cf6 1.15264\cf0  | time: 272.103s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.15264 - acc: 0.5889 | val_loss: 1.68062 - val_acc: 0.4609 -- iter: 30976/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 634  | total loss: \cf6 1.14081\cf0  | time: 273.224s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.14081 - acc: 0.5917 | val_loss: 1.78824 - val_acc: 0.4297 -- iter: 31104/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 635  | total loss: \cf6 1.15754\cf0  | time: 274.348s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.15754 - acc: 0.5802 | val_loss: 1.82504 - val_acc: 0.4062 -- iter: 31232/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 636  | total loss: \cf6 1.17187\cf0  | time: 275.468s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.17187 - acc: 0.5761 | val_loss: 1.58560 - val_acc: 0.4375 -- iter: 31360/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 637  | total loss: \cf6 1.17190\cf0  | time: 276.586s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.17190 - acc: 0.5763 | val_loss: 1.42967 - val_acc: 0.4688 -- iter: 31488/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 638  | total loss: \cf6 1.17527\cf0  | time: 277.708s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.17527 - acc: 0.5773 | val_loss: 1.30046 - val_acc: 0.4766 -- iter: 31616/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 639  | total loss: \cf6 1.16707\cf0  | time: 278.830s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.16707 - acc: 0.5789 | val_loss: 1.25894 - val_acc: 0.5469 -- iter: 31744/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 640  | total loss: \cf6 1.18123\cf0  | time: 279.950s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.18123 - acc: 0.5757 | val_loss: 1.24053 - val_acc: 0.5469 -- iter: 31872/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 641  | total loss: \cf6 1.17592\cf0  | time: 281.066s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.17592 - acc: 0.5760 | val_loss: 1.23484 - val_acc: 0.5156 -- iter: 32000/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 642  | total loss: \cf6 1.19169\cf0  | time: 282.176s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.19169 - acc: 0.5660 | val_loss: 1.21255 - val_acc: 0.5391 -- iter: 32128/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 643  | total loss: \cf6 1.17457\cf0  | time: 283.292s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.17457 - acc: 0.5758 | val_loss: 1.19255 - val_acc: 0.5781 -- iter: 32256/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 644  | total loss: \cf6 1.17117\cf0  | time: 284.410s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.17117 - acc: 0.5792 | val_loss: 1.24190 - val_acc: 0.5547 -- iter: 32384/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 645  | total loss: \cf6 1.15934\cf0  | time: 285.530s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.15934 - acc: 0.5869 | val_loss: 1.34051 - val_acc: 0.5547 -- iter: 32512/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 646  | total loss: \cf6 1.15725\cf0  | time: 286.650s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.15725 - acc: 0.5930 | val_loss: 1.42942 - val_acc: 0.5391 -- iter: 32640/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 647  | total loss: \cf6 1.15325\cf0  | time: 287.771s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.15325 - acc: 0.5939 | val_loss: 1.44689 - val_acc: 0.5000 -- iter: 32768/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 648  | total loss: \cf6 1.15139\cf0  | time: 288.894s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.15139 - acc: 0.5908 | val_loss: 1.47432 - val_acc: 0.5000 -- iter: 32896/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 649  | total loss: \cf6 1.14675\cf0  | time: 290.016s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.14675 - acc: 0.5871 | val_loss: 1.51698 - val_acc: 0.4922 -- iter: 33024/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 650  | total loss: \cf6 1.14780\cf0  | time: 291.130s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.14780 - acc: 0.5909 | val_loss: 1.50011 - val_acc: 0.4922 -- iter: 33152/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 651  | total loss: \cf6 1.13453\cf0  | time: 292.250s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.13453 - acc: 0.5967 | val_loss: 1.44260 - val_acc: 0.5156 -- iter: 33280/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 652  | total loss: \cf6 1.13497\cf0  | time: 293.368s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.13497 - acc: 0.5964 | val_loss: 1.35721 - val_acc: 0.5234 -- iter: 33408/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 653  | total loss: \cf6 1.12822\cf0  | time: 294.492s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.12822 - acc: 0.5985 | val_loss: 1.31699 - val_acc: 0.5469 -- iter: 33536/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 654  | total loss: \cf6 1.11757\cf0  | time: 295.617s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.11757 - acc: 0.5972 | val_loss: 1.29941 - val_acc: 0.5547 -- iter: 33664/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 655  | total loss: \cf6 1.12411\cf0  | time: 296.749s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.12411 - acc: 0.5984 | val_loss: 1.28462 - val_acc: 0.5469 -- iter: 33792/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 656  | total loss: \cf6 1.12206\cf0  | time: 297.869s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.12206 - acc: 0.6011 | val_loss: 1.24153 - val_acc: 0.5156 -- iter: 33920/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 657  | total loss: \cf6 1.12599\cf0  | time: 298.989s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.12599 - acc: 0.5988 | val_loss: 1.20339 - val_acc: 0.5547 -- iter: 34048/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 658  | total loss: \cf6 1.13570\cf0  | time: 300.112s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.13570 - acc: 0.5983 | val_loss: 1.16958 - val_acc: 0.5469 -- iter: 34176/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 659  | total loss: \cf6 1.11490\cf0  | time: 301.233s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.11490 - acc: 0.6041 | val_loss: 1.16320 - val_acc: 0.5469 -- iter: 34304/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 660  | total loss: \cf6 1.10700\cf0  | time: 302.352s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.10700 - acc: 0.6054 | val_loss: 1.17482 - val_acc: 0.5312 -- iter: 34432/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 661  | total loss: \cf6 1.11032\cf0  | time: 303.474s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.11032 - acc: 0.6019 | val_loss: 1.17138 - val_acc: 0.5703 -- iter: 34560/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 662  | total loss: \cf6 1.12471\cf0  | time: 304.593s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.12471 - acc: 0.6042 | val_loss: 1.16985 - val_acc: 0.5781 -- iter: 34688/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 663  | total loss: \cf6 1.13371\cf0  | time: 305.713s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.13371 - acc: 0.6032 | val_loss: 1.21316 - val_acc: 0.5859 -- iter: 34816/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 664  | total loss: \cf6 1.11990\cf0  | time: 306.833s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.11990 - acc: 0.6077 | val_loss: 1.25303 - val_acc: 0.5312 -- iter: 34944/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 665  | total loss: \cf6 1.12414\cf0  | time: 307.956s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.12414 - acc: 0.6063 | val_loss: 1.25074 - val_acc: 0.5312 -- iter: 35072/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 666  | total loss: \cf6 1.13486\cf0  | time: 309.083s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.13486 - acc: 0.6003 | val_loss: 1.24437 - val_acc: 0.5391 -- iter: 35200/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 667  | total loss: \cf6 1.14911\cf0  | time: 310.204s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.14911 - acc: 0.5966 | val_loss: 1.32391 - val_acc: 0.5391 -- iter: 35328/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 668  | total loss: \cf6 1.13824\cf0  | time: 311.324s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.13824 - acc: 0.6010 | val_loss: 1.63567 - val_acc: 0.4922 -- iter: 35456/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 669  | total loss: \cf6 1.13758\cf0  | time: 312.445s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.13758 - acc: 0.5979 | val_loss: 2.11742 - val_acc: 0.4375 -- iter: 35584/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 670  | total loss: \cf6 1.12767\cf0  | time: 313.566s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.12767 - acc: 0.6022 | val_loss: 2.61920 - val_acc: 0.3906 -- iter: 35712/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 671  | total loss: \cf6 1.14933\cf0  | time: 314.688s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.14933 - acc: 0.5990 | val_loss: 2.73943 - val_acc: 0.4141 -- iter: 35840/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 672  | total loss: \cf6 1.15735\cf0  | time: 315.811s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.15735 - acc: 0.5922 | val_loss: 2.60127 - val_acc: 0.4062 -- iter: 35968/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 673  | total loss: \cf6 1.15204\cf0  | time: 316.930s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.15204 - acc: 0.5931 | val_loss: 2.34503 - val_acc: 0.3984 -- iter: 36096/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 674  | total loss: \cf6 1.14526\cf0  | time: 318.052s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.14526 - acc: 0.5909 | val_loss: 1.89324 - val_acc: 0.4531 -- iter: 36224/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 675  | total loss: \cf6 1.14154\cf0  | time: 319.174s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.14154 - acc: 0.5904 | val_loss: 1.49462 - val_acc: 0.5000 -- iter: 36352/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 676  | total loss: \cf6 1.12025\cf0  | time: 320.300s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.12025 - acc: 0.6001 | val_loss: 1.32526 - val_acc: 0.5391 -- iter: 36480/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 677  | total loss: \cf6 1.12572\cf0  | time: 321.424s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.12572 - acc: 0.6026 | val_loss: 1.27890 - val_acc: 0.5312 -- iter: 36608/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 678  | total loss: \cf6 1.11266\cf0  | time: 322.545s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.11266 - acc: 0.6079 | val_loss: 1.28632 - val_acc: 0.5078 -- iter: 36736/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 679  | total loss: \cf6 1.10933\cf0  | time: 323.671s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.10933 - acc: 0.6073 | val_loss: 1.27582 - val_acc: 0.5234 -- iter: 36864/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 680  | total loss: \cf6 1.12717\cf0  | time: 324.804s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.12717 - acc: 0.6005 | val_loss: 1.28411 - val_acc: 0.5234 -- iter: 36992/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 681  | total loss: \cf6 1.11629\cf0  | time: 325.923s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.11629 - acc: 0.6022 | val_loss: 1.26007 - val_acc: 0.5234 -- iter: 37120/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 682  | total loss: \cf6 1.12326\cf0  | time: 327.042s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.12326 - acc: 0.6005 | val_loss: 1.24839 - val_acc: 0.5469 -- iter: 37248/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 683  | total loss: \cf6 1.10918\cf0  | time: 328.165s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.10918 - acc: 0.6038 | val_loss: 1.25122 - val_acc: 0.5625 -- iter: 37376/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 684  | total loss: \cf6 1.11511\cf0  | time: 329.283s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.11511 - acc: 0.5996 | val_loss: 1.24843 - val_acc: 0.5391 -- iter: 37504/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 685  | total loss: \cf6 1.10896\cf0  | time: 330.407s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.10896 - acc: 0.6014 | val_loss: 1.29158 - val_acc: 0.5312 -- iter: 37632/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 686  | total loss: \cf6 1.11937\cf0  | time: 331.532s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.11937 - acc: 0.5991 | val_loss: 1.31987 - val_acc: 0.5391 -- iter: 37760/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 687  | total loss: \cf6 1.13055\cf0  | time: 332.649s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.13055 - acc: 0.5954 | val_loss: 1.33284 - val_acc: 0.5547 -- iter: 37888/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 688  | total loss: \cf6 1.14166\cf0  | time: 333.764s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.14166 - acc: 0.5937 | val_loss: 1.30076 - val_acc: 0.5469 -- iter: 38016/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 689  | total loss: \cf6 1.14626\cf0  | time: 334.883s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.14626 - acc: 0.5921 | val_loss: 1.27089 - val_acc: 0.5469 -- iter: 38144/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 690  | total loss: \cf6 1.16119\cf0  | time: 336.010s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.16119 - acc: 0.5868 | val_loss: 1.23866 - val_acc: 0.5234 -- iter: 38272/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 691  | total loss: \cf6 1.15178\cf0  | time: 337.126s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.15178 - acc: 0.5883 | val_loss: 1.23732 - val_acc: 0.5234 -- iter: 38400/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 692  | total loss: \cf6 1.16891\cf0  | time: 338.245s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.16891 - acc: 0.5818 | val_loss: 1.24318 - val_acc: 0.5312 -- iter: 38528/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 693  | total loss: \cf6 1.16676\cf0  | time: 339.363s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.16676 - acc: 0.5861 | val_loss: 1.25978 - val_acc: 0.5000 -- iter: 38656/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 694  | total loss: \cf6 1.16938\cf0  | time: 340.481s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.16938 - acc: 0.5830 | val_loss: 1.25499 - val_acc: 0.5469 -- iter: 38784/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 695  | total loss: \cf6 1.17343\cf0  | time: 341.609s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.17343 - acc: 0.5809 | val_loss: 1.26272 - val_acc: 0.5547 -- iter: 38912/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 696  | total loss: \cf6 1.17319\cf0  | time: 341.726s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.17319 - acc: 0.5752 | val_loss: 0.00000 - val_acc: 0.0000 -- iter: 39040/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 697  | total loss: \cf6 1.15630\cf0  | time: 342.849s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.15630 - acc: 0.5817 | val_loss: 1.42241 - val_acc: 0.5391 -- iter: 39168/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 698  | total loss: \cf6 1.15586\cf0  | time: 343.967s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.15586 - acc: 0.5814 | val_loss: 1.47909 - val_acc: 0.5469 -- iter: 39296/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 699  | total loss: \cf6 1.13511\cf0  | time: 345.087s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.13511 - acc: 0.5873 | val_loss: 1.51521 - val_acc: 0.5391 -- iter: 39424/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 700  | total loss: \cf6 1.12413\cf0  | time: 346.207s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.12413 - acc: 0.5934 | val_loss: 1.60756 - val_acc: 0.5156 -- iter: 39552/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 701  | total loss: \cf6 1.13319\cf0  | time: 347.325s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.13319 - acc: 0.5911 | val_loss: 1.70664 - val_acc: 0.5000 -- iter: 39680/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 702  | total loss: \cf6 1.12453\cf0  | time: 348.448s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.12453 - acc: 0.5906 | val_loss: 1.86166 - val_acc: 0.4844 -- iter: 39808/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 703  | total loss: \cf6 1.11969\cf0  | time: 349.573s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.11969 - acc: 0.5948 | val_loss: 1.80647 - val_acc: 0.4922 -- iter: 39936/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 704  | total loss: \cf6 1.13483\cf0  | time: 350.694s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.13483 - acc: 0.5799 | val_loss: 1.70254 - val_acc: 0.4688 -- iter: 40064/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 705  | total loss: \cf6 1.12744\cf0  | time: 351.813s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.12744 - acc: 0.5836 | val_loss: 1.50490 - val_acc: 0.5078 -- iter: 40192/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 706  | total loss: \cf6 1.15085\cf0  | time: 352.934s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.15085 - acc: 0.5752 | val_loss: 1.36158 - val_acc: 0.5391 -- iter: 40320/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 707  | total loss: \cf6 1.13508\cf0  | time: 354.052s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.13508 - acc: 0.5802 | val_loss: 1.26298 - val_acc: 0.5625 -- iter: 40448/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 708  | total loss: \cf6 1.13222\cf0  | time: 355.170s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.13222 - acc: 0.5808 | val_loss: 1.27267 - val_acc: 0.5547 -- iter: 40576/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 709  | total loss: \cf6 1.13810\cf0  | time: 356.291s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.13810 - acc: 0.5782 | val_loss: 1.24942 - val_acc: 0.5547 -- iter: 40704/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 710  | total loss: \cf6 1.14592\cf0  | time: 357.416s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.14592 - acc: 0.5735 | val_loss: 1.22321 - val_acc: 0.5391 -- iter: 40832/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 711  | total loss: \cf6 1.12646\cf0  | time: 358.538s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.12646 - acc: 0.5802 | val_loss: 1.19249 - val_acc: 0.5469 -- iter: 40960/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 712  | total loss: \cf6 1.12108\cf0  | time: 359.656s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.12108 - acc: 0.5831 | val_loss: 1.15034 - val_acc: 0.5469 -- iter: 41088/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 713  | total loss: \cf6 1.10385\cf0  | time: 360.778s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.10385 - acc: 0.5904 | val_loss: 1.12569 - val_acc: 0.5703 -- iter: 41216/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 714  | total loss: \cf6 1.10303\cf0  | time: 361.897s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.10303 - acc: 0.5978 | val_loss: 1.10644 - val_acc: 0.5781 -- iter: 41344/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 715  | total loss: \cf6 1.12616\cf0  | time: 363.017s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.12616 - acc: 0.5927 | val_loss: 1.10441 - val_acc: 0.5781 -- iter: 41472/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 716  | total loss: \cf6 1.12706\cf0  | time: 364.140s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.12706 - acc: 0.5889 | val_loss: 1.13230 - val_acc: 0.5625 -- iter: 41600/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 717  | total loss: \cf6 1.13221\cf0  | time: 365.269s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.13221 - acc: 0.5894 | val_loss: 1.18929 - val_acc: 0.5078 -- iter: 41728/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 718  | total loss: \cf6 1.11671\cf0  | time: 366.389s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.11671 - acc: 0.5976 | val_loss: 1.24445 - val_acc: 0.5312 -- iter: 41856/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 719  | total loss: \cf6 1.11580\cf0  | time: 367.510s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.11580 - acc: 0.5965 | val_loss: 1.29712 - val_acc: 0.5391 -- iter: 41984/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 720  | total loss: \cf6 1.11869\cf0  | time: 368.631s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.11869 - acc: 0.5946 | val_loss: 1.36186 - val_acc: 0.5234 -- iter: 42112/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 721  | total loss: \cf6 1.11193\cf0  | time: 369.748s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.11193 - acc: 0.5992 | val_loss: 1.37082 - val_acc: 0.5000 -- iter: 42240/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 722  | total loss: \cf6 1.10072\cf0  | time: 370.870s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.10072 - acc: 0.5995 | val_loss: 1.30821 - val_acc: 0.5391 -- iter: 42368/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 723  | total loss: \cf6 1.10258\cf0  | time: 371.994s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.10258 - acc: 0.6005 | val_loss: 1.23520 - val_acc: 0.5547 -- iter: 42496/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 724  | total loss: \cf6 1.10209\cf0  | time: 373.116s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.10209 - acc: 0.6021 | val_loss: 1.16525 - val_acc: 0.5859 -- iter: 42624/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 725  | total loss: \cf6 1.10546\cf0  | time: 374.237s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.10546 - acc: 0.6013 | val_loss: 1.10750 - val_acc: 0.6016 -- iter: 42752/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 726  | total loss: \cf6 1.09825\cf0  | time: 375.357s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.09825 - acc: 0.6037 | val_loss: 1.05321 - val_acc: 0.5938 -- iter: 42880/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 727  | total loss: \cf6 1.11001\cf0  | time: 376.475s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.11001 - acc: 0.5980 | val_loss: 1.01356 - val_acc: 0.6016 -- iter: 43008/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 728  | total loss: \cf6 1.10531\cf0  | time: 377.593s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.10531 - acc: 0.5991 | val_loss: 0.98907 - val_acc: 0.6016 -- iter: 43136/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 729  | total loss: \cf6 1.08795\cf0  | time: 378.717s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.08795 - acc: 0.6048 | val_loss: 0.97504 - val_acc: 0.6328 -- iter: 43264/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 730  | total loss: \cf6 1.09647\cf0  | time: 379.841s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.09647 - acc: 0.6053 | val_loss: 0.98060 - val_acc: 0.6484 -- iter: 43392/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 731  | total loss: \cf6 1.08372\cf0  | time: 380.964s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.08372 - acc: 0.6112 | val_loss: 1.01226 - val_acc: 0.6406 -- iter: 43520/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 732  | total loss: \cf6 1.08853\cf0  | time: 382.083s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.08853 - acc: 0.6118 | val_loss: 1.06077 - val_acc: 0.6094 -- iter: 43648/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 733  | total loss: \cf6 1.08271\cf0  | time: 383.203s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.08271 - acc: 0.6131 | val_loss: 1.11356 - val_acc: 0.6094 -- iter: 43776/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 734  | total loss: \cf6 1.07669\cf0  | time: 384.319s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.07669 - acc: 0.6213 | val_loss: 1.13580 - val_acc: 0.5859 -- iter: 43904/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 735  | total loss: \cf6 1.07961\cf0  | time: 385.435s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.07961 - acc: 0.6201 | val_loss: 1.10839 - val_acc: 0.6094 -- iter: 44032/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 736  | total loss: \cf6 1.08457\cf0  | time: 386.552s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.08457 - acc: 0.6128 | val_loss: 1.05911 - val_acc: 0.6250 -- iter: 44160/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 737  | total loss: \cf6 1.08038\cf0  | time: 387.672s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.08038 - acc: 0.6117 | val_loss: 1.04840 - val_acc: 0.6016 -- iter: 44288/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 738  | total loss: \cf6 1.08412\cf0  | time: 388.788s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.08412 - acc: 0.6107 | val_loss: 1.08856 - val_acc: 0.5625 -- iter: 44416/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 739  | total loss: \cf6 1.07520\cf0  | time: 389.904s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.07520 - acc: 0.6129 | val_loss: 1.19012 - val_acc: 0.5391 -- iter: 44544/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 740  | total loss: \cf6 1.08003\cf0  | time: 391.023s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.08003 - acc: 0.6125 | val_loss: 1.26709 - val_acc: 0.5156 -- iter: 44672/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 741  | total loss: \cf6 1.07849\cf0  | time: 392.139s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.07849 - acc: 0.6130 | val_loss: 1.32214 - val_acc: 0.4922 -- iter: 44800/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 742  | total loss: \cf6 1.06825\cf0  | time: 393.256s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.06825 - acc: 0.6150 | val_loss: 1.39979 - val_acc: 0.4844 -- iter: 44928/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 743  | total loss: \cf6 1.05434\cf0  | time: 394.371s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.05434 - acc: 0.6199 | val_loss: 1.48178 - val_acc: 0.4922 -- iter: 45056/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 744  | total loss: \cf6 1.06663\cf0  | time: 395.492s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.06663 - acc: 0.6181 | val_loss: 1.51344 - val_acc: 0.4453 -- iter: 45184/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 745  | total loss: \cf6 1.08258\cf0  | time: 396.628s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.08258 - acc: 0.6117 | val_loss: 1.59429 - val_acc: 0.4141 -- iter: 45312/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 746  | total loss: \cf6 1.06104\cf0  | time: 397.749s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.06104 - acc: 0.6185 | val_loss: 1.69314 - val_acc: 0.4297 -- iter: 45440/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 747  | total loss: \cf6 1.07510\cf0  | time: 398.869s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.07510 - acc: 0.6106 | val_loss: 1.68868 - val_acc: 0.4297 -- iter: 45568/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 748  | total loss: \cf6 1.08188\cf0  | time: 400.000s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.08188 - acc: 0.6089 | val_loss: 1.56600 - val_acc: 0.4297 -- iter: 45696/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 749  | total loss: \cf6 1.07070\cf0  | time: 401.123s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.07070 - acc: 0.6144 | val_loss: 1.45995 - val_acc: 0.4844 -- iter: 45824/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 750  | total loss: \cf6 1.07888\cf0  | time: 402.243s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.07888 - acc: 0.6123 | val_loss: 1.40239 - val_acc: 0.5156 -- iter: 45952/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 751  | total loss: \cf6 1.17398\cf0  | time: 403.365s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.17398 - acc: 0.6074 | val_loss: 1.36478 - val_acc: 0.5078 -- iter: 46080/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 752  | total loss: \cf6 1.18004\cf0  | time: 404.488s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.18004 - acc: 0.5997 | val_loss: 1.32662 - val_acc: 0.5312 -- iter: 46208/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 753  | total loss: \cf6 1.15784\cf0  | time: 405.624s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.15784 - acc: 0.6031 | val_loss: 1.29774 - val_acc: 0.5547 -- iter: 46336/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 754  | total loss: \cf6 1.15776\cf0  | time: 406.745s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.15776 - acc: 0.6006 | val_loss: 1.27642 - val_acc: 0.5391 -- iter: 46464/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 755  | total loss: \cf6 1.15852\cf0  | time: 407.865s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.15852 - acc: 0.5983 | val_loss: 1.25428 - val_acc: 0.5312 -- iter: 46592/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 756  | total loss: \cf6 1.18069\cf0  | time: 408.984s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.18069 - acc: 0.5885 | val_loss: 1.24114 - val_acc: 0.5547 -- iter: 46720/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 757  | total loss: \cf6 1.17311\cf0  | time: 410.103s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.17311 - acc: 0.5898 | val_loss: 1.18900 - val_acc: 0.5938 -- iter: 46848/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 758  | total loss: \cf6 1.15616\cf0  | time: 411.223s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.15616 - acc: 0.5949 | val_loss: 1.17632 - val_acc: 0.5703 -- iter: 46976/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 759  | total loss: \cf6 1.16491\cf0  | time: 412.361s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.16491 - acc: 0.5932 | val_loss: 1.18153 - val_acc: 0.5469 -- iter: 47104/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 760  | total loss: \cf6 1.14178\cf0  | time: 413.489s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.14178 - acc: 0.5948 | val_loss: 1.18309 - val_acc: 0.5391 -- iter: 47232/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 761  | total loss: \cf6 1.12282\cf0  | time: 414.605s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.12282 - acc: 0.5971 | val_loss: 1.15659 - val_acc: 0.5625 -- iter: 47360/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 762  | total loss: \cf6 1.10789\cf0  | time: 415.725s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.10789 - acc: 0.6069 | val_loss: 1.15999 - val_acc: 0.6016 -- iter: 47488/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 763  | total loss: \cf6 1.09058\cf0  | time: 416.847s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.09058 - acc: 0.6095 | val_loss: 1.21150 - val_acc: 0.5781 -- iter: 47616/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 764  | total loss: \cf6 1.07892\cf0  | time: 417.965s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.07892 - acc: 0.6165 | val_loss: 1.29401 - val_acc: 0.5312 -- iter: 47744/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 765  | total loss: \cf6 1.07022\cf0  | time: 419.083s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.07022 - acc: 0.6150 | val_loss: 1.41126 - val_acc: 0.5000 -- iter: 47872/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 766  | total loss: \cf6 1.06839\cf0  | time: 420.203s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.06839 - acc: 0.6160 | val_loss: 1.45941 - val_acc: 0.4375 -- iter: 48000/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 767  | total loss: \cf6 1.07995\cf0  | time: 421.320s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.07995 - acc: 0.6130 | val_loss: 1.43136 - val_acc: 0.4453 -- iter: 48128/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 768  | total loss: \cf6 1.08638\cf0  | time: 422.438s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.08638 - acc: 0.6103 | val_loss: 1.34841 - val_acc: 0.4609 -- iter: 48256/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 769  | total loss: \cf6 1.08739\cf0  | time: 423.558s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.08739 - acc: 0.6086 | val_loss: 1.23032 - val_acc: 0.4844 -- iter: 48384/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 770  | total loss: \cf6 1.07922\cf0  | time: 424.676s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.07922 - acc: 0.6157 | val_loss: 1.09685 - val_acc: 0.5469 -- iter: 48512/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 771  | total loss: \cf6 1.08013\cf0  | time: 425.792s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.08013 - acc: 0.6120 | val_loss: 1.01439 - val_acc: 0.6250 -- iter: 48640/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 772  | total loss: \cf6 1.07681\cf0  | time: 426.910s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.07681 - acc: 0.6094 | val_loss: 0.96939 - val_acc: 0.6562 -- iter: 48768/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 773  | total loss: \cf6 1.07528\cf0  | time: 428.035s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.07528 - acc: 0.6117 | val_loss: 0.94287 - val_acc: 0.6484 -- iter: 48896/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 774  | total loss: \cf6 1.06513\cf0  | time: 429.156s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.06513 - acc: 0.6138 | val_loss: 0.92665 - val_acc: 0.6484 -- iter: 49024/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 775  | total loss: \cf6 1.05245\cf0  | time: 430.278s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.05245 - acc: 0.6220 | val_loss: 0.93975 - val_acc: 0.6406 -- iter: 49152/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 776  | total loss: \cf6 1.04528\cf0  | time: 431.399s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.04528 - acc: 0.6238 | val_loss: 0.98684 - val_acc: 0.6094 -- iter: 49280/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 777  | total loss: \cf6 1.03646\cf0  | time: 432.520s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.03646 - acc: 0.6286 | val_loss: 1.00320 - val_acc: 0.6172 -- iter: 49408/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 778  | total loss: \cf6 1.02505\cf0  | time: 433.639s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.02505 - acc: 0.6314 | val_loss: 1.04152 - val_acc: 0.6016 -- iter: 49536/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 779  | total loss: \cf6 1.01505\cf0  | time: 434.760s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.01505 - acc: 0.6339 | val_loss: 1.13117 - val_acc: 0.5938 -- iter: 49664/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 780  | total loss: \cf6 1.02081\cf0  | time: 435.879s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.02081 - acc: 0.6322 | val_loss: 1.22745 - val_acc: 0.5703 -- iter: 49792/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 781  | total loss: \cf6 1.01576\cf0  | time: 437.002s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.01576 - acc: 0.6362 | val_loss: 1.25947 - val_acc: 0.5469 -- iter: 49920/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 Training Step: 782  | total loss: \cf6 1.01904\cf0  | time: 438.098s\cf1\highlight1 
\par \cf0\highlight0 | Momentum | epoch: 002 | loss: 1.01904 - acc: 0.6358 | val_loss: 1.23880 - val_acc: 0.5547 -- iter: 50000/50000\cf1\highlight1 
\par \cf0\highlight0 --\cf1\highlight1 
\par \cf0\highlight0 (tf) xdong@gpu2:/data/xdong/codes/avg_para$ cd keras-res/\cf1\highlight1 
\par \cf0\highlight0 (tf) xdong@gpu2:/data/xdong/codes/avg_para/keras-res$ ls\cf1\highlight1 
\par \cf0\highlight0 cifar10_resnet.py  parameters.html\cf1\highlight1 
\par \cf0\highlight0 (tf) xdong@gpu2:/data/xdong/codes/avg_para/keras-res$ CUDA_VISIBLE_DEVICES=0 python cifar10_resnet.py\cf1\highlight1 
\par \cf0\highlight0 Using TensorFlow backend.\cf1\highlight1 
\par \cf0\highlight0 Couldn't import dot_parser, loading of dot files will not be possible.\cf1\highlight1 
\par \cf0\highlight0 x_train shape: (50000, 32, 32, 3)\cf1\highlight1 
\par \cf0\highlight0 50000 train samples\cf1\highlight1 
\par \cf0\highlight0 10000 test samples\cf1\highlight1 
\par \cf0\highlight0 y_train shape: (50000, 1)\cf1\highlight1 
\par \cf0\highlight0 ^CTraceback (most recent call last):\cf1\highlight1 
\par \cf0\highlight0   File "cifar10_resnet.py", line 136, in <module>\cf1\highlight1 
\par \cf0\highlight0     metrics=['accuracy'])\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/keras/engine/training.py", line 850, in compile\cf1\highlight1 
\par \cf0\highlight0     sample_weight, mask)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/keras/engine/training.py", line 450, in weighted\cf1\highlight1 
\par \cf0\highlight0     score_array = fn(y_true, y_pred)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/keras/losses.py", line 49, in categorical_crossentropy\cf1\highlight1 
\par \cf0\highlight0     return K.categorical_crossentropy(y_true, y_pred)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py", line 2758, in categorical_crossentropy\cf1\highlight1 
\par \cf0\highlight0     output = tf.clip_by_value(output, _epsilon, 1. - _epsilon)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tensorflow/python/ops/clip_ops.py", line 58, in clip_by_value\cf1\highlight1 
\par \cf0\highlight0     [t, clip_value_min, clip_value_max]) as name:\cf1\highlight1 
\par \cf0\highlight0   File "/usr/lib/python2.7/contextlib.py", line 17, in __enter__\cf1\highlight1 
\par \cf0\highlight0     return self.gen.next()\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 4521, in name_scope\cf1\highlight1 
\par \cf0\highlight0     g = _get_graph_from_inputs(values)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 4236, in _get_graph_from_inputs\cf1\highlight1 
\par \cf0\highlight0     if get_default_graph().building_function:\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 4186, in get_default_graph\cf1\highlight1 
\par \cf0\highlight0     return _default_graph_stack.get_default()\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 4132, in get_default\cf1\highlight1 
\par \cf0\highlight0     ret = super(_DefaultGraphStack, self).get_default()\cf1\highlight1 
\par \cf0\highlight0 KeyboardInterrupt\cf1\highlight1 
\par \cf0\highlight0 (tf) xdong@gpu2:/data/xdong/codes/avg_para/keras-res$ CUDA_VISIBLE_DEVICES=0 python cifar10_resnet.py\cf1\highlight1 
\par \cf0\highlight0 Using TensorFlow backend.\cf1\highlight1 
\par \cf0\highlight0 Couldn't import dot_parser, loading of dot files will not be possible.\cf1\highlight1 
\par \cf0\highlight0 x_train shape: (50000, 32, 32, 3)\cf1\highlight1 
\par \cf0\highlight0 50000 train samples\cf1\highlight1 
\par \cf0\highlight0 10000 test samples\cf1\highlight1 
\par \cf0\highlight0 y_train shape: (50000, 1)\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 Layer (type)                     Output Shape          Param #     Connected to\cf1\highlight1 
\par \cf0\highlight0 ====================================================================================================\cf1\highlight1 
\par \cf0\highlight0 input_1 (InputLayer)             (None, 32, 32, 3)     0\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_1 (Conv2D)                (None, 16, 16, 64)    9472        input_1[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 batch_normalization_1 (BatchNorm (None, 16, 16, 64)    256         conv2d_1[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 activation_1 (Activation)        (None, 16, 16, 64)    0           batch_normalization_1[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_2 (Conv2D)                (None, 16, 16, 64)    36928       activation_1[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 batch_normalization_2 (BatchNorm (None, 16, 16, 64)    256         conv2d_2[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 activation_2 (Activation)        (None, 16, 16, 64)    0           batch_normalization_2[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_3 (Conv2D)                (None, 16, 16, 64)    36928       activation_2[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 batch_normalization_3 (BatchNorm (None, 16, 16, 64)    256         conv2d_3[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 add_1 (Add)                      (None, 16, 16, 64)    0           activation_1[0][0]\cf1\highlight1 
\par \cf0\highlight0                                                                    batch_normalization_3[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 activation_3 (Activation)        (None, 16, 16, 64)    0           add_1[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_4 (Conv2D)                (None, 16, 16, 64)    36928       activation_3[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 batch_normalization_4 (BatchNorm (None, 16, 16, 64)    256         conv2d_4[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 activation_4 (Activation)        (None, 16, 16, 64)    0           batch_normalization_4[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_5 (Conv2D)                (None, 16, 16, 64)    36928       activation_4[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 batch_normalization_5 (BatchNorm (None, 16, 16, 64)    256         conv2d_5[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 add_2 (Add)                      (None, 16, 16, 64)    0           activation_3[0][0]\cf1\highlight1 
\par \cf0\highlight0                                                                    batch_normalization_5[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 activation_5 (Activation)        (None, 16, 16, 64)    0           add_2[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_6 (Conv2D)                (None, 8, 8, 128)     73856       activation_5[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 batch_normalization_6 (BatchNorm (None, 8, 8, 128)     512         conv2d_6[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 activation_6 (Activation)        (None, 8, 8, 128)     0           batch_normalization_6[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_7 (Conv2D)                (None, 8, 8, 128)     147584      activation_6[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_8 (Conv2D)                (None, 8, 8, 128)     8320        activation_5[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 batch_normalization_7 (BatchNorm (None, 8, 8, 128)     512         conv2d_7[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 add_3 (Add)                      (None, 8, 8, 128)     0           conv2d_8[0][0]\cf1\highlight1 
\par \cf0\highlight0                                                                    batch_normalization_7[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 activation_7 (Activation)        (None, 8, 8, 128)     0           add_3[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_9 (Conv2D)                (None, 8, 8, 128)     147584      activation_7[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 batch_normalization_8 (BatchNorm (None, 8, 8, 128)     512         conv2d_9[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 activation_8 (Activation)        (None, 8, 8, 128)     0           batch_normalization_8[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_10 (Conv2D)               (None, 8, 8, 128)     147584      activation_8[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 batch_normalization_9 (BatchNorm (None, 8, 8, 128)     512         conv2d_10[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 add_4 (Add)                      (None, 8, 8, 128)     0           activation_7[0][0]\cf1\highlight1 
\par \cf0\highlight0                                                                    batch_normalization_9[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 activation_9 (Activation)        (None, 8, 8, 128)     0           add_4[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_11 (Conv2D)               (None, 4, 4, 256)     295168      activation_9[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 batch_normalization_10 (BatchNor (None, 4, 4, 256)     1024        conv2d_11[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 activation_10 (Activation)       (None, 4, 4, 256)     0           batch_normalization_10[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_12 (Conv2D)               (None, 4, 4, 256)     590080      activation_10[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_13 (Conv2D)               (None, 4, 4, 256)     33024       activation_9[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 batch_normalization_11 (BatchNor (None, 4, 4, 256)     1024        conv2d_12[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 add_5 (Add)                      (None, 4, 4, 256)     0           conv2d_13[0][0]\cf1\highlight1 
\par \cf0\highlight0                                                                    batch_normalization_11[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 activation_11 (Activation)       (None, 4, 4, 256)     0           add_5[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_14 (Conv2D)               (None, 4, 4, 256)     590080      activation_11[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 batch_normalization_12 (BatchNor (None, 4, 4, 256)     1024        conv2d_14[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 activation_12 (Activation)       (None, 4, 4, 256)     0           batch_normalization_12[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_15 (Conv2D)               (None, 4, 4, 256)     590080      activation_12[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 batch_normalization_13 (BatchNor (None, 4, 4, 256)     1024        conv2d_15[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 add_6 (Add)                      (None, 4, 4, 256)     0           activation_11[0][0]\cf1\highlight1 
\par \cf0\highlight0                                                                    batch_normalization_13[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 activation_13 (Activation)       (None, 4, 4, 256)     0           add_6[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_16 (Conv2D)               (None, 2, 2, 512)     1180160     activation_13[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 batch_normalization_14 (BatchNor (None, 2, 2, 512)     2048        conv2d_16[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 activation_14 (Activation)       (None, 2, 2, 512)     0           batch_normalization_14[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_17 (Conv2D)               (None, 2, 2, 512)     2359808     activation_14[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_18 (Conv2D)               (None, 2, 2, 512)     131584      activation_13[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 batch_normalization_15 (BatchNor (None, 2, 2, 512)     2048        conv2d_17[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 add_7 (Add)                      (None, 2, 2, 512)     0           conv2d_18[0][0]\cf1\highlight1 
\par \cf0\highlight0                                                                    batch_normalization_15[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 activation_15 (Activation)       (None, 2, 2, 512)     0           add_7[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_19 (Conv2D)               (None, 2, 2, 512)     2359808     activation_15[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 batch_normalization_16 (BatchNor (None, 2, 2, 512)     2048        conv2d_19[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 activation_16 (Activation)       (None, 2, 2, 512)     0           batch_normalization_16[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_20 (Conv2D)               (None, 2, 2, 512)     2359808     activation_16[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 batch_normalization_17 (BatchNor (None, 2, 2, 512)     2048        conv2d_20[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 add_8 (Add)                      (None, 2, 2, 512)     0           activation_15[0][0]\cf1\highlight1 
\par \cf0\highlight0                                                                    batch_normalization_17[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 activation_17 (Activation)       (None, 2, 2, 512)     0           add_8[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 average_pooling2d_1 (AveragePool (None, 1, 1, 512)     0           activation_17[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 flatten_1 (Flatten)              (None, 512)           0           average_pooling2d_1[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 dense_1 (Dense)                  (None, 10)            5130        flatten_1[0][0]\cf1\highlight1 
\par \cf0\highlight0 ====================================================================================================\cf1\highlight1 
\par \cf0\highlight0 Total params: 11,192,458\cf1\highlight1 
\par \cf0\highlight0 Trainable params: 11,184,650\cf1\highlight1 
\par \cf0\highlight0 Non-trainable params: 7,808\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 Using real-time data augmentation.\cf1\highlight1 
\par \cf0\highlight0 Epoch 1/100\cf1\highlight1 
\par \cf0\highlight0 2017-10-24 16:32:25.644933: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could spePU computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-24 16:32:25.644989: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could spePU computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-24 16:32:25.645005: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-24 16:32:25.645025: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-24 16:32:25.645040: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-24 16:32:26.051000: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties:\cf1\highlight1 
\par \cf0\highlight0 name: GeForce GTX TITAN X\cf1\highlight1 
\par \cf0\highlight0 major: 5 minor: 2 memoryClockRate (GHz) 1.076\cf1\highlight1 
\par \cf0\highlight0 pciBusID 0000:02:00.0\cf1\highlight1 
\par \cf0\highlight0 Total memory: 11.92GiB\cf1\highlight1 
\par \cf0\highlight0 Free memory: 11.81GiB\cf1\highlight1 
\par \cf0\highlight0 2017-10-24 16:32:26.051085: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0\cf1\highlight1 
\par \cf0\highlight0 2017-10-24 16:32:26.051104: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y\cf1\highlight1 
\par \cf0\highlight0 2017-10-24 16:32:26.051124: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:02:00.0)\cf1\highlight1 
\par \cf0\highlight0 Traceback (most recent call last):\cf1\highlight1 
\par \cf0\highlight0   File "cifar10_resnet.py", line 237, in <module>\cf1\highlight1 
\par \cf0\highlight0     callbacks=callbacks)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/keras/legacy/interfaces.py", line 87, in wrapper\cf1\highlight1 
\par \cf0\highlight0     return func(*args, **kwargs)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/keras/engine/training.py", line 2049, in fit_generator\cf1\highlight1 
\par \cf0\highlight0     callbacks.on_batch_end(batch_index, batch_logs)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/keras/callbacks.py", line 113, in on_batch_end\cf1\highlight1 
\par \cf0\highlight0     callback.on_batch_end(batch, logs)\cf1\highlight1 
\par \cf0\highlight0   File "cifar10_resnet.py", line 177, in on_batch_end\cf1\highlight1 
\par \cf0\highlight0     do_avg4keras(name_list, self.model)\cf1\highlight1 
\par \cf0\highlight0   File "cifar10_resnet.py", line 168, in do_avg4keras\cf1\highlight1 
\par \cf0\highlight0     w_avg = sum(w_list)/float(w_list)\cf1\highlight1 
\par \cf0\highlight0 TypeError: float() argument must be a string or a number\cf1\highlight1 
\par \cf0\highlight0 (tf) xdong@gpu2:/data/xdong/codes/avg_para/keras-res$ CUDA_VISIBLE_DEVICES=0 python cifar10_resnet.py\cf1\highlight1 
\par \cf0\highlight0 Using TensorFlow backend.\cf1\highlight1 
\par \cf0\highlight0 Couldn't import dot_parser, loading of dot files will not be possible.\cf1\highlight1 
\par \cf0\highlight0 x_train shape: (50000, 32, 32, 3)\cf1\highlight1 
\par \cf0\highlight0 50000 train samples\cf1\highlight1 
\par \cf0\highlight0 10000 test samples\cf1\highlight1 
\par \cf0\highlight0 y_train shape: (50000, 1)\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 Layer (type)                     Output Shape          Param #     Connected to\cf1\highlight1 
\par \cf0\highlight0 ====================================================================================================\cf1\highlight1 
\par \cf0\highlight0 input_1 (InputLayer)             (None, 32, 32, 3)     0\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_1 (Conv2D)                (None, 16, 16, 64)    9472        input_1[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 batch_normalization_1 (BatchNorm (None, 16, 16, 64)    256         conv2d_1[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 activation_1 (Activation)        (None, 16, 16, 64)    0           batch_normalization_1[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_2 (Conv2D)                (None, 16, 16, 64)    36928       activation_1[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 batch_normalization_2 (BatchNorm (None, 16, 16, 64)    256         conv2d_2[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 activation_2 (Activation)        (None, 16, 16, 64)    0           batch_normalization_2[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_3 (Conv2D)                (None, 16, 16, 64)    36928       activation_2[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 batch_normalization_3 (BatchNorm (None, 16, 16, 64)    256         conv2d_3[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 add_1 (Add)                      (None, 16, 16, 64)    0           activation_1[0][0]\cf1\highlight1 
\par \cf0\highlight0                                                                    batch_normalization_3[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 activation_3 (Activation)        (None, 16, 16, 64)    0           add_1[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_4 (Conv2D)                (None, 16, 16, 64)    36928       activation_3[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 batch_normalization_4 (BatchNorm (None, 16, 16, 64)    256         conv2d_4[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 activation_4 (Activation)        (None, 16, 16, 64)    0           batch_normalization_4[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_5 (Conv2D)                (None, 16, 16, 64)    36928       activation_4[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 batch_normalization_5 (BatchNorm (None, 16, 16, 64)    256         conv2d_5[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 add_2 (Add)                      (None, 16, 16, 64)    0           activation_3[0][0]\cf1\highlight1 
\par \cf0\highlight0                                                                    batch_normalization_5[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 activation_5 (Activation)        (None, 16, 16, 64)    0           add_2[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_6 (Conv2D)                (None, 8, 8, 128)     73856       activation_5[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 batch_normalization_6 (BatchNorm (None, 8, 8, 128)     512         conv2d_6[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 activation_6 (Activation)        (None, 8, 8, 128)     0           batch_normalization_6[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_7 (Conv2D)                (None, 8, 8, 128)     147584      activation_6[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_8 (Conv2D)                (None, 8, 8, 128)     8320        activation_5[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 batch_normalization_7 (BatchNorm (None, 8, 8, 128)     512         conv2d_7[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 add_3 (Add)                      (None, 8, 8, 128)     0           conv2d_8[0][0]\cf1\highlight1 
\par \cf0\highlight0                                                                    batch_normalization_7[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 activation_7 (Activation)        (None, 8, 8, 128)     0           add_3[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_9 (Conv2D)                (None, 8, 8, 128)     147584      activation_7[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 batch_normalization_8 (BatchNorm (None, 8, 8, 128)     512         conv2d_9[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 activation_8 (Activation)        (None, 8, 8, 128)     0           batch_normalization_8[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_10 (Conv2D)               (None, 8, 8, 128)     147584      activation_8[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 batch_normalization_9 (BatchNorm (None, 8, 8, 128)     512         conv2d_10[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 add_4 (Add)                      (None, 8, 8, 128)     0           activation_7[0][0]\cf1\highlight1 
\par \cf0\highlight0                                                                    batch_normalization_9[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 activation_9 (Activation)        (None, 8, 8, 128)     0           add_4[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_11 (Conv2D)               (None, 4, 4, 256)     295168      activation_9[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 batch_normalization_10 (BatchNor (None, 4, 4, 256)     1024        conv2d_11[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 activation_10 (Activation)       (None, 4, 4, 256)     0           batch_normalization_10[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_12 (Conv2D)               (None, 4, 4, 256)     590080      activation_10[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_13 (Conv2D)               (None, 4, 4, 256)     33024       activation_9[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 batch_normalization_11 (BatchNor (None, 4, 4, 256)     1024        conv2d_12[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 add_5 (Add)                      (None, 4, 4, 256)     0           conv2d_13[0][0]\cf1\highlight1 
\par \cf0\highlight0                                                                    batch_normalization_11[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 activation_11 (Activation)       (None, 4, 4, 256)     0           add_5[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_14 (Conv2D)               (None, 4, 4, 256)     590080      activation_11[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 batch_normalization_12 (BatchNor (None, 4, 4, 256)     1024        conv2d_14[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 activation_12 (Activation)       (None, 4, 4, 256)     0           batch_normalization_12[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_15 (Conv2D)               (None, 4, 4, 256)     590080      activation_12[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 batch_normalization_13 (BatchNor (None, 4, 4, 256)     1024        conv2d_15[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 add_6 (Add)                      (None, 4, 4, 256)     0           activation_11[0][0]\cf1\highlight1 
\par \cf0\highlight0                                                                    batch_normalization_13[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 activation_13 (Activation)       (None, 4, 4, 256)     0           add_6[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_16 (Conv2D)               (None, 2, 2, 512)     1180160     activation_13[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 batch_normalization_14 (BatchNor (None, 2, 2, 512)     2048        conv2d_16[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 activation_14 (Activation)       (None, 2, 2, 512)     0           batch_normalization_14[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_17 (Conv2D)               (None, 2, 2, 512)     2359808     activation_14[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_18 (Conv2D)               (None, 2, 2, 512)     131584      activation_13[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 batch_normalization_15 (BatchNor (None, 2, 2, 512)     2048        conv2d_17[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 add_7 (Add)                      (None, 2, 2, 512)     0           conv2d_18[0][0]\cf1\highlight1 
\par \cf0\highlight0                                                                    batch_normalization_15[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 activation_15 (Activation)       (None, 2, 2, 512)     0           add_7[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_19 (Conv2D)               (None, 2, 2, 512)     2359808     activation_15[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 batch_normalization_16 (BatchNor (None, 2, 2, 512)     2048        conv2d_19[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 activation_16 (Activation)       (None, 2, 2, 512)     0           batch_normalization_16[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_20 (Conv2D)               (None, 2, 2, 512)     2359808     activation_16[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 batch_normalization_17 (BatchNor (None, 2, 2, 512)     2048        conv2d_20[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 add_8 (Add)                      (None, 2, 2, 512)     0           activation_15[0][0]\cf1\highlight1 
\par \cf0\highlight0                                                                    batch_normalization_17[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 activation_17 (Activation)       (None, 2, 2, 512)     0           add_8[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 average_pooling2d_1 (AveragePool (None, 1, 1, 512)     0           activation_17[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 flatten_1 (Flatten)              (None, 512)           0           average_pooling2d_1[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 dense_1 (Dense)                  (None, 10)            5130        flatten_1[0][0]\cf1\highlight1 
\par \cf0\highlight0 ====================================================================================================\cf1\highlight1 
\par \cf0\highlight0 Total params: 11,192,458\cf1\highlight1 
\par \cf0\highlight0 Trainable params: 11,184,650\cf1\highlight1 
\par \cf0\highlight0 Non-trainable params: 7,808\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 Using real-time data augmentation.\cf1\highlight1 
\par \cf0\highlight0 Epoch 1/100\cf1\highlight1 
\par \cf0\highlight0 2017-10-24 16:33:22.693830: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could spePU computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-24 16:33:22.693860: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could spePU computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-24 16:33:22.693870: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-24 16:33:22.693879: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-24 16:33:22.693887: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-24 16:33:23.097772: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties:\cf1\highlight1 
\par \cf0\highlight0 name: GeForce GTX TITAN X\cf1\highlight1 
\par \cf0\highlight0 major: 5 minor: 2 memoryClockRate (GHz) 1.076\cf1\highlight1 
\par \cf0\highlight0 pciBusID 0000:02:00.0\cf1\highlight1 
\par \cf0\highlight0 Total memory: 11.92GiB\cf1\highlight1 
\par \cf0\highlight0 Free memory: 11.81GiB\cf1\highlight1 
\par \cf0\highlight0 2017-10-24 16:33:23.097862: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0\cf1\highlight1 
\par \cf0\highlight0 2017-10-24 16:33:23.097880: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y\cf1\highlight1 
\par \cf0\highlight0 2017-10-24 16:33:23.097898: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:02:00.0)\cf1\highlight1 
\par \cf0\highlight0 Traceback (most recent call last):\cf1\highlight1 
\par \cf0\highlight0   File "cifar10_resnet.py", line 237, in <module>\cf1\highlight1 
\par \cf0\highlight0     callbacks=callbacks)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/keras/legacy/interfaces.py", line 87, in wrapper\cf1\highlight1 
\par \cf0\highlight0     return func(*args, **kwargs)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/keras/engine/training.py", line 2049, in fit_generator\cf1\highlight1 
\par \cf0\highlight0     callbacks.on_batch_end(batch_index, batch_logs)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/keras/callbacks.py", line 113, in on_batch_end\cf1\highlight1 
\par \cf0\highlight0     callback.on_batch_end(batch, logs)\cf1\highlight1 
\par \cf0\highlight0   File "cifar10_resnet.py", line 183, in on_batch_end\cf1\highlight1 
\par \cf0\highlight0     do_avg4keras(name_list, self.model)\cf1\highlight1 
\par \cf0\highlight0   File "cifar10_resnet.py", line 168, in do_avg4keras\cf1\highlight1 
\par \cf0\highlight0     w_avg = sum(w_list)/float(layer_num)\cf1\highlight1 
\par \cf0\highlight0 ValueError: operands could not be broadcast together with shapes (3,3,64,128) (3,3,128,128)\cf1\highlight1 
\par \cf0\highlight0 (tf) xdong@gpu2:/data/xdong/codes/avg_para/keras-res$ CUDA_VISIBLE_DEVICES=0 python cifar10_resnet.py\cf1\highlight1 
\par \cf0\highlight0 Using TensorFlow backend.\cf1\highlight1 
\par \cf0\highlight0 Couldn't import dot_parser, loading of dot files will not be possible.\cf1\highlight1 
\par \cf0\highlight0 x_train shape: (50000, 32, 32, 3)\cf1\highlight1 
\par \cf0\highlight0 50000 train samples\cf1\highlight1 
\par \cf0\highlight0 10000 test samples\cf1\highlight1 
\par \cf0\highlight0 y_train shape: (50000, 1)\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 Layer (type)                     Output Shape          Param #     Connected to\cf1\highlight1 
\par \cf0\highlight0 ====================================================================================================\cf1\highlight1 
\par \cf0\highlight0 input_1 (InputLayer)             (None, 32, 32, 3)     0\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_1 (Conv2D)                (None, 16, 16, 64)    9472        input_1[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 batch_normalization_1 (BatchNorm (None, 16, 16, 64)    256         conv2d_1[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 activation_1 (Activation)        (None, 16, 16, 64)    0           batch_normalization_1[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_2 (Conv2D)                (None, 16, 16, 64)    36928       activation_1[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 batch_normalization_2 (BatchNorm (None, 16, 16, 64)    256         conv2d_2[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 activation_2 (Activation)        (None, 16, 16, 64)    0           batch_normalization_2[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_3 (Conv2D)                (None, 16, 16, 64)    36928       activation_2[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 batch_normalization_3 (BatchNorm (None, 16, 16, 64)    256         conv2d_3[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 add_1 (Add)                      (None, 16, 16, 64)    0           activation_1[0][0]\cf1\highlight1 
\par \cf0\highlight0                                                                    batch_normalization_3[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 activation_3 (Activation)        (None, 16, 16, 64)    0           add_1[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_4 (Conv2D)                (None, 16, 16, 64)    36928       activation_3[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 batch_normalization_4 (BatchNorm (None, 16, 16, 64)    256         conv2d_4[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 activation_4 (Activation)        (None, 16, 16, 64)    0           batch_normalization_4[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_5 (Conv2D)                (None, 16, 16, 64)    36928       activation_4[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 batch_normalization_5 (BatchNorm (None, 16, 16, 64)    256         conv2d_5[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 add_2 (Add)                      (None, 16, 16, 64)    0           activation_3[0][0]\cf1\highlight1 
\par \cf0\highlight0                                                                    batch_normalization_5[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 activation_5 (Activation)        (None, 16, 16, 64)    0           add_2[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_6 (Conv2D)                (None, 8, 8, 128)     73856       activation_5[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 batch_normalization_6 (BatchNorm (None, 8, 8, 128)     512         conv2d_6[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 activation_6 (Activation)        (None, 8, 8, 128)     0           batch_normalization_6[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_7 (Conv2D)                (None, 8, 8, 128)     147584      activation_6[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_8 (Conv2D)                (None, 8, 8, 128)     8320        activation_5[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 batch_normalization_7 (BatchNorm (None, 8, 8, 128)     512         conv2d_7[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 add_3 (Add)                      (None, 8, 8, 128)     0           conv2d_8[0][0]\cf1\highlight1 
\par \cf0\highlight0                                                                    batch_normalization_7[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 activation_7 (Activation)        (None, 8, 8, 128)     0           add_3[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_9 (Conv2D)                (None, 8, 8, 128)     147584      activation_7[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 batch_normalization_8 (BatchNorm (None, 8, 8, 128)     512         conv2d_9[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 activation_8 (Activation)        (None, 8, 8, 128)     0           batch_normalization_8[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_10 (Conv2D)               (None, 8, 8, 128)     147584      activation_8[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 batch_normalization_9 (BatchNorm (None, 8, 8, 128)     512         conv2d_10[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 add_4 (Add)                      (None, 8, 8, 128)     0           activation_7[0][0]\cf1\highlight1 
\par \cf0\highlight0                                                                    batch_normalization_9[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 activation_9 (Activation)        (None, 8, 8, 128)     0           add_4[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_11 (Conv2D)               (None, 4, 4, 256)     295168      activation_9[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 batch_normalization_10 (BatchNor (None, 4, 4, 256)     1024        conv2d_11[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 activation_10 (Activation)       (None, 4, 4, 256)     0           batch_normalization_10[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_12 (Conv2D)               (None, 4, 4, 256)     590080      activation_10[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_13 (Conv2D)               (None, 4, 4, 256)     33024       activation_9[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 batch_normalization_11 (BatchNor (None, 4, 4, 256)     1024        conv2d_12[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 add_5 (Add)                      (None, 4, 4, 256)     0           conv2d_13[0][0]\cf1\highlight1 
\par \cf0\highlight0                                                                    batch_normalization_11[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 activation_11 (Activation)       (None, 4, 4, 256)     0           add_5[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_14 (Conv2D)               (None, 4, 4, 256)     590080      activation_11[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 batch_normalization_12 (BatchNor (None, 4, 4, 256)     1024        conv2d_14[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 activation_12 (Activation)       (None, 4, 4, 256)     0           batch_normalization_12[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_15 (Conv2D)               (None, 4, 4, 256)     590080      activation_12[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 batch_normalization_13 (BatchNor (None, 4, 4, 256)     1024        conv2d_15[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 add_6 (Add)                      (None, 4, 4, 256)     0           activation_11[0][0]\cf1\highlight1 
\par \cf0\highlight0                                                                    batch_normalization_13[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 activation_13 (Activation)       (None, 4, 4, 256)     0           add_6[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_16 (Conv2D)               (None, 2, 2, 512)     1180160     activation_13[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 batch_normalization_14 (BatchNor (None, 2, 2, 512)     2048        conv2d_16[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 activation_14 (Activation)       (None, 2, 2, 512)     0           batch_normalization_14[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_17 (Conv2D)               (None, 2, 2, 512)     2359808     activation_14[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_18 (Conv2D)               (None, 2, 2, 512)     131584      activation_13[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 batch_normalization_15 (BatchNor (None, 2, 2, 512)     2048        conv2d_17[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 add_7 (Add)                      (None, 2, 2, 512)     0           conv2d_18[0][0]\cf1\highlight1 
\par \cf0\highlight0                                                                    batch_normalization_15[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 activation_15 (Activation)       (None, 2, 2, 512)     0           add_7[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_19 (Conv2D)               (None, 2, 2, 512)     2359808     activation_15[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 batch_normalization_16 (BatchNor (None, 2, 2, 512)     2048        conv2d_19[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 activation_16 (Activation)       (None, 2, 2, 512)     0           batch_normalization_16[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_20 (Conv2D)               (None, 2, 2, 512)     2359808     activation_16[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 batch_normalization_17 (BatchNor (None, 2, 2, 512)     2048        conv2d_20[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 add_8 (Add)                      (None, 2, 2, 512)     0           activation_15[0][0]\cf1\highlight1 
\par \cf0\highlight0                                                                    batch_normalization_17[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 activation_17 (Activation)       (None, 2, 2, 512)     0           add_8[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 average_pooling2d_1 (AveragePool (None, 1, 1, 512)     0           activation_17[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 flatten_1 (Flatten)              (None, 512)           0           average_pooling2d_1[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 dense_1 (Dense)                  (None, 10)            5130        flatten_1[0][0]\cf1\highlight1 
\par \cf0\highlight0 ====================================================================================================\cf1\highlight1 
\par \cf0\highlight0 Total params: 11,192,458\cf1\highlight1 
\par \cf0\highlight0 Trainable params: 11,184,650\cf1\highlight1 
\par \cf0\highlight0 Non-trainable params: 7,808\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 Using real-time data augmentation.\cf1\highlight1 
\par \cf0\highlight0 Epoch 1/100\cf1\highlight1 
\par \cf0\highlight0 2017-10-24 16:35:27.105482: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could spePU computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-24 16:35:27.105524: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could spePU computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-24 16:35:27.105533: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-24 16:35:27.105541: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-24 16:35:27.105548: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-24 16:35:27.512918: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties:\cf1\highlight1 
\par \cf0\highlight0 name: GeForce GTX TITAN X\cf1\highlight1 
\par \cf0\highlight0 major: 5 minor: 2 memoryClockRate (GHz) 1.076\cf1\highlight1 
\par \cf0\highlight0 pciBusID 0000:02:00.0\cf1\highlight1 
\par \cf0\highlight0 Total memory: 11.92GiB\cf1\highlight1 
\par \cf0\highlight0 Free memory: 11.81GiB\cf1\highlight1 
\par \cf0\highlight0 2017-10-24 16:35:27.513003: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0\cf1\highlight1 
\par \cf0\highlight0 2017-10-24 16:35:27.513020: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y\cf1\highlight1 
\par \cf0\highlight0 2017-10-24 16:35:27.513039: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:02:00.0)\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 Traceback (most recent call last):\cf1\highlight1 
\par \cf0\highlight0   File "cifar10_resnet.py", line 237, in <module>\cf1\highlight1 
\par \cf0\highlight0     callbacks=callbacks)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/keras/legacy/interfaces.py", line 87, in wrapper\cf1\highlight1 
\par \cf0\highlight0     return func(*args, **kwargs)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/keras/engine/training.py", line 2049, in fit_generator\cf1\highlight1 
\par \cf0\highlight0     callbacks.on_batch_end(batch_index, batch_logs)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/keras/callbacks.py", line 113, in on_batch_end\cf1\highlight1 
\par \cf0\highlight0     callback.on_batch_end(batch, logs)\cf1\highlight1 
\par \cf0\highlight0   File "cifar10_resnet.py", line 183, in on_batch_end\cf1\highlight1 
\par \cf0\highlight0     do_avg4keras(name_list, self.model)\cf1\highlight1 
\par \cf0\highlight0   File "cifar10_resnet.py", line 168, in do_avg4keras\cf1\highlight1 
\par \cf0\highlight0     w_avg = sum(w_list)/float(layer_num)\cf1\highlight1 
\par \cf0\highlight0 ValueError: operands could not be broadcast together with shapes (3,3,64,128) (3,3,128,128)\cf1\highlight1 
\par \cf0\highlight0 (tf) xdong@gpu2:/data/xdong/codes/avg_para/keras-res$ CUDA_VISIBLE_DEVICES=0 python cifar10_resnet.py\cf1\highlight1 
\par \cf0\highlight0 Using TensorFlow backend.\cf1\highlight1 
\par \cf0\highlight0 Couldn't import dot_parser, loading of dot files will not be possible.\cf1\highlight1 
\par \cf0\highlight0 x_train shape: (50000, 32, 32, 3)\cf1\highlight1 
\par \cf0\highlight0 50000 train samples\cf1\highlight1 
\par \cf0\highlight0 10000 test samples\cf1\highlight1 
\par \cf0\highlight0 y_train shape: (50000, 1)\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 Layer (type)                     Output Shape          Param #     Connected to\cf1\highlight1 
\par \cf0\highlight0 ====================================================================================================\cf1\highlight1 
\par \cf0\highlight0 input_1 (InputLayer)             (None, 32, 32, 3)     0\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_1 (Conv2D)                (None, 16, 16, 64)    9472        input_1[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 batch_normalization_1 (BatchNorm (None, 16, 16, 64)    256         conv2d_1[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 activation_1 (Activation)        (None, 16, 16, 64)    0           batch_normalization_1[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_2 (Conv2D)                (None, 16, 16, 64)    36928       activation_1[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 batch_normalization_2 (BatchNorm (None, 16, 16, 64)    256         conv2d_2[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 activation_2 (Activation)        (None, 16, 16, 64)    0           batch_normalization_2[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_3 (Conv2D)                (None, 16, 16, 64)    36928       activation_2[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 batch_normalization_3 (BatchNorm (None, 16, 16, 64)    256         conv2d_3[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 add_1 (Add)                      (None, 16, 16, 64)    0           activation_1[0][0]\cf1\highlight1 
\par \cf0\highlight0                                                                    batch_normalization_3[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 activation_3 (Activation)        (None, 16, 16, 64)    0           add_1[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_4 (Conv2D)                (None, 16, 16, 64)    36928       activation_3[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 batch_normalization_4 (BatchNorm (None, 16, 16, 64)    256         conv2d_4[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 activation_4 (Activation)        (None, 16, 16, 64)    0           batch_normalization_4[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_5 (Conv2D)                (None, 16, 16, 64)    36928       activation_4[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 batch_normalization_5 (BatchNorm (None, 16, 16, 64)    256         conv2d_5[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 add_2 (Add)                      (None, 16, 16, 64)    0           activation_3[0][0]\cf1\highlight1 
\par \cf0\highlight0                                                                    batch_normalization_5[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 activation_5 (Activation)        (None, 16, 16, 64)    0           add_2[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_6 (Conv2D)                (None, 8, 8, 128)     73856       activation_5[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 batch_normalization_6 (BatchNorm (None, 8, 8, 128)     512         conv2d_6[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 activation_6 (Activation)        (None, 8, 8, 128)     0           batch_normalization_6[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_7 (Conv2D)                (None, 8, 8, 128)     147584      activation_6[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_8 (Conv2D)                (None, 8, 8, 128)     8320        activation_5[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 batch_normalization_7 (BatchNorm (None, 8, 8, 128)     512         conv2d_7[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 add_3 (Add)                      (None, 8, 8, 128)     0           conv2d_8[0][0]\cf1\highlight1 
\par \cf0\highlight0                                                                    batch_normalization_7[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 activation_7 (Activation)        (None, 8, 8, 128)     0           add_3[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_9 (Conv2D)                (None, 8, 8, 128)     147584      activation_7[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 batch_normalization_8 (BatchNorm (None, 8, 8, 128)     512         conv2d_9[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 activation_8 (Activation)        (None, 8, 8, 128)     0           batch_normalization_8[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_10 (Conv2D)               (None, 8, 8, 128)     147584      activation_8[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 batch_normalization_9 (BatchNorm (None, 8, 8, 128)     512         conv2d_10[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 add_4 (Add)                      (None, 8, 8, 128)     0           activation_7[0][0]\cf1\highlight1 
\par \cf0\highlight0                                                                    batch_normalization_9[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 activation_9 (Activation)        (None, 8, 8, 128)     0           add_4[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_11 (Conv2D)               (None, 4, 4, 256)     295168      activation_9[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 batch_normalization_10 (BatchNor (None, 4, 4, 256)     1024        conv2d_11[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 activation_10 (Activation)       (None, 4, 4, 256)     0           batch_normalization_10[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_12 (Conv2D)               (None, 4, 4, 256)     590080      activation_10[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_13 (Conv2D)               (None, 4, 4, 256)     33024       activation_9[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 batch_normalization_11 (BatchNor (None, 4, 4, 256)     1024        conv2d_12[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 add_5 (Add)                      (None, 4, 4, 256)     0           conv2d_13[0][0]\cf1\highlight1 
\par \cf0\highlight0                                                                    batch_normalization_11[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 activation_11 (Activation)       (None, 4, 4, 256)     0           add_5[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_14 (Conv2D)               (None, 4, 4, 256)     590080      activation_11[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 batch_normalization_12 (BatchNor (None, 4, 4, 256)     1024        conv2d_14[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 activation_12 (Activation)       (None, 4, 4, 256)     0           batch_normalization_12[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_15 (Conv2D)               (None, 4, 4, 256)     590080      activation_12[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 batch_normalization_13 (BatchNor (None, 4, 4, 256)     1024        conv2d_15[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 add_6 (Add)                      (None, 4, 4, 256)     0           activation_11[0][0]\cf1\highlight1 
\par \cf0\highlight0                                                                    batch_normalization_13[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 activation_13 (Activation)       (None, 4, 4, 256)     0           add_6[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_16 (Conv2D)               (None, 2, 2, 512)     1180160     activation_13[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 batch_normalization_14 (BatchNor (None, 2, 2, 512)     2048        conv2d_16[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 activation_14 (Activation)       (None, 2, 2, 512)     0           batch_normalization_14[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_17 (Conv2D)               (None, 2, 2, 512)     2359808     activation_14[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_18 (Conv2D)               (None, 2, 2, 512)     131584      activation_13[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 batch_normalization_15 (BatchNor (None, 2, 2, 512)     2048        conv2d_17[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 add_7 (Add)                      (None, 2, 2, 512)     0           conv2d_18[0][0]\cf1\highlight1 
\par \cf0\highlight0                                                                    batch_normalization_15[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 activation_15 (Activation)       (None, 2, 2, 512)     0           add_7[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_19 (Conv2D)               (None, 2, 2, 512)     2359808     activation_15[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 batch_normalization_16 (BatchNor (None, 2, 2, 512)     2048        conv2d_19[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 activation_16 (Activation)       (None, 2, 2, 512)     0           batch_normalization_16[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_20 (Conv2D)               (None, 2, 2, 512)     2359808     activation_16[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 batch_normalization_17 (BatchNor (None, 2, 2, 512)     2048        conv2d_20[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 add_8 (Add)                      (None, 2, 2, 512)     0           activation_15[0][0]\cf1\highlight1 
\par \cf0\highlight0                                                                    batch_normalization_17[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 activation_17 (Activation)       (None, 2, 2, 512)     0           add_8[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 average_pooling2d_1 (AveragePool (None, 1, 1, 512)     0           activation_17[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 flatten_1 (Flatten)              (None, 512)           0           average_pooling2d_1[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 dense_1 (Dense)                  (None, 10)            5130        flatten_1[0][0]\cf1\highlight1 
\par \cf0\highlight0 ====================================================================================================\cf1\highlight1 
\par \cf0\highlight0 Total params: 11,192,458\cf1\highlight1 
\par \cf0\highlight0 Trainable params: 11,184,650\cf1\highlight1 
\par \cf0\highlight0 Non-trainable params: 7,808\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 Using real-time data augmentation.\cf1\highlight1 
\par \cf0\highlight0 Epoch 1/100\cf1\highlight1 
\par \cf0\highlight0 2017-10-24 16:38:49.351630: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could spePU computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-24 16:38:49.351688: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could spePU computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-24 16:38:49.351703: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-24 16:38:49.351716: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-24 16:38:49.351729: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-24 16:38:49.774197: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties:\cf1\highlight1 
\par \cf0\highlight0 name: GeForce GTX TITAN X\cf1\highlight1 
\par \cf0\highlight0 major: 5 minor: 2 memoryClockRate (GHz) 1.076\cf1\highlight1 
\par \cf0\highlight0 pciBusID 0000:02:00.0\cf1\highlight1 
\par \cf0\highlight0 Total memory: 11.92GiB\cf1\highlight1 
\par \cf0\highlight0 Free memory: 11.81GiB\cf1\highlight1 
\par \cf0\highlight0 2017-10-24 16:38:49.774284: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0\cf1\highlight1 
\par \cf0\highlight0 2017-10-24 16:38:49.774300: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y\cf1\highlight1 
\par \cf0\highlight0 2017-10-24 16:38:49.774317: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:02:00.0)\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0    1/1562 [..............................] - ETA: 8793s - loss: 4.4338 - acc: 0.06251\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0    2/1562 [..............................] - ETA: 4487s - loss: 3.8211 - acc: 0.1250/home/xdong/tf/local/lib/python2.7/site-packages/keras/callbacks.py:120: UserWarning: Method on_batch_end() is slow d to the batch update (1.026700). Check your callbacks.\cf1\highlight1 
\par \cf0\highlight0   % delta_t_median)\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0    3/1562 [..............................] - ETA: 3053s - loss: 3.8980 - acc: 0.09381\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0    4/1562 [..............................] - ETA: 2334s - loss: 3.8673 - acc: 0.12501\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0    5/1562 [..............................] - ETA: 1907s - loss: 3.9072 - acc: 0.11871\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0    6/1562 [..............................] - ETA: 1620s - loss: 3.8069 - acc: 0.14581\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0    7/1562 [..............................] - ETA: 1417s - loss: 3.7165 - acc: 0.12501\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0    8/1562 [..............................] - ETA: 1264s - loss: 3.6399 - acc: 0.12111\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0    9/1562 [..............................] - ETA: 1147s - loss: 3.5443 - acc: 0.13891\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0   10/1562 [..............................] - ETA: 1051s - loss: 3.4808 - acc: 0.14691\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0   11/1562 [..............................] - ETA: 973s - loss: 3.4280 - acc: 0.1534 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0   12/1562 [..............................] - ETA: 907s - loss: 3.3693 - acc: 0.16151\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0   13/1562 [..............................] - ETA: 852s - loss: 3.3358 - acc: 0.16831\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0   14/1562 [..............................] - ETA: 805s - loss: 3.3073 - acc: 0.16741\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0   15/1562 [..............................] - ETA: 764s - loss: 3.2715 - acc: 0.17081\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0   16/1562 [..............................] - ETA: 728s - loss: 3.2411 - acc: 0.17191\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0   17/1562 [..............................] - ETA: 697s - loss: 3.2244 - acc: 0.17101\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0   18/1562 [..............................] - ETA: 668s - loss: 3.2006 - acc: 0.17711\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0   19/1562 [..............................] - ETA: 643s - loss: 3.1838 - acc: 0.17601\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0   20/1562 [..............................] - ETA: 620s - loss: 3.1663 - acc: 0.17811\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0   21/1562 [..............................] - ETA: 599s - loss: 3.1626 - acc: 0.17411\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0   22/1562 [..............................] - ETA: 581s - loss: 3.1498 - acc: 0.18041\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0   23/1562 [..............................] - ETA: 563s - loss: 3.1201 - acc: 0.19161\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0   24/1562 [..............................] - ETA: 547s - loss: 3.1143 - acc: 0.19271\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0   25/1562 [..............................] - ETA: 533s - loss: 3.0901 - acc: 0.19631\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0   26/1562 [..............................] - ETA: 520s - loss: 3.0706 - acc: 0.20071\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0   27/1562 [..............................] - ETA: 507s - loss: 3.0544 - acc: 0.20021\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0   28/1562 [..............................] - ETA: 496s - loss: 3.0278 - acc: 0.20651\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0   29/1562 [..............................] - ETA: 485s - loss: 3.0156 - acc: 0.20581\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0   30/1562 [..............................] - ETA: 475s - loss: 3.0079 - acc: 0.20311\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0   31/1562 [..............................] - ETA: 466s - loss: 2.9976 - acc: 0.20261\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0   32/1562 [..............................] - ETA: 457s - loss: 2.9783 - acc: 0.20611\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0   33/1562 [..............................] - ETA: 449s - loss: 2.9762 - acc: 0.20551\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0   34/1562 [..............................] - ETA: 442s - loss: 2.9745 - acc: 0.20681\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0   35/1562 [..............................] - ETA: 435s - loss: 2.9651 - acc: 0.20981\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0   36/1562 [..............................] - ETA: 428s - loss: 2.9629 - acc: 0.21271\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0   37/1562 [..............................] - ETA: 421s - loss: 2.9604 - acc: 0.21621\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0   38/1562 [..............................] - ETA: 414s - loss: 2.9525 - acc: 0.21961\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0   39/1562 [..............................] - ETA: 408s - loss: 2.9434 - acc: 0.22441\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0   40/1562 [..............................] - ETA: 403s - loss: 2.9289 - acc: 0.22731\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0   41/1562 [..............................] - ETA: 397s - loss: 2.9196 - acc: 0.22711\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0   42/1562 [..............................] - ETA: 392s - loss: 2.9091 - acc: 0.22691\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0   43/1562 [..............................] - ETA: 387s - loss: 2.8973 - acc: 0.22821\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0   44/1562 [..............................] - ETA: 383s - loss: 2.8871 - acc: 0.22871\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0   45/1562 [..............................] - ETA: 378s - loss: 2.8768 - acc: 0.23131\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0   46/1562 [..............................] - ETA: 374s - loss: 2.8709 - acc: 0.23231\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0   47/1562 [..............................] - ETA: 370s - loss: 2.8638 - acc: 0.23401\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0   48/1562 [..............................] - ETA: 365s - loss: 2.8582 - acc: 0.23371\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0   49/1562 [..............................] - ETA: 361s - loss: 2.8597 - acc: 0.23411\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0   50/1562 [..............................] - ETA: 358s - loss: 2.8518 - acc: 0.23621\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0   51/1562 [..............................] - ETA: 354s - loss: 2.8390 - acc: 0.24021\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0   52/1562 [..............................] - ETA: 351s - loss: 2.8258 - acc: 0.24521\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0   53/1562 [>.............................] - ETA: 348s - loss: 2.8277 - acc: 0.24291\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0   54/1562 [>.............................] - ETA: 345s - loss: 2.8381 - acc: 0.23961\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0   55/1562 [>.............................] - ETA: 342s - loss: 2.8393 - acc: 0.23981\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0   56/1562 [>.............................] - ETA: 339s - loss: 2.8318 - acc: 0.24111\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0   57/1562 [>.............................] - ETA: 336s - loss: 2.8242 - acc: 0.24341\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0   58/1562 [>.............................] - ETA: 333s - loss: 2.8210 - acc: 0.24301\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0   59/1562 [>.............................] - ETA: 331s - loss: 2.8117 - acc: 0.24521\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0   60/1562 [>.............................] - ETA: 328s - loss: 2.8042 - acc: 0.24741\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0   61/1562 [>.............................] - ETA: 326s - loss: 2.7979 - acc: 0.24851\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0   62/1562 [>.............................] - ETA: 323s - loss: 2.7935 - acc: 0.24901\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0   63/1562 [>.............................] - ETA: 321s - loss: 2.7864 - acc: 0.24901\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0   64/1562 [>.............................] - ETA: 319s - loss: 2.7799 - acc: 0.25101\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0   65/1562 [>.............................] - ETA: 317s - loss: 2.7759 - acc: 0.25101\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0   66/1562 [>.............................] - ETA: 315s - loss: 2.7761 - acc: 0.25001\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0   67/1562 [>.............................] - ETA: 313s - loss: 2.7745 - acc: 0.25001\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0   68/1562 [>.............................] - ETA: 311s - loss: 2.7715 - acc: 0.25001\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0   69/1562 [>.............................] - ETA: 309s - loss: 2.7631 - acc: 0.25361\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0   70/1562 [>.............................] - ETA: 307s - loss: 2.7544 - acc: 0.25711\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0   71/1562 [>.............................] - ETA: 305s - loss: 2.7505 - acc: 0.25701\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0   72/1562 [>.............................] - ETA: 303s - loss: 2.7460 - acc: 0.25691\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0   73/1562 [>.............................] - ETA: 301s - loss: 2.7462 - acc: 0.25681\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0   74/1562 [>.............................] - ETA: 299s - loss: 2.7415 - acc: 0.25801\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0   75/1562 [>.............................] - ETA: 298s - loss: 2.7334 - acc: 0.26121\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0   76/1562 [>.............................] - ETA: 296s - loss: 2.7322 - acc: 0.26111\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0   77/1562 [>.............................] - ETA: 294s - loss: 2.7247 - acc: 0.26341\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0   78/1562 [>.............................] - ETA: 293s - loss: 2.7214 - acc: 0.26561\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0   79/1562 [>.............................] - ETA: 291s - loss: 2.7189 - acc: 0.26621\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0   80/1562 [>.............................] - ETA: 290s - loss: 2.7182 - acc: 0.26561\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0   81/1562 [>.............................] - ETA: 289s - loss: 2.7191 - acc: 0.26621\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0   82/1562 [>.............................] - ETA: 287s - loss: 2.7159 - acc: 0.26751\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0   83/1562 [>.............................] - ETA: 286s - loss: 2.7126 - acc: 0.26961\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0   84/1562 [>.............................] - ETA: 284s - loss: 2.7109 - acc: 0.26861\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0   85/1562 [>.............................] - ETA: 283s - loss: 2.7073 - acc: 0.26951\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0   86/1562 [>.............................] - ETA: 282s - loss: 2.7046 - acc: 0.26891\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0   87/1562 [>.............................] - ETA: 280s - loss: 2.6994 - acc: 0.26981\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0   88/1562 [>.............................] - ETA: 279s - loss: 2.7004 - acc: 0.26881\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0   89/1562 [>.............................] - ETA: 278s - loss: 2.7000 - acc: 0.26931\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0   90/1562 [>.............................] - ETA: 277s - loss: 2.6952 - acc: 0.26941\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0   91/1562 [>.............................] - ETA: 276s - loss: 2.6943 - acc: 0.26961\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0   92/1562 [>.............................] - ETA: 274s - loss: 2.6980 - acc: 0.26901\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0   93/1562 [>.............................] - ETA: 273s - loss: 2.6971 - acc: 0.26951\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0   94/1562 [>.............................] - ETA: 272s - loss: 2.6967 - acc: 0.26861\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0   95/1562 [>.............................] - ETA: 271s - loss: 2.6915 - acc: 0.26971\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0   96/1562 [>.............................] - ETA: 270s - loss: 2.6910 - acc: 0.26921\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0   97/1562 [>.............................] - ETA: 269s - loss: 2.6845 - acc: 0.27001\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0   98/1562 [>.............................] - ETA: 268s - loss: 2.6801 - acc: 0.27071\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0   99/1562 [>.............................] - ETA: 267s - loss: 2.6795 - acc: 0.27051\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0  100/1562 [>.............................] - ETA: 266s - loss: 2.6755 - acc: 0.27191\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0  101/1562 [>.............................] - ETA: 265s - loss: 2.6718 - acc: 0.27201\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0  102/1562 [>.............................] - ETA: 264s - loss: 2.6698 - acc: 0.27211\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0  103/1562 [>.............................] - ETA: 263s - loss: 2.6698 - acc: 0.27121\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0  104/1562 [>.............................] - ETA: 263s - loss: 2.6668 - acc: 0.27131\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0  105/1562 [=>............................] - ETA: 262s - loss: 2.6645 - acc: 0.27231\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0  106/1562 [=>............................] - ETA: 261s - loss: 2.6630 - acc: 0.27331\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0  107/1562 [=>............................] - ETA: 260s - loss: 2.6591 - acc: 0.27391\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0  108/1562 [=>............................] - ETA: 259s - loss: 2.6557 - acc: 0.27521\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0  109/1562 [=>............................] - ETA: 259s - loss: 2.6530 - acc: 0.27521\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0  110/1562 [=>............................] - ETA: 258s - loss: 2.6495 - acc: 0.27591\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0  111/1562 [=>............................] - ETA: 257s - loss: 2.6452 - acc: 0.27761\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0  112/1562 [=>............................] - ETA: 256s - loss: 2.6399 - acc: 0.27931\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0  113/1562 [=>............................] - ETA: 256s - loss: 2.6380 - acc: 0.27961\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0  114/1562 [=>............................] - ETA: 255s - loss: 2.6344 - acc: 0.27991\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0  115/1562 [=>............................] - ETA: 254s - loss: 2.6325 - acc: 0.28021\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0  116/1562 [=>............................] - ETA: 253s - loss: 2.6280 - acc: 0.28181\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0  117/1562 [=>............................] - ETA: 253s - loss: 2.6239 - acc: 0.28341\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0  118/1562 [=>............................] - ETA: 252s - loss: 2.6236 - acc: 0.28281\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0  119/1562 [=>............................] - ETA: 251s - loss: 2.6218 - acc: 0.28361\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0  120/1562 [=>............................] - ETA: 250s - loss: 2.6193 - acc: 0.28491\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0  121/1562 [=>............................] - ETA: 250s - loss: 2.6165 - acc: 0.28561\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0  122/1562 [=>............................] - ETA: 249s - loss: 2.6129 - acc: 0.28691\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0  123/1562 [=>............................] - ETA: 248s - loss: 2.6093 - acc: 0.28731\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0  124/1562 [=>............................] - ETA: 248s - loss: 2.6035 - acc: 0.28961\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0  125/1562 [=>............................] - ETA: 247s - loss: 2.6055 - acc: 0.28901\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0  126/1562 [=>............................] - ETA: 246s - loss: 2.6055 - acc: 0.28841\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0  127/1562 [=>............................] - ETA: 246s - loss: 2.6072 - acc: 0.28791\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0  128/1562 [=>............................] - ETA: 245s - loss: 2.6054 - acc: 0.28781\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0  129/1562 [=>............................] - ETA: 245s - loss: 2.6030 - acc: 0.28881\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0  130/1562 [=>............................] - ETA: 244s - loss: 2.6024 - acc: 0.28871\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0  131/1562 [=>............................] - ETA: 243s - loss: 2.5986 - acc: 0.28961\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0  132/1562 [=>............................] - ETA: 243s - loss: 2.5983 - acc: 0.28981\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0  133/1562 [=>............................] - ETA: 242s - loss: 2.5953 - acc: 0.29111\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0  134/1562 [=>............................] - ETA: 242s - loss: 2.5940 - acc: 0.29081\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0  135/1562 [=>............................] - ETA: 241s - loss: 2.5908 - acc: 0.29191\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0  136/1562 [=>............................] - ETA: 240s - loss: 2.5885 - acc: 0.29321\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0  137/1562 [=>............................] - ETA: 240s - loss: 2.5863 - acc: 0.29381\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0  138/1562 [=>............................] - ETA: 239s - loss: 2.5851 - acc: 0.29371\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0  139/1562 [=>............................] - ETA: 239s - loss: 2.5831 - acc: 0.29381\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0  140/1562 [=>............................] - ETA: 238s - loss: 2.5810 - acc: 0.29381\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0  141/1562 [=>............................] - ETA: 237s - loss: 2.5783 - acc: 0.29481\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0  142/1562 [=>............................] - ETA: 237s - loss: 2.5783 - acc: 0.29511\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0  143/1562 [=>............................] - ETA: 236s - loss: 2.5771 - acc: 0.29481\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0  144/1562 [=>............................] - ETA: 236s - loss: 2.5759 - acc: 0.29541\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0  145/1562 [=>............................] - ETA: 235s - loss: 2.5728 - acc: 0.29571\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0  146/1562 [=>............................] - ETA: 235s - loss: 2.5723 - acc: 0.29521\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0  147/1562 [=>............................] - ETA: 234s - loss: 2.5707 - acc: 0.29511\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0  148/1562 [=>............................] - ETA: 234s - loss: 2.5708 - acc: 0.29521\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0  149/1562 [=>............................] - ETA: 233s - loss: 2.5688 - acc: 0.29591\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0  150/1562 [=>............................] - ETA: 233s - loss: 2.5641 - acc: 0.29731\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0  151/1562 [=>............................] - ETA: 232s - loss: 2.5620 - acc: 0.29841\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0  152/1562 [=>............................] - ETA: 232s - loss: 2.5596 - acc: 0.29951\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0 1\cf1\highlight1 
\par \cf0\highlight0  153/1562 [=>............................] - ETA: 231s - loss: 2.5588 - acc: 0.29941\cf1\highlight1 
\par \cf0\highlight0 ^CTraceback (most recent call last):\cf1\highlight1 
\par \cf0\highlight0   File "cifar10_resnet.py", line 237, in <module>\cf1\highlight1 
\par \cf0\highlight0     callbacks=callbacks)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/keras/legacy/interfaces.py", line 87, in wrapper\cf1\highlight1 
\par \cf0\highlight0     return func(*args, **kwargs)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/keras/engine/training.py", line 2049, in fit_generator\cf1\highlight1 
\par \cf0\highlight0     callbacks.on_batch_end(batch_index, batch_logs)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/keras/callbacks.py", line 113, in on_batch_end\cf1\highlight1 
\par \cf0\highlight0     callback.on_batch_end(batch, logs)\cf1\highlight1 
\par \cf0\highlight0   File "cifar10_resnet.py", line 177, in on_batch_end\cf1\highlight1 
\par \cf0\highlight0     do_avg4keras(name_list, self.model)\cf1\highlight1 
\par \cf0\highlight0   File "cifar10_resnet.py", line 170, in do_avg4keras\cf1\highlight1 
\par \cf0\highlight0     map(wrap_set_weights,layer_name_list, [[w_avg, b_avg]]*layer_num, [model]*layer_num)\cf1\highlight1 
\par \cf0\highlight0   File "cifar10_resnet.py", line 158, in wrap_set_weights\cf1\highlight1 
\par \cf0\highlight0     return model.get_layer(name_str).set_weights(value)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/keras/engine/topology.py", line 1194, in set_weights\cf1\highlight1 
\par \cf0\highlight0     param_values = K.batch_get_value(params)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py", line 2143, in batch_get_value\cf1\highlight1 
\par \cf0\highlight0     return get_session().run(ops)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 895, in run\cf1\highlight1 
\par \cf0\highlight0     run_metadata_ptr)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1124, in _run\cf1\highlight1 
\par \cf0\highlight0     feed_dict_tensor, options, run_metadata)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1291, in _do_run\cf1\highlight1 
\par \cf0\highlight0     fetches = _name_list(fetch_list)\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 515, in _name_list\cf1\highlight1 
\par \cf0\highlight0     return [compat.as_bytes(t.name) for t in tensor_list]\cf1\highlight1 
\par \cf0\highlight0   File "/home/xdong/tf/local/lib/python2.7/site-packages/tensorflow/python/util/compat.py", line 60, in as_bytes\cf1\highlight1 
\par \cf0\highlight0     return bytes_or_text.encode(encoding)\cf1\highlight1 
\par \cf0\highlight0 KeyboardInterrupt\cf1\highlight1 
\par \cf0\highlight0 (tf) xdong@gpu2:/data/xdong/codes/avg_para/keras-res$ clear\cf1\highlight1 
\par \cf0\highlight0 (tf) xdong@gpu2:/data/xdong/codes/avg_para/keras-res$ CUDA_VISIBLE_DEVICES=0 python cifar10_resnet.py\cf1\highlight1 
\par \cf0\highlight0 Using TensorFlow backend.\cf1\highlight1 
\par \cf0\highlight0 Couldn't import dot_parser, loading of dot files will not be possible.\cf1\highlight1 
\par \cf0\highlight0 x_train shape: (50000, 32, 32, 3)\cf1\highlight1 
\par \cf0\highlight0 50000 train samples\cf1\highlight1 
\par \cf0\highlight0 10000 test samples\cf1\highlight1 
\par \cf0\highlight0 y_train shape: (50000, 1)\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 Layer (type)                     Output Shape          Param #     Connected to\cf1\highlight1 
\par \cf0\highlight0 ====================================================================================================\cf1\highlight1 
\par \cf0\highlight0 input_1 (InputLayer)             (None, 32, 32, 3)     0\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_1 (Conv2D)                (None, 16, 16, 64)    9472        input_1[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 batch_normalization_1 (BatchNorm (None, 16, 16, 64)    256         conv2d_1[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 activation_1 (Activation)        (None, 16, 16, 64)    0           batch_normalization_1[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_2 (Conv2D)                (None, 16, 16, 64)    36928       activation_1[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 batch_normalization_2 (BatchNorm (None, 16, 16, 64)    256         conv2d_2[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 activation_2 (Activation)        (None, 16, 16, 64)    0           batch_normalization_2[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_3 (Conv2D)                (None, 16, 16, 64)    36928       activation_2[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 batch_normalization_3 (BatchNorm (None, 16, 16, 64)    256         conv2d_3[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 add_1 (Add)                      (None, 16, 16, 64)    0           activation_1[0][0]\cf1\highlight1 
\par \cf0\highlight0                                                                    batch_normalization_3[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 activation_3 (Activation)        (None, 16, 16, 64)    0           add_1[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_4 (Conv2D)                (None, 16, 16, 64)    36928       activation_3[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 batch_normalization_4 (BatchNorm (None, 16, 16, 64)    256         conv2d_4[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 activation_4 (Activation)        (None, 16, 16, 64)    0           batch_normalization_4[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_5 (Conv2D)                (None, 16, 16, 64)    36928       activation_4[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 batch_normalization_5 (BatchNorm (None, 16, 16, 64)    256         conv2d_5[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 add_2 (Add)                      (None, 16, 16, 64)    0           activation_3[0][0]\cf1\highlight1 
\par \cf0\highlight0                                                                    batch_normalization_5[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 activation_5 (Activation)        (None, 16, 16, 64)    0           add_2[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_6 (Conv2D)                (None, 8, 8, 128)     73856       activation_5[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 batch_normalization_6 (BatchNorm (None, 8, 8, 128)     512         conv2d_6[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 activation_6 (Activation)        (None, 8, 8, 128)     0           batch_normalization_6[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_7 (Conv2D)                (None, 8, 8, 128)     147584      activation_6[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_8 (Conv2D)                (None, 8, 8, 128)     8320        activation_5[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 batch_normalization_7 (BatchNorm (None, 8, 8, 128)     512         conv2d_7[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 add_3 (Add)                      (None, 8, 8, 128)     0           conv2d_8[0][0]\cf1\highlight1 
\par \cf0\highlight0                                                                    batch_normalization_7[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 activation_7 (Activation)        (None, 8, 8, 128)     0           add_3[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_9 (Conv2D)                (None, 8, 8, 128)     147584      activation_7[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 batch_normalization_8 (BatchNorm (None, 8, 8, 128)     512         conv2d_9[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 activation_8 (Activation)        (None, 8, 8, 128)     0           batch_normalization_8[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_10 (Conv2D)               (None, 8, 8, 128)     147584      activation_8[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 batch_normalization_9 (BatchNorm (None, 8, 8, 128)     512         conv2d_10[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 add_4 (Add)                      (None, 8, 8, 128)     0           activation_7[0][0]\cf1\highlight1 
\par \cf0\highlight0                                                                    batch_normalization_9[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 activation_9 (Activation)        (None, 8, 8, 128)     0           add_4[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_11 (Conv2D)               (None, 4, 4, 256)     295168      activation_9[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 batch_normalization_10 (BatchNor (None, 4, 4, 256)     1024        conv2d_11[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 activation_10 (Activation)       (None, 4, 4, 256)     0           batch_normalization_10[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_12 (Conv2D)               (None, 4, 4, 256)     590080      activation_10[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_13 (Conv2D)               (None, 4, 4, 256)     33024       activation_9[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 batch_normalization_11 (BatchNor (None, 4, 4, 256)     1024        conv2d_12[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 add_5 (Add)                      (None, 4, 4, 256)     0           conv2d_13[0][0]\cf1\highlight1 
\par \cf0\highlight0                                                                    batch_normalization_11[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 activation_11 (Activation)       (None, 4, 4, 256)     0           add_5[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_14 (Conv2D)               (None, 4, 4, 256)     590080      activation_11[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 batch_normalization_12 (BatchNor (None, 4, 4, 256)     1024        conv2d_14[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 activation_12 (Activation)       (None, 4, 4, 256)     0           batch_normalization_12[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_15 (Conv2D)               (None, 4, 4, 256)     590080      activation_12[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 batch_normalization_13 (BatchNor (None, 4, 4, 256)     1024        conv2d_15[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 add_6 (Add)                      (None, 4, 4, 256)     0           activation_11[0][0]\cf1\highlight1 
\par \cf0\highlight0                                                                    batch_normalization_13[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 activation_13 (Activation)       (None, 4, 4, 256)     0           add_6[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_16 (Conv2D)               (None, 2, 2, 512)     1180160     activation_13[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 batch_normalization_14 (BatchNor (None, 2, 2, 512)     2048        conv2d_16[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 activation_14 (Activation)       (None, 2, 2, 512)     0           batch_normalization_14[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_17 (Conv2D)               (None, 2, 2, 512)     2359808     activation_14[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_18 (Conv2D)               (None, 2, 2, 512)     131584      activation_13[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 batch_normalization_15 (BatchNor (None, 2, 2, 512)     2048        conv2d_17[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 add_7 (Add)                      (None, 2, 2, 512)     0           conv2d_18[0][0]\cf1\highlight1 
\par \cf0\highlight0                                                                    batch_normalization_15[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 activation_15 (Activation)       (None, 2, 2, 512)     0           add_7[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_19 (Conv2D)               (None, 2, 2, 512)     2359808     activation_15[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 batch_normalization_16 (BatchNor (None, 2, 2, 512)     2048        conv2d_19[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 activation_16 (Activation)       (None, 2, 2, 512)     0           batch_normalization_16[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 conv2d_20 (Conv2D)               (None, 2, 2, 512)     2359808     activation_16[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 batch_normalization_17 (BatchNor (None, 2, 2, 512)     2048        conv2d_20[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 add_8 (Add)                      (None, 2, 2, 512)     0           activation_15[0][0]\cf1\highlight1 
\par \cf0\highlight0                                                                    batch_normalization_17[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 activation_17 (Activation)       (None, 2, 2, 512)     0           add_8[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 average_pooling2d_1 (AveragePool (None, 1, 1, 512)     0           activation_17[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 flatten_1 (Flatten)              (None, 512)           0           average_pooling2d_1[0][0]\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 dense_1 (Dense)                  (None, 10)            5130        flatten_1[0][0]\cf1\highlight1 
\par \cf0\highlight0 ====================================================================================================\cf1\highlight1 
\par \cf0\highlight0 Total params: 11,192,458\cf1\highlight1 
\par \cf0\highlight0 Trainable params: 11,184,650\cf1\highlight1 
\par \cf0\highlight0 Non-trainable params: 7,808\cf1\highlight1 
\par \cf0\highlight0 ____________________________________________________________________________________________________\cf1\highlight1 
\par \cf0\highlight0 Using real-time data augmentation.\cf1\highlight1 
\par \cf0\highlight0 Epoch 1/100\cf1\highlight1 
\par \cf0\highlight0 2017-10-24 16:41:03.057772: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could spePU computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-24 16:41:03.057809: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could spePU computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-24 16:41:03.057818: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-24 16:41:03.057826: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-24 16:41:03.057834: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed computations.\cf1\highlight1 
\par \cf0\highlight0 2017-10-24 16:41:03.465391: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties:\cf1\highlight1 
\par \cf0\highlight0 name: GeForce GTX TITAN X\cf1\highlight1 
\par \cf0\highlight0 major: 5 minor: 2 memoryClockRate (GHz) 1.076\cf1\highlight1 
\par \cf0\highlight0 pciBusID 0000:02:00.0\cf1\highlight1 
\par \cf0\highlight0 Total memory: 11.92GiB\cf1\highlight1 
\par \cf0\highlight0 Free memory: 11.81GiB\cf1\highlight1 
\par \cf0\highlight0 2017-10-24 16:41:03.465502: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0\cf1\highlight1 
\par \cf0\highlight0 2017-10-24 16:41:03.465523: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y\cf1\highlight1 
\par \cf0\highlight0 2017-10-24 16:41:03.465545: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:02:00.0)\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0    2/1562 [..............................] - ETA: 4676s - loss: 4.5297 - acc: 0.0781/home/xdong/tf/local/lib/python2.7/site-packages/keras/callbacks.py:120: UserWarning: Method on_batch_end() is slow d to the batch update (1.083620). Check your callbacks.\cf1\highlight1 
\par \cf0\highlight0   % delta_t_median)\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 1.9575 - acc: 0.4592Ave Done ......\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 200s - loss: 1.9574 - acc: 0.4592 - val_loss: 1.7060 - val_acc: 0.5234ls/cifar10_resnet_model.h5\cf1\highlight1 
\par \cf0\highlight0 Epoch 2/100\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 1.4444 - acc: 0.6052Ave Done .....\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 193s - loss: 1.4441 - acc: 0.6053 - val_loss: 1.4740 - val_acc: 0.5978models/cifar10_resnet_model.h5\cf1\highlight1 
\par \cf0\highlight0 Epoch 3/100\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 1.2583 - acc: 0.6702Ave Done .....\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 190s - loss: 1.2581 - acc: 0.6703 - val_loss: 1.6705 - val_acc: 0.5797\cf1\highlight1 
\par \cf0\highlight0 Epoch 4/100\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 1.1518 - acc: 0.7056Ave Done .....\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 191s - loss: 1.1517 - acc: 0.7056 - val_loss: 1.2797 - val_acc: 0.6717models/cifar10_resnet_model.h5\cf1\highlight1 
\par \cf0\highlight0 Epoch 5/100\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 1.0659 - acc: 0.7384Ave Done .....\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 191s - loss: 1.0660 - acc: 0.7384 - val_loss: 1.4753 - val_acc: 0.6296\cf1\highlight1 
\par \cf0\highlight0 Epoch 6/100\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 1.0119 - acc: 0.7572Ave Done .....\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 193s - loss: 1.0116 - acc: 0.7573 - val_loss: 1.0960 - val_acc: 0.7329models/cifar10_resnet_model.h5\cf1\highlight1 
\par \cf0\highlight0 Epoch 7/100\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 0.9724 - acc: 0.7698Ave Done .....\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 191s - loss: 0.9724 - acc: 0.7697 - val_loss: 1.2353 - val_acc: 0.6813\cf1\highlight1 
\par \cf0\highlight0 Epoch 8/100\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 0.9356 - acc: 0.7815Ave Done .....\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 191s - loss: 0.9358 - acc: 0.7815 - val_loss: 0.9566 - val_acc: 0.7761models/cifar10_resnet_model.h5\cf1\highlight1 
\par \cf0\highlight0 Epoch 9/100\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 0.9056 - acc: 0.7938Ave Done .....\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 188s - loss: 0.9058 - acc: 0.7937 - val_loss: 1.5386 - val_acc: 0.6358\cf1\highlight1 
\par \cf0\highlight0 Epoch 10/100\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 0.8795 - acc: 0.8020Ave Done .....\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 188s - loss: 0.8796 - acc: 0.8019 - val_loss: 0.9698 - val_acc: 0.7719\cf1\highlight1 
\par \cf0\highlight0 Epoch 11/100\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 0.8488 - acc: 0.8112Ave Done .....\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 190s - loss: 0.8490 - acc: 0.8111 - val_loss: 1.0444 - val_acc: 0.7485\cf1\highlight1 
\par \cf0\highlight0 Epoch 12/100\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 0.8392 - acc: 0.8137Ave Done .....\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 191s - loss: 0.8393 - acc: 0.8136 - val_loss: 0.9127 - val_acc: 0.7904models/cifar10_resnet_model.h5\cf1\highlight1 
\par \cf0\highlight0 Epoch 13/100\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 0.8169 - acc: 0.8225Ave Done .....\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 190s - loss: 0.8170 - acc: 0.8225 - val_loss: 0.9932 - val_acc: 0.7622\cf1\highlight1 
\par \cf0\highlight0 Epoch 14/100\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 0.8043 - acc: 0.8257Ave Done .....\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 191s - loss: 0.8042 - acc: 0.8257 - val_loss: 0.8637 - val_acc: 0.8068models/cifar10_resnet_model.h5\cf1\highlight1 
\par \cf0\highlight0 Epoch 15/100\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 0.7871 - acc: 0.8302Ave Done .....\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 188s - loss: 0.7870 - acc: 0.8301 - val_loss: 1.0595 - val_acc: 0.7474\cf1\highlight1 
\par \cf0\highlight0 Epoch 16/100\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 0.7765 - acc: 0.8339Ave Done .....\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 189s - loss: 0.7765 - acc: 0.8339 - val_loss: 0.8002 - val_acc: 0.8265models/cifar10_resnet_model.h5\cf1\highlight1 
\par \cf0\highlight0 Epoch 17/100\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 0.7643 - acc: 0.8378Ave Done .....\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 189s - loss: 0.7642 - acc: 0.8378 - val_loss: 0.9758 - val_acc: 0.7791\cf1\highlight1 
\par \cf0\highlight0 Epoch 18/100\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 0.7563 - acc: 0.8409Ave Done .....\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 189s - loss: 0.7565 - acc: 0.8408 - val_loss: 0.9721 - val_acc: 0.7759\cf1\highlight1 
\par \cf0\highlight0 Epoch 19/100\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 0.7500 - acc: 0.8446Ave Done .....\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 189s - loss: 0.7499 - acc: 0.8447 - val_loss: 0.8246 - val_acc: 0.8157\cf1\highlight1 
\par \cf0\highlight0 Epoch 20/100\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 0.7396 - acc: 0.8464Ave Done .....\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 189s - loss: 0.7396 - acc: 0.8463 - val_loss: 1.1904 - val_acc: 0.7161\cf1\highlight1 
\par \cf0\highlight0 Epoch 21/100\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 0.7323 - acc: 0.8475Ave Done .....\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 189s - loss: 0.7325 - acc: 0.8475 - val_loss: 0.8486 - val_acc: 0.8162\cf1\highlight1 
\par \cf0\highlight0 Epoch 22/100\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 0.7283 - acc: 0.8496Ave Done .....\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 190s - loss: 0.7282 - acc: 0.8496 - val_loss: 0.8740 - val_acc: 0.8059\cf1\highlight1 
\par \cf0\highlight0 Epoch 23/100\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 0.6083 - acc: 0.8878Ave Done .....\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 191s - loss: 0.6084 - acc: 0.8878 - val_loss: 0.6733 - val_acc: 0.8622models/cifar10_resnet_model.h5\cf1\highlight1 
\par \cf0\highlight0 Epoch 24/100\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 0.5535 - acc: 0.8987Ave Done .....\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 191s - loss: 0.5536 - acc: 0.8986 - val_loss: 0.6551 - val_acc: 0.8659models/cifar10_resnet_model.h5\cf1\highlight1 
\par \cf0\highlight0 Epoch 25/100\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 0.5248 - acc: 0.9051Ave Done .....\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 192s - loss: 0.5248 - acc: 0.9051 - val_loss: 0.6251 - val_acc: 0.8754models/cifar10_resnet_model.h5\cf1\highlight1 
\par \cf0\highlight0 Epoch 26/100\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 0.5033 - acc: 0.9093Ave Done .....\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 192s - loss: 0.5033 - acc: 0.9093 - val_loss: 0.6114 - val_acc: 0.8756models/cifar10_resnet_model.h5\cf1\highlight1 
\par \cf0\highlight0 Epoch 27/100\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 0.4853 - acc: 0.9131Ave Done .....\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 191s - loss: 0.4853 - acc: 0.9131 - val_loss: 0.6117 - val_acc: 0.8728\cf1\highlight1 
\par \cf0\highlight0 Epoch 28/100\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 0.4698 - acc: 0.9154Ave Done .....\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 192s - loss: 0.4699 - acc: 0.9154 - val_loss: 0.5977 - val_acc: 0.8768models/cifar10_resnet_model.h5\cf1\highlight1 
\par \cf0\highlight0 Epoch 29/100\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 0.4578 - acc: 0.9171Ave Done .....\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 189s - loss: 0.4577 - acc: 0.9171 - val_loss: 0.6055 - val_acc: 0.8741\cf1\highlight1 
\par \cf0\highlight0 Epoch 30/100\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 0.4508 - acc: 0.9191Ave Done .....\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 189s - loss: 0.4508 - acc: 0.9191 - val_loss: 0.6166 - val_acc: 0.8669\cf1\highlight1 
\par \cf0\highlight0 Epoch 31/100\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 0.4414 - acc: 0.9210Ave Done .....\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 191s - loss: 0.4414 - acc: 0.9210 - val_loss: 0.5987 - val_acc: 0.8736\cf1\highlight1 
\par \cf0\highlight0 Epoch 32/100\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 0.4281 - acc: 0.9230Ave Done .....\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 192s - loss: 0.4282 - acc: 0.9230 - val_loss: 0.5886 - val_acc: 0.8777models/cifar10_resnet_model.h5\cf1\highlight1 
\par \cf0\highlight0 Epoch 33/100\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 0.4262 - acc: 0.9231Ave Done .....\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 190s - loss: 0.4262 - acc: 0.9231 - val_loss: 0.6307 - val_acc: 0.8679\cf1\highlight1 
\par \cf0\highlight0 Epoch 34/100\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 0.4171 - acc: 0.9247Ave Done .....\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 191s - loss: 0.4171 - acc: 0.9247 - val_loss: 0.6540 - val_acc: 0.8568\cf1\highlight1 
\par \cf0\highlight0 Epoch 35/100\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 0.4129 - acc: 0.9255Ave Done .....\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 191s - loss: 0.4129 - acc: 0.9255 - val_loss: 0.5988 - val_acc: 0.8700\cf1\highlight1 
\par \cf0\highlight0 Epoch 36/100\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 0.4039 - acc: 0.9291Ave Done .....\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 189s - loss: 0.4040 - acc: 0.9291 - val_loss: 0.6079 - val_acc: 0.8715\cf1\highlight1 
\par \cf0\highlight0 Epoch 37/100\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 0.4014 - acc: 0.9282Ave Done .....\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 187s - loss: 0.4015 - acc: 0.9281 - val_loss: 0.5981 - val_acc: 0.8713\cf1\highlight1 
\par \cf0\highlight0 Epoch 38/100\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 0.3900 - acc: 0.9322Ave Done .....\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 187s - loss: 0.3901 - acc: 0.9321 - val_loss: 0.6364 - val_acc: 0.8625\cf1\highlight1 
\par \cf0\highlight0 Epoch 39/100\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 0.3492 - acc: 0.9458Ave Done .....\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 189s - loss: 0.3493 - acc: 0.9457 - val_loss: 0.5277 - val_acc: 0.8941models/cifar10_resnet_model.h5\cf1\highlight1 
\par \cf0\highlight0 Epoch 40/100\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 0.3265 - acc: 0.9530Ave Done .....\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 187s - loss: 0.3266 - acc: 0.9530 - val_loss: 0.5312 - val_acc: 0.8945\cf1\highlight1 
\par \cf0\highlight0 Epoch 41/100\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 0.3140 - acc: 0.9562Ave Done .....\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 186s - loss: 0.3140 - acc: 0.9562 - val_loss: 0.5472 - val_acc: 0.8918\cf1\highlight1 
\par \cf0\highlight0 Epoch 42/100\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 0.3036 - acc: 0.9602Ave Done .....\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 186s - loss: 0.3036 - acc: 0.9602 - val_loss: 0.5507 - val_acc: 0.8920\cf1\highlight1 
\par \cf0\highlight0 Epoch 43/100\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 0.3000 - acc: 0.9593Ave Done .....\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 187s - loss: 0.3000 - acc: 0.9593 - val_loss: 0.5513 - val_acc: 0.8921\cf1\highlight1 
\par \cf0\highlight0 Epoch 44/100\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 0.2923 - acc: 0.9621Ave Done .....\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 186s - loss: 0.2923 - acc: 0.9621 - val_loss: 0.5493 - val_acc: 0.8920\cf1\highlight1 
\par \cf0\highlight0 Epoch 45/100\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 0.2862 - acc: 0.9638Ave Done .....\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 187s - loss: 0.2863 - acc: 0.9637 - val_loss: 0.5576 - val_acc: 0.8926\cf1\highlight1 
\par \cf0\highlight0 Epoch 46/100\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 0.2716 - acc: 0.9675Ave Done .....\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 186s - loss: 0.2717 - acc: 0.9675 - val_loss: 0.5320 - val_acc: 0.8987\cf1\highlight1 
\par \cf0\highlight0 Epoch 47/100\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 0.2622 - acc: 0.9719Ave Done .....\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 187s - loss: 0.2623 - acc: 0.9718 - val_loss: 0.5360 - val_acc: 0.8993\cf1\highlight1 
\par \cf0\highlight0 Epoch 48/100\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 0.2582 - acc: 0.9720Ave Done .....\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 186s - loss: 0.2582 - acc: 0.9720 - val_loss: 0.5380 - val_acc: 0.8989\cf1\highlight1 
\par \cf0\highlight0 Epoch 49/100\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 0.2544 - acc: 0.9733Ave Done .....\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 186s - loss: 0.2544 - acc: 0.9733 - val_loss: 0.5380 - val_acc: 0.9000\cf1\highlight1 
\par \cf0\highlight0 Epoch 50/100\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 0.2532 - acc: 0.9733Ave Done .....\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 186s - loss: 0.2532 - acc: 0.9734 - val_loss: 0.5461 - val_acc: 0.8964\cf1\highlight1 
\par \cf0\highlight0 Epoch 51/100\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 0.2455 - acc: 0.9757Ave Done .....\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 185s - loss: 0.2455 - acc: 0.9757 - val_loss: 0.5399 - val_acc: 0.8996\cf1\highlight1 
\par \cf0\highlight0 Epoch 52/100\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 0.2451 - acc: 0.9758Ave Done .....\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 186s - loss: 0.2451 - acc: 0.9758 - val_loss: 0.5374 - val_acc: 0.9004\cf1\highlight1 
\par \cf0\highlight0 Epoch 53/100\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 0.2451 - acc: 0.9759Ave Done .....\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 186s - loss: 0.2451 - acc: 0.9759 - val_loss: 0.5377 - val_acc: 0.9002\cf1\highlight1 
\par \cf0\highlight0 Epoch 54/100\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 0.2407 - acc: 0.9773Ave Done .....\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 186s - loss: 0.2407 - acc: 0.9773 - val_loss: 0.5406 - val_acc: 0.8994\cf1\highlight1 
\par \cf0\highlight0 Epoch 55/100\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 0.2447 - acc: 0.9757Ave Done .....\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 185s - loss: 0.2446 - acc: 0.9757 - val_loss: 0.5373 - val_acc: 0.9015\cf1\highlight1 
\par \cf0\highlight0 Epoch 56/100\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 0.2419 - acc: 0.9774Ave Done .....\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 186s - loss: 0.2419 - acc: 0.9774 - val_loss: 0.5381 - val_acc: 0.9005\cf1\highlight1 
\par \cf0\highlight0 Epoch 57/100\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 0.2415 - acc: 0.9760Ave Done .....\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 186s - loss: 0.2415 - acc: 0.9760 - val_loss: 0.5388 - val_acc: 0.9007\cf1\highlight1 
\par \cf0\highlight0 Epoch 58/100\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 0.2392 - acc: 0.9780Ave Done .....\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 186s - loss: 0.2392 - acc: 0.9780 - val_loss: 0.5386 - val_acc: 0.9015\cf1\highlight1 
\par \cf0\highlight0 Epoch 59/100\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 0.2424 - acc: 0.9761Ave Done .....\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 186s - loss: 0.2424 - acc: 0.9761 - val_loss: 0.5379 - val_acc: 0.8998\cf1\highlight1 
\par \cf0\highlight0 Epoch 60/100\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 0.2352 - acc: 0.9797Ave Done .....\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 185s - loss: 0.2352 - acc: 0.9797 - val_loss: 0.5384 - val_acc: 0.9018\cf1\highlight1 
\par \cf0\highlight0 Epoch 61/100\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 0.2384 - acc: 0.9776Ave Done .....\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 187s - loss: 0.2384 - acc: 0.9776 - val_loss: 0.5381 - val_acc: 0.9010\cf1\highlight1 
\par \cf0\highlight0 Epoch 62/100\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 0.2367 - acc: 0.9786Ave Done .....\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 186s - loss: 0.2367 - acc: 0.9786 - val_loss: 0.5390 - val_acc: 0.9019\cf1\highlight1 
\par \cf0\highlight0 Epoch 63/100\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 0.2394 - acc: 0.9773Ave Done .....\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 186s - loss: 0.2394 - acc: 0.9773 - val_loss: 0.5394 - val_acc: 0.9007\cf1\highlight1 
\par \cf0\highlight0 Epoch 64/100\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 0.2388 - acc: 0.9781Ave Done .....\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 186s - loss: 0.2388 - acc: 0.9781 - val_loss: 0.5390 - val_acc: 0.9011\cf1\highlight1 
\par \cf0\highlight0 Epoch 65/100\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 0.2374 - acc: 0.9776Ave Done .....\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 186s - loss: 0.2374 - acc: 0.9777 - val_loss: 0.5401 - val_acc: 0.9001\cf1\highlight1 
\par \cf0\highlight0 Epoch 66/100\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 0.2354 - acc: 0.9788Ave Done .....\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 185s - loss: 0.2354 - acc: 0.9788 - val_loss: 0.5385 - val_acc: 0.9010\cf1\highlight1 
\par \cf0\highlight0 Epoch 67/100\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 0.2365 - acc: 0.9788Ave Done .....\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 186s - loss: 0.2365 - acc: 0.9788 - val_loss: 0.5389 - val_acc: 0.9013\cf1\highlight1 
\par \cf0\highlight0 Epoch 68/100\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 0.2377 - acc: 0.9777Ave Done .....\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 186s - loss: 0.2378 - acc: 0.9777 - val_loss: 0.5396 - val_acc: 0.9013\cf1\highlight1 
\par \cf0\highlight0 Epoch 69/100\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 0.2366 - acc: 0.9786Ave Done .....\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 187s - loss: 0.2366 - acc: 0.9786 - val_loss: 0.5393 - val_acc: 0.9010\cf1\highlight1 
\par \cf0\highlight0 Epoch 70/100\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 0.2366 - acc: 0.9781Ave Done .....\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 188s - loss: 0.2367 - acc: 0.9781 - val_loss: 0.5391 - val_acc: 0.9007\cf1\highlight1 
\par \cf0\highlight0 Epoch 71/100\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 0.2375 - acc: 0.9777Ave Done .....\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 189s - loss: 0.2376 - acc: 0.9777 - val_loss: 0.5388 - val_acc: 0.9011\cf1\highlight1 
\par \cf0\highlight0 Epoch 72/100\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 0.2355 - acc: 0.9791Ave Done .....\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 188s - loss: 0.2355 - acc: 0.9791 - val_loss: 0.5401 - val_acc: 0.9008\cf1\highlight1 
\par \cf0\highlight0 Epoch 73/100\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 0.2358 - acc: 0.9783Ave Done .....\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 189s - loss: 0.2358 - acc: 0.9783 - val_loss: 0.5395 - val_acc: 0.9011\cf1\highlight1 
\par \cf0\highlight0 Epoch 74/100\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 0.2385 - acc: 0.9778Ave Done .....\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 188s - loss: 0.2384 - acc: 0.9779 - val_loss: 0.5401 - val_acc: 0.9002\cf1\highlight1 
\par \cf0\highlight0 Epoch 75/100\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 0.2377 - acc: 0.9785Ave Done .....\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 188s - loss: 0.2376 - acc: 0.9785 - val_loss: 0.5396 - val_acc: 0.9008\cf1\highlight1 
\par \cf0\highlight0 Epoch 76/100\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 0.2355 - acc: 0.9779Ave Done .....\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 188s - loss: 0.2355 - acc: 0.9779 - val_loss: 0.5396 - val_acc: 0.9010\cf1\highlight1 
\par \cf0\highlight0 Epoch 77/100\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 0.2390 - acc: 0.9775Ave Done .....\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 187s - loss: 0.2389 - acc: 0.9775 - val_loss: 0.5404 - val_acc: 0.9007\cf1\highlight1 
\par \cf0\highlight0 Epoch 78/100\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 0.2368 - acc: 0.9783Ave Done .....\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 188s - loss: 0.2368 - acc: 0.9783 - val_loss: 0.5398 - val_acc: 0.9005\cf1\highlight1 
\par \cf0\highlight0 Epoch 79/100\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 0.2378 - acc: 0.9778Ave Done .....\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 188s - loss: 0.2379 - acc: 0.9778 - val_loss: 0.5398 - val_acc: 0.9012\cf1\highlight1 
\par \cf0\highlight0 Epoch 80/100\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 0.2370 - acc: 0.9781Ave Done .....\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 188s - loss: 0.2370 - acc: 0.9781 - val_loss: 0.5394 - val_acc: 0.9009\cf1\highlight1 
\par \cf0\highlight0 Epoch 81/100\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 0.2380 - acc: 0.9775Ave Done .....\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 187s - loss: 0.2381 - acc: 0.9775 - val_loss: 0.5397 - val_acc: 0.9008\cf1\highlight1 
\par \cf0\highlight0 Epoch 82/100\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 0.2390 - acc: 0.9768Ave Done .....\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 188s - loss: 0.2390 - acc: 0.9769 - val_loss: 0.5395 - val_acc: 0.9014\cf1\highlight1 
\par \cf0\highlight0 Epoch 83/100\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 0.2376 - acc: 0.9776Ave Done .....\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 188s - loss: 0.2376 - acc: 0.9776 - val_loss: 0.5397 - val_acc: 0.9008\cf1\highlight1 
\par \cf0\highlight0 Epoch 84/100\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 0.2366 - acc: 0.9785Ave Done .....\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 188s - loss: 0.2366 - acc: 0.9785 - val_loss: 0.5394 - val_acc: 0.9011\cf1\highlight1 
\par \cf0\highlight0 Epoch 85/100\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 0.2357 - acc: 0.9785Ave Done .....\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 188s - loss: 0.2357 - acc: 0.9785 - val_loss: 0.5399 - val_acc: 0.9013\cf1\highlight1 
\par \cf0\highlight0 Epoch 86/100\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 0.2362 - acc: 0.9793Ave Done .....\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 188s - loss: 0.2362 - acc: 0.9794 - val_loss: 0.5400 - val_acc: 0.9011\cf1\highlight1 
\par \cf0\highlight0 Epoch 87/100\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 0.2371 - acc: 0.9782Ave Done .....\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 188s - loss: 0.2370 - acc: 0.9782 - val_loss: 0.5401 - val_acc: 0.9010\cf1\highlight1 
\par \cf0\highlight0 Epoch 88/100\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 0.2370 - acc: 0.9780Ave Done .....\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 188s - loss: 0.2370 - acc: 0.9780 - val_loss: 0.5396 - val_acc: 0.9011\cf1\highlight1 
\par \cf0\highlight0 Epoch 89/100\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 0.2350 - acc: 0.9789Ave Done .....\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 187s - loss: 0.2350 - acc: 0.9789 - val_loss: 0.5402 - val_acc: 0.9008\cf1\highlight1 
\par \cf0\highlight0 Epoch 90/100\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 0.2372 - acc: 0.9777Ave Done .....\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 188s - loss: 0.2373 - acc: 0.9777 - val_loss: 0.5400 - val_acc: 0.9013\cf1\highlight1 
\par \cf0\highlight0 Epoch 91/100\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 0.2338 - acc: 0.9794Ave Done .....\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 188s - loss: 0.2338 - acc: 0.9794 - val_loss: 0.5404 - val_acc: 0.9009\cf1\highlight1 
\par \cf0\highlight0 Epoch 92/100\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 0.2367 - acc: 0.9784Ave Done .....\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 188s - loss: 0.2367 - acc: 0.9784 - val_loss: 0.5403 - val_acc: 0.9012\cf1\highlight1 
\par \cf0\highlight0 Epoch 93/100\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 0.2352 - acc: 0.9788Ave Done .....\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 188s - loss: 0.2352 - acc: 0.9788 - val_loss: 0.5405 - val_acc: 0.9011\cf1\highlight1 
\par \cf0\highlight0 Epoch 94/100\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 0.2357 - acc: 0.9780Ave Done .....\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 188s - loss: 0.2357 - acc: 0.9780 - val_loss: 0.5402 - val_acc: 0.9010\cf1\highlight1 
\par \cf0\highlight0 Epoch 95/100\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 0.2357 - acc: 0.9785Ave Done .....\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 188s - loss: 0.2357 - acc: 0.9785 - val_loss: 0.5405 - val_acc: 0.9010\cf1\highlight1 
\par \cf0\highlight0 Epoch 96/100\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 0.2361 - acc: 0.9784Ave Done .....\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 188s - loss: 0.2361 - acc: 0.9784 - val_loss: 0.5408 - val_acc: 0.9010\cf1\highlight1 
\par \cf0\highlight0 Epoch 97/100\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 0.2377 - acc: 0.9785Ave Done .....\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 188s - loss: 0.2377 - acc: 0.9785 - val_loss: 0.5396 - val_acc: 0.9016\cf1\highlight1 
\par \cf0\highlight0 Epoch 98/100\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 0.2351 - acc: 0.9783Ave Done .....\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 188s - loss: 0.2351 - acc: 0.9783 - val_loss: 0.5404 - val_acc: 0.9005\cf1\highlight1 
\par \cf0\highlight0 Epoch 99/100\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 0.2345 - acc: 0.9791Ave Done .....\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 187s - loss: 0.2345 - acc: 0.9791 - val_loss: 0.5403 - val_acc: 0.9015\cf1\highlight1 
\par \cf0\highlight0 Epoch 100/100\cf1\highlight1 
\par \cf0\highlight0 Ave Done ...\cf1\highlight1 
\par \cf0\highlight0 1561/1562 [============================>.] - ETA: 0s - loss: 0.2363 - acc: 0.9783Ave Done .....\cf1\highlight1 
\par \cf0\highlight0 1562/1562 [==============================] - 188s - loss: 0.2363 - acc: 0.9783 - val_loss: 0.5397 - val_acc: 0.9015\cf1\highlight1 
\par \cf0\highlight0 10000/10000 [==============================] - 4s\cf1\highlight1 
\par \cf0\highlight0 Test loss: 0.539706042004\cf1\highlight1 
\par \cf0\highlight0 Test accuracy: 0.9015\cf1\highlight1 
\par \cf0\highlight0 (tf) xdong@gpu2:/data/xdong/codes/avg_para/keras-res$\cf1\highlight1 
\par \pard\cf0\highlight0\f2\fs20 
\par }
 